{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#samba-package-sandbox-for-mixing-using-bayesian-analysis","title":"SAMBA package: SAndbox for Mixing using Bayesian Analysis","text":"<p>Bayesian model mixing is a novel concept that surpasses the more widely used Bayesian model averaging (BMA) in its use of location-dependent weights, allowing multiple models to dominate in regions where they are the most accurate model known, instead of averaging over all of the models in the space. In this repo, we store the preliminary code and tests of three model mixing techniques studied on a test case meant to symbolise the type of nuclear physics problem we want to apply these methods to in the future. This will, in the next few months, be compiled into a package to be released this spring/summer (2022) under the name of SAMBA---SAndbox for Mixing using Bayesian Analysis. This accompanies the paper we have posted to arXiv (see the paper here) where we detail our work using these techniques on our toy problem: the zero-dimensional $\\phi^4$ theory partition function.</p>"},{"location":"#about","title":"About","text":"<p>This package is intended to contain a variety of methods to perform Bayesian Model Mixing on a given set of data points and errors across an input space. In the current version, it can apply three different BMM methods to series expansions in the limits of a coupling constant. In future versions, we will include the option for users to implement their own functions or data sets to mix using SAMBA. It is a part of the BAND collaboration's v0.2 software. </p> <p></p>"},{"location":"#workflow","title":"Workflow","text":"<ul> <li> <p>Instantiate an object using the method of your choice (<code>LMM</code> (linear mixture model), <code>Bivariate</code> (bivariate BMM), or <code>GP</code> (trivariate BMM with a GP)) and select the truncation orders for the small-g and large-g expansions. </p> </li> <li> <p>Mix the models using the provided functions in the method classes listed above. </p> </li> </ul> <p>Refer to the Tutorials folder in the repo for a comprehensive set of notebooks on how to implement SAMBA on the current toy model setup. </p> <p>An animated <code>.gif</code> of our toy models is below. </p> <p> </p>"},{"location":"#testing","title":"Testing","text":"<p>At its present status, there are two ways to test SAMBA:</p> <ul> <li> <p>After cloning this repo, make sure you have all of the required packages installed (see requirements.txt for a list of external packages needed) and open the docs/Tutorials folder. Open the 3 notebooks there and run each notebook. If each cell is free of any errors, SAMBA is working correctly!</p> </li> <li> <p>After cloning this repo, open a terminal and go to the Tests folder. This contains two pytest-compatible files (stay tuned for more!) that test the Bivariate and GP methods. Type <code>pytest</code> in your terminal and it should run both of the files. If all tests are passing and only a couple of warnings (about external packages) show up, SAMBA is passing its tests!</p> </li> </ul>"},{"location":"discrepancy/","title":"Method 2: Bivariate model mixing","text":"<p>In this method, we use pointwise bivariate model mixing, or precision-weighted mixing, requiring models to be evaluated at every point in the input space where we desire a prediction to be made. This method can be written succinctly as</p> <p>$$ f_{\\dagger} = \\frac{1}{Z_P}\\sum_{k=1}^{K} \\frac{1}{v_k}f_k,    \\qquad Z_P \\equiv \\sum_{k=1}^{K}\\frac{1}{v_k}, $$</p> <p>where we can also define</p> <p>$$ f_{\\dagger} \\sim \\mathcal{N}\\bigl(Z_P^{-1}\\sum_k \\frac{1}{v_k}f_k, Z_P^{-1}\\bigr). $$</p> <p>This method is precision-weighted because it uses the variances of the models at each point in the input space as the inverse weights of the corresponding model prediction, hence the model with the smallest variance at a given point will dominate the mixed model at that location.</p>"},{"location":"discrepancy/#samba.discrepancy.Bivariate","title":"<code>Bivariate</code>","text":"<p>               Bases: <code>Models</code>, <code>Uncertainties</code></p> Source code in <code>samba/discrepancy.py</code> <pre><code>class Bivariate(Models, Uncertainties):\n\n    def __init__(self, loworder, highorder, error_model='informative', ci=68):\n\n        r'''\n        The bivariate BMM method used to construct the mixed model of two series\n        expansions. This class contains the fdagger function and the plotter.\n\n        Example:\n            Bivariate(loworder=5, highorder=10)\n\n        Parameters:\n            loworder (numpy.ndarray, int, float): The value of N_s to be \n                used to truncate the small-g expansion.\n\n            highorder (numpy.ndarray, int, float): The value of N_l to be \n                used to truncate the large-g expansion.\n\n            error_model (str): The error model to be used in the calculation. \n                Options are 'uninformative' and 'informative'. Default is 'informative'. \n\n            ci (int): The value of the credibility interval desired (can be 68 or 95).\n\n        Returns:\n            None.\n        '''\n\n        #get interval\n        self.ci = ci\n\n        #instantiate the Uncertainties class and error model\n        self.u = Uncertainties(error_model)\n        self.error_model = self.u.error_model\n\n        #check type and assign class variables\n        if isinstance(loworder, float) == True or isinstance(loworder, int) == True:\n            loworder = np.array([loworder])\n\n        if isinstance(highorder, float) == True or isinstance(highorder, int) == True:\n            highorder = np.array([highorder])\n\n        self.loworder = loworder \n        self.highorder = highorder\n\n        #instantiate Models() class here\n        self.m = Models(self.loworder, self.highorder)\n\n        return None\n\n\n    def fdagger(self, g, GP_mean=np.zeros([2]), GP_var=np.zeros([2])): \n\n        r'''\n        A do-it-all function to determine the pdf of the mixed model. Can use models \n        indicated by inputting arrays into the loworder and highorder variables,\n        and accept GP mean and variance arrays in the GP_mean and GP_var options.\n\n        Example:\n            Bivariate.fdagger(g=np.linspace(1e-6, 0.5, 100), GP_mean=np.array([]), \n                GP_var=np.array([]))\n\n        Parameters:\n            g (numpy.linspace): The linspace over which this calculation is performed.\n\n            GP_mean (numpy.ndarray): An array of mean values from a Gaussian process \n                to be mixed in as a third model (optional).  \n\n            GP_var (numpy.ndarray): An array of variances from a Gaussian process to \n                be mixed in as a third model (optional).\n\n        Returns:\n            mean (numpy.ndarray): The mixed model mean (either including a GP or not \n                depending on the function arguments).\n\n            intervals (numpy.ndarray): The credibility interval of the mixed model mean.\n\n            interval_low (numpy.ndarray): The variance interval for the small-g expansion \n                (calculated from the next order after the truncation). \n\n            interval_high (numpy.ndarray): The variance interval for the large-g expansion \n                (calculated from the next order after the truncation).\n        '''\n\n        #check type\n        if isinstance(self.loworder, float) == True or isinstance(self.loworder, int) == True:\n            self.loworder = np.array([self.loworder])\n\n        if isinstance(self.highorder, float) == True or isinstance(self.highorder, int) == True:\n            self.highorder = np.array([self.highorder])\n\n        #uncertainties\n        v_low = np.asarray([self.u.variance_low(g, self.loworder[i]) for i in range(len(self.loworder))])\n        v_high = np.asarray([self.u.variance_high(g, self.highorder[i]) for i in range(len(self.highorder))])\n\n        #calculating models\n        f_low = np.asarray([self.m.low_g(g)[i,:] for i in range(len(self.loworder))])\n        f_high = np.asarray([self.m.high_g(g)[i,:] for i in range(len(self.highorder))])\n\n        #concatenate models and variances\n        if GP_mean.any() and GP_var.any() != 0:\n            f = np.concatenate((f_low, f_high, GP_mean.reshape(-1,1).T), axis=0)\n            v = np.concatenate((v_low, v_high, GP_var.reshape(-1,1).T), axis=0)\n        else:\n            f = np.concatenate((f_low, f_high), axis=0)\n            v = np.concatenate((v_low, v_high), axis=0)\n\n        #initialise arrays\n        mean_n = np.zeros([len(f), len(g)])\n        mean_d = np.zeros([len(f), len(g)])\n        mean = np.zeros([len(g)])\n        var = np.zeros([len(f), len(g)])\n\n        #create fdagger for each value of g\n        for i in range(len(f)):\n            mean_n[i] = f[i]/v[i]\n            mean_d[i] = 1.0/v[i]\n            var[i] = 1.0/v[i]\n\n        #save variances for each model\n        self.var_weights = var/(np.sum(var, axis=0))\n\n        mean_n = np.sum(mean_n, axis=0)\n        mean_d = np.sum(mean_d, axis=0)\n\n        #mean, variance calculation\n        mean = mean_n/mean_d\n        var = 1.0/np.sum(var, axis=0)\n\n        #which credibility interval to use\n        if self.ci == 68:\n            val = 1.0\n        elif self.ci == 95:\n            val = 1.96\n        else:\n            raise ValueError('Please enter either 68 or 95.')\n\n        #initialise credibility intervals\n        intervals = np.zeros([len(g), 2])\n        interval_low = np.zeros([len(self.loworder), len(g), 2])\n        interval_high = np.zeros([len(self.highorder), len(g), 2])\n\n        #calculate credibility intervals\n        intervals[:, 0] = (mean - val * np.sqrt(var))\n        intervals[:, 1] = (mean + val * np.sqrt(var))\n\n        for i in range(len(self.loworder)):\n            interval_low[i,:,0] = (self.m.low_g(g)[i,:] - val * np.sqrt(v_low[i,:]))\n            interval_low[i,:,1] = (self.m.low_g(g)[i,:] + val * np.sqrt(v_low[i,:]))\n\n        for i in range(len(self.highorder)):\n            interval_high[i,:,0] = (self.m.high_g(g)[i,:] - val * np.sqrt(v_high[i,:]))\n            interval_high[i,:,1] = (self.m.high_g(g)[i,:] + val * np.sqrt(v_high[i,:]))\n\n        return mean, intervals, interval_low, interval_high\n\n\n    def plot_mix(self, g, plot_fdagger=True, plot_true=True, GP_mean=np.zeros([2]), GP_var=np.zeros([2])):\n\n        r'''\n        An all-in-one plotting function that will plot the results of fdagger for N numbers\n        of models, the next orders of the expansion models, and the validation step of the \n        model mixing in fdagger to test fdagger results.\n\n        Example:\n            Bivariate.plot_mix(g=np.linspace(1e-6, 0.5, 100), plot_fdagger=True)\n\n        Parameters:\n            g (numpy.linspace): The space over which the models are calculated.\n\n            plot_fdagger (bool): If True, this parameter will allow for the \n                plotting of fdagger and its credibility interval. \n\n            plot_true (bool): Determines whether or not to plot the true model curve. \n                Default is True. \n\n            GP_mean (numpy.ndarray): The mean array from the GP being included. \n\n            GP_var (numpy.ndarray): The variance array from the GP being included.\n\n        Returns:\n            mean (numpy.ndarray): The mean of the mixed model at each point in g.\n\n            intervals (numpy.ndarray): The values of the credibility intervals at each\n                point in g. \n        '''\n\n        #set up plot configuration\n        fig = plt.figure(figsize=(8,6), dpi=600)\n        ax = plt.axes()\n        ax.tick_params(axis='x', labelsize=18)\n        ax.tick_params(axis='y', labelsize=18)\n        ax.locator_params(nbins=8)\n        ax.xaxis.set_minor_locator(AutoMinorLocator())\n        ax.yaxis.set_minor_locator(AutoMinorLocator())\n\n        #set up x and y limits\n        ax.set_xlim(0.0,1.0)\n       # ax.set_xlim(0.0,0.5)\n        ax.set_ylim(1.2,3.2)\n       # ax.set_ylim(1.0,3.0)\n        ax.set_yticks([1.2, 1.6, 2.0, 2.4, 2.8, 3.2])\n      #  ax.set_ylim(2.0,2.8)\n       # ax.set_yticks([2.0, 2.2, 2.4, 2.6, 2.8])\n\n        #labels and true model\n        ax.set_xlabel('g', fontsize=22)\n        ax.set_ylabel('F(g)', fontsize=22)\n\n        if plot_true is True:\n            ax.plot(g, self.m.true_model(g), 'k', label='True model')\n\n        #call fdagger to calculate results\n        if GP_mean.any() and GP_var.any() != 0:\n            mean, intervals, interval_low, interval_high = self.fdagger(g, GP_mean, GP_var)\n\n        else:\n            mean, intervals, interval_low, interval_high = self.fdagger(g)\n\n        #plot the small-g expansions and error bands\n        for i,j in zip(range(len(self.loworder)), self.loworder):\n            ax.plot(g, self.m.low_g(g)[i,:], 'r--', label=r'$f_s$ ($N_s$ = {})'.format(j))\n\n        for i in range(len(self.loworder)):\n            ax.plot(g, interval_low[i, :, 0], 'r', linestyle='dotted', \\\n                label=r'$f_s$ ($N_s$ = {}) {}\\% CI'.format(self.loworder[i], int(self.ci)))\n            ax.plot(g, interval_low[i, :, 1], 'r', linestyle='dotted')\n\n        #for each large-g order, calculate and plot\n        for i,j in zip(range(len(self.highorder)), self.highorder):\n            ax.plot(g, self.high_g(g)[i,:], 'b--', label=r'$f_l$ ($N_l$ = {})'.format(j))\n\n        for i in range(len(self.highorder)):\n            ax.plot(g, interval_high[i, :, 0], 'b', linestyle='dotted', \\\n                label=r'$f_l$ ($N_l$ = {}) {}\\% CI'.format(self.highorder[i], int(self.ci)))\n            ax.plot(g, interval_high[i, :, 1], 'b', linestyle='dotted')\n\n        # GP\n#         if GP_mean.any() and GP_var.any() != 0:\n#             ax.plot(g, GP_mean, 'm')\n#             ax.fill_between(g, GP_mean-np.sqrt(GP_var), GP_mean+np.sqrt(GP_var), color='m', alpha=0.2)\n\n        if plot_fdagger == True:\n            ax.plot(g, mean, 'g', label='Mean')\n            ax.plot(g, intervals[:,0], 'g', linestyle='dotted', label=r'{}$\\%$ CI'.format(int(self.ci)))\n            ax.plot(g, intervals[:,1], 'g', linestyle='dotted')\n            ax.fill_between(g, intervals[:,0], intervals[:,1], color='green', alpha=0.2)\n\n        ax.legend(fontsize=16, loc='upper right')\n        plt.show()\n\n        # #save figure option\n        # response = input('Would you like to save this figure? (yes/no)')\n\n        # if response == 'yes':\n        #     name = input('Enter a file name (include .jpg, .png, etc.)')\n        #     fig.savefig(name, bbox_inches='tight')\n\n        return mean, intervals \n\n\n    def subplot_mix(self, g, GP_mean=np.zeros([2]), GP_var=np.zeros([2]), log=False): \n\n        r'''\n        An all-in-one plotting function that will plot the results of fdagger for N numbers\n        of models side-by-side with the 2 model case to compare. Currently used to plot the GP\n        results alongside those without the GP; N models case not color-coded yet.  \n\n        Example:\n            Bivariate.subplot_mix(g=np.linspace(1e-6, 0.5, 100))\n\n        Parameters:\n            g (numpy.linspace): The space over which the models are calculated.\n\n            GP_mean (numpy.ndarray): An array of GP PPD results (that MUST be at \n                input points in g) to be mixed in with the expansions chosen. Optional. \n\n            GP_var (numpy.ndarray): An array of GP variance results (that MUST be at \n                input points in g) to be mixed in with the expansions chosen. Optional. \n\n            log (bool): A toggle for logscale. Default is False. \n\n        Returns:\n            None.\n        '''\n\n        #set up plot configuration\n        fig = plt.figure(figsize=(16,6), dpi=600)\n        gs = fig.add_gridspec(1, 2, hspace=0, wspace=0)\n        (ax1, ax2) = gs.subplots(sharex='col', sharey='row')\n\n        for ax in (ax1, ax2):\n\n            ax.tick_params(axis='x', labelsize=18)\n            ax.tick_params(axis='y', labelsize=18)\n            ax.locator_params(nbins=5)\n            ax.xaxis.set_minor_locator(AutoMinorLocator())\n            ax.yaxis.set_minor_locator(AutoMinorLocator())\n            ax.set_xlim(0.0,1.0)\n            ax.set_xticks([0.1, 0.3, 0.5, 0.7, 0.9])\n            ax.set_ylim(1.0,3.0)\n\n            #labels and true model\n            ax.set_xlabel('g', fontsize=22)\n            ax.set_ylabel('F(g)', fontsize=22)\n            ax.plot(g, self.m.true_model(g), 'k', label='True model')\n\n            #only label outer plot axes\n            ax.label_outer()\n\n        #log scale option\n        if log is True:\n            for ax in (ax1, ax2):\n                ax.set_yscale('log')\n                ax.set_ylim(1e-2, 10.0)\n\n        #call fdagger to calculate results\n        mean2, intervals2, _, _ = self.fdagger(g)\n\n        if GP_mean.any() and GP_var.any() != 0:\n            mean, intervals, interval_low, interval_high = self.fdagger(g, GP_mean, GP_var)\n\n        else:\n            mean, intervals, interval_low, interval_high = self.fdagger(g)\n\n        #plot the small-g expansions and error bands (panel a)\n        for i,j in zip(range(len(self.loworder)), self.loworder):\n            ax1.plot(g, self.m.low_g(g)[i,:], 'r--', \\\n                label=r'$f_s$ ($N_s$ = {})'.format(j))\n            ax1.plot(g, interval_low[i, :, 0], 'r', linestyle='dotted',\\\n                label=r'$f_s$ {}\\% CI'.format(int(self.ci)))\n            ax1.plot(g, interval_low[i, :, 1], 'r', linestyle='dotted')\n\n        #plot the small-g expansions and error bands (panel b)\n        for i,j in zip(range(len(self.loworder)), self.loworder):\n            ax2.plot(g, self.m.low_g(g)[i,:], 'r--', label=r'$f_s$ ($N_s$ = {})'.format(j))\n            ax2.plot(g, interval_low[i, :, 0], 'r', linestyle='dotted',\\\n                    label=r'$f_s$ {}\\% CI'.format(int(self.ci)))\n            ax2.plot(g, interval_low[i, :, 1], 'r', linestyle='dotted')\n\n        #plot the large-g expansions and error bands (panel a)\n        for i,j in zip(range(len(self.highorder)), self.highorder):\n            ax1.plot(g, self.m.high_g(g)[i,:], 'b--', label=r'$f_l$ ($N_l$ = {})'.format(j))\n            ax1.plot(g, interval_high[i, :, 0], 'b', linestyle='dotted', \\\n                label=r'$f_l$ {}\\% CI'.format(int(self.ci)))\n            ax1.plot(g, interval_high[i, :, 1], 'b', linestyle='dotted')\n\n        #plot the large-g expansions and error bands (panel b)\n        for i,j in zip(range(len(self.highorder)), self.highorder):\n            ax2.plot(g, self.m.high_g(g)[i,:], 'b--', label=r'$f_l$ ($N_l$ = {})'.format(j))\n            ax2.plot(g, interval_high[i, :, 0], 'b', linestyle='dotted', \\\n                    label=r'$f_l$ {}\\% CI'.format(int(self.ci)))\n            ax2.plot(g, interval_high[i, :, 1], 'b', linestyle='dotted')\n\n        #2 model case\n        ax1.plot(g, mean2, 'g', label='Mixed model')\n        ax1.plot(g, intervals2[:,0], 'g', linestyle='dotted', label=r'{}$\\%$ CI'.format(int(self.ci)))\n        ax1.plot(g, intervals2[:,1], 'g', linestyle='dotted')\n        ax1.fill_between(g, intervals2[:,0], intervals2[:,1], color='green', alpha=0.2)\n\n        #N model case\n        ax2.plot(g, mean, 'g', label='Mixed model')\n        ax2.plot(g, intervals[:,0], 'g', linestyle='dotted', label=r'{}$\\%$ CI'.format(int(self.ci)))\n        ax2.plot(g, intervals[:,1], 'g', linestyle='dotted')\n        ax2.fill_between(g, intervals[:,0], intervals[:,1], color='green', alpha=0.2)\n\n        #ax2.legend(bbox_to_anchor=(1.0, 0.5), fontsize=12, loc='center left')\n        ax2.legend(fontsize=16, loc='upper right')\n\n        #label panels\n        ax1.text(0.94, 1.1, '(a)', fontsize=16)\n        ax2.text(0.94, 1.1, '(b)', fontsize=16)\n\n        plt.show()\n\n        #save figure option\n        # response = input('Would you like to save this figure? (yes/no)')\n\n        # if response == 'yes':\n        #     name = input('Enter a file name (include .jpg, .png, etc.)')\n        #     fig.savefig(name, bbox_inches='tight')\n\n        return None\n\n\n    def plot_error_models(self, g): \n\n        r'''\n        A plotter to compare the uninformative error model results of two models \n        to the informative error model results for the same two models. Panel a\n        refers to the uninformative error model panel in the subplot, and panel b\n        corresponds to the informative error model panel. \n\n        Example:\n            Bivariate.plot_error_models(g=np.linspace(1e-6, 0.5, 100))\n\n        Parameters:\n            g (numpy.linspace): The space over which the models are calculated.\n\n        Returns:\n            None.\n        '''\n\n        #set up plot configuration\n        fig = plt.figure(figsize=(16,6), dpi=600)\n        gs = fig.add_gridspec(1, 2, hspace=0, wspace=0)\n        (ax1, ax2) = gs.subplots(sharex='col', sharey='row')\n\n        for ax in (ax1, ax2):\n\n            ax.tick_params(axis='x', labelsize=18)\n            ax.tick_params(axis='y', labelsize=18)\n            ax.locator_params(nbins=5)\n            ax.xaxis.set_minor_locator(AutoMinorLocator())\n            ax.yaxis.set_minor_locator(AutoMinorLocator())\n            ax.set_xlim(0.0,1.0)\n            ax.set_xticks([0.1, 0.3, 0.5, 0.7, 0.9])\n            ax.set_ylim(1.0,3.0)\n\n            #labels and true model\n            ax.set_xlabel('g', fontsize=22)\n            ax.set_ylabel('F(g)', fontsize=22)\n            ax.plot(g, self.m.true_model(g), 'k', label='True model')\n\n            #only label outer plot axes\n            ax.label_outer()\n\n        #call fdagger to calculate results (overwrite class variable)\n        self.u = Uncertainties(error_model='uninformative')\n        self.error_model = self.u.error_model\n        mean_u, intervals_u, interval_low_u, interval_high_u = self.fdagger(g)\n        self.u = Uncertainties(error_model='informative')\n        self.error_model = self.u.error_model \n        mean_i, intervals_i, interval_low_i, interval_high_i = self.fdagger(g)\n\n        #plot the small-g expansions and error bands (panel a)\n        ax1.plot(g, self.m.low_g(g)[0,:], 'r--', \\\n            label=r'$f_s$ ($N_s$ = {})'.format(self.loworder[0]))\n        ax1.plot(g, interval_low_u[0, :, 0], 'r', linestyle='dotted',\\\n             label=r'$f_s$ ($N_s$ = {}) {}\\% CI'.format(self.loworder[0], int(self.ci)))\n        ax1.plot(g, interval_low_u[0, :, 1], 'r', linestyle='dotted')\n\n        #plot the small-g expansions and error bands (panel b)\n        ax2.plot(g, self.m.low_g(g)[0,:], 'r--', label=r'$f_s$ ($N_s$ = {})'.format(self.loworder[0]))\n        ax2.plot(g, interval_low_i[0, :, 0], 'r', linestyle='dotted',\\\n                label=r'$f_s$ {}\\% CI'.format(int(self.ci)))\n        ax2.plot(g, interval_low_i[0, :, 1], 'r', linestyle='dotted')\n\n        #plot the large-g expansions and error bands (panel a)\n        ax1.plot(g, self.m.high_g(g)[0,:], 'b--', label=r'$f_l$ ($N_l$ = {})'.format(self.highorder[0]))\n        ax1.plot(g, interval_high_u[0, :, 0], 'b', linestyle='dotted', label=r'$f_l$ {}\\% CI'.format(int(self.ci)))\n        ax1.plot(g, interval_high_u[0, :, 1], 'b', linestyle='dotted')\n\n        #plot the large-g expansions and error bands (panel b)\n        ax2.plot(g, self.m.high_g(g)[0,:], 'b--', label=r'$f_l$ ($N_l$ = {})'.format(self.highorder[0]))\n        ax2.plot(g, interval_high_i[0, :, 0], 'b', linestyle='dotted', \\\n            label=r'$f_l$ {}\\% CI'.format(int(self.ci)))\n        ax2.plot(g, interval_high_i[0, :, 1], 'b', linestyle='dotted')\n\n        #PPD (panel a)\n        ax1.plot(g, mean_u, 'g', label='Mixed model')\n        ax1.plot(g, intervals_u[:,0], 'g', linestyle='dotted', label=r'{}$\\%$ CI'.format(int(self.ci)))\n        ax1.plot(g, intervals_u[:,1], 'g', linestyle='dotted')\n        ax1.fill_between(g, intervals_u[:,0], intervals_u[:,1], color='green', alpha=0.2)\n\n        #PPD (panel b)\n        ax2.plot(g, mean_i, 'g', label='Mixed model')\n        ax2.plot(g, intervals_i[:,0], 'g', linestyle='dotted', label=r'{}$\\%$ CI'.format(int(self.ci)))\n        ax2.plot(g, intervals_i[:,1], 'g', linestyle='dotted')\n        ax2.fill_between(g, intervals_i[:,0], intervals_i[:,1], color='green', alpha=0.2)\n\n        #ax2.legend(bbox_to_anchor=(1.0, 0.5), fontsize=16, loc='center left')\n        ax2.legend(fontsize=16, loc='upper right')\n\n        #label panels\n        ax1.text(0.94, 1.1, '(a)', fontsize=16)\n        ax2.text(0.94, 1.1, '(b)', fontsize=16)\n\n        plt.show()\n\n        #save figure option\n        # response = input('Would you like to save this figure? (yes/no)')\n\n        # if response == 'yes':\n        #     name = input('Enter a file name (include .jpg, .png, etc.)')\n        #     fig.savefig(name, bbox_inches='tight')\n\n        return None\n\n\n    def vertical_plot_fdagger(self, g1, g2, gp_mean1=np.zeros([2]), gp_mean2=np.zeros([2]), gp_var1=np.zeros([2]), gp_var2=np.zeros([2])):\n\n        r'''\n        Vertical panel plotter for the paper to generate two mixed model plots. \n\n        Example:\n            Bivariate.vertical_plot_fdagger(g1=np.linspace(1e-6, 0.5, 100), g2=np.linspace(1e-6,1.0,100),\n            gp_mean1=np.array([]), gp_mean2=np.array([]), gp_var1=np.array([,]), gp_var2=np.array([,]))\n\n        Parameters:\n            g1 (numpy.linspace): The space over which the models (and GP) were calculated \n                for panel a. \n\n            g2 (numpy.linspace): The space over which the models (and GP) were calculated \n                for panel b. \n\n            gp_mean1 (numpy.ndarray): GP mean results to be mixed with the models in panel a. \n                Optional. \n\n            gp_mean2 (numpy.ndarray): GP mean results to be mixed with the models in panel b. \n                Optional.\n\n            gp_var1 (numpy.ndarray): GP variance results for panel a. Optional.\n\n            gp_var2 (numpy.ndarray): GP variance results for panel b. Optional. \n\n        Returns:\n            None.\n        '''\n\n        #set up plot configuration\n        fig = plt.figure(figsize=(8,12), dpi=600)\n        gs = fig.add_gridspec(2,1, hspace=0, wspace=0)\n        (ax1, ax2) = gs.subplots(sharex='col', sharey='row')\n\n        for ax in (ax1, ax2):\n\n            ax.tick_params(axis='x', labelsize=18)\n            ax.tick_params(axis='y', labelsize=18)\n            ax.locator_params(nbins=5)\n            ax.xaxis.set_minor_locator(AutoMinorLocator())\n            ax.yaxis.set_minor_locator(AutoMinorLocator())\n            ax.set_xlim(0.0,1.0)\n            ax.set_ylim(1.2,3.2)\n            ax.set_yticks([1.4, 1.8, 2.2, 2.6, 3.0])\n\n            #labels and true model\n            ax.set_xlabel('g', fontsize=22)\n            ax.set_ylabel('F(g)', fontsize=22)\n            ax.plot(g1, self.m.true_model(g1), 'k', label='True model')\n\n            #only label outer plot axes\n            ax.label_outer()\n\n        #copy class variables to force changes\n        loworder = self.loworder.copy()\n        highorder = self.highorder.copy()\n\n        #call GP &amp; fdagger to calculate results (only works if both are nonzero)\n        if gp_mean1.any() and gp_mean2.any() != 0:\n\n            #mixed results (panel a)\n            self.loworder = np.array([loworder[0]])\n            self.highorder = np.array([highorder[0]])\n            self.m = Models(self.loworder, self.highorder)\n            mean_1, intervals_1, interval_low_1, interval_high_1 = self.fdagger(g1, \\\n                GP_mean=gp_mean1, GP_var=gp_var1) \n\n            #mixed results (panel b)\n            self.loworder = np.array([loworder[1]])\n            self.highorder = np.array([highorder[1]])\n            self.m = Models(self.loworder, self.highorder)\n            mean_2, intervals_2, interval_low_2, interval_high_2 = self.fdagger(g2, \\\n                GP_mean=gp_mean2, GP_var=gp_var2)\n\n        else:\n            #mixed results (panel a)\n            self.loworder = np.array([loworder[0]])\n            self.highorder = np.array([highorder[0]])\n            self.m = Models(self.loworder, self.highorder)\n            mean_1, intervals_1, interval_low_1, interval_high_1 = self.fdagger(g1)\n\n            #mixed results (panel b)\n            self.loworder = np.array([loworder[1]])\n            self.highorder = np.array([highorder[1]])\n            self.m = Models(self.loworder, self.highorder)\n            mean_2, intervals_2, interval_low_2, interval_high_2 = self.fdagger(g2)\n\n        #plot the small-g expansions and error bands (panel a)\n        self.loworder = np.array([loworder[0]])\n        self.m = Models(self.loworder, self.highorder)\n        ax1.plot(g1, self.m.low_g(g1)[0,:], 'r--', \\\n            label=r'$f_s$ ($N_s$ = {})'.format(self.loworder[0]))\n        ax1.plot(g1, interval_low_1[0, :, 0], 'r', linestyle='dotted',\\\n             label=r'$f_s$ {}\\% CI'.format(int(self.ci)))\n        ax1.plot(g1, interval_low_1[0, :, 1], 'r', linestyle='dotted')\n\n        #plot the small-g expansions and error bands (panel b)\n        self.loworder = np.array([loworder[1]])\n        self.m = Models(self.loworder, self.highorder)\n        ax2.plot(g2, self.m.low_g(g2)[0,:], color='r', linestyle='dashed',\\\n                label=r'$f_s$ ($N_s$ = {})'.format(self.loworder[0]))\n        ax2.plot(g2, interval_low_2[0, :, 0], color='r', linestyle='dotted', \\\n            label=r'$f_s$ {}\\% CI'.format(int(self.ci)))\n        ax2.plot(g2, interval_low_2[0, :, 1], color='r', linestyle='dotted')\n\n        #plot the large-g expansions and error bands (panel a)\n        self.highorder = np.array([highorder[0]])\n        self.m = Models(self.loworder, self.highorder)\n        ax1.plot(g1, self.m.high_g(g1)[0,:], 'b--', label=r'$f_l$ ($N_l$ = {})'.format(self.highorder[0]))\n        ax1.plot(g1, interval_high_1[0, :, 0], 'b', linestyle='dotted', label=r'$f_l$ {}\\% CI'.format(int(self.ci)))\n        ax1.plot(g1, interval_high_1[0, :, 1], 'b', linestyle='dotted')\n\n        #plot the large-g expansions and error bands (panel b)\n        self.highorder = np.array([highorder[1]])\n        self.m = Models(self.loworder, self.highorder)\n        ax2.plot(g2, self.m.high_g(g2)[0,:], color='b', linestyle='dashed', \\\n                label=r'$f_l$ ($N_l$ = {})'.format(self.highorder[0]))\n        ax2.plot(g2, interval_high_2[0, :, 0], color='b', linestyle='dotted', \\\n            label=r'$f_l$ {}\\% CI'.format(int(self.ci)))\n        ax2.plot(g2, interval_high_2[0, :, 1], color='b', linestyle='dotted')\n\n        #uninformative model case\n        ax1.plot(g1, mean_1, 'g', label='Mixed model')\n        ax1.plot(g1, intervals_1[:,0], 'g', linestyle='dotted', label=r'{}$\\%$ CI'.format(int(self.ci)))\n        ax1.plot(g1, intervals_1[:,1], 'g', linestyle='dotted')\n        ax1.fill_between(g1, intervals_1[:,0], intervals_1[:,1], color='green', alpha=0.2)\n\n        #informative model case\n        ax2.plot(g2, mean_2, 'g', label='Mixed model')\n        ax2.plot(g2, intervals_2[:,0], 'g', linestyle='dotted', label=r'{}$\\%$ CI'.format(int(self.ci)))\n        ax2.plot(g2, intervals_2[:,1], 'g', linestyle='dotted')\n        ax2.fill_between(g2, intervals_2[:,0], intervals_2[:,1], color='green', alpha=0.2)\n\n        #add panel labels\n        ax1.text(0.94, 1.3, '(a)', fontsize=18)\n        ax2.text(0.94, 1.3, '(b)', fontsize=18)\n\n        ax2.legend(fontsize=18, loc='upper right')\n        ax1.legend(fontsize=18, loc='upper right')\n\n        plt.show()\n\n        #save figure option\n        # response = input('Would you like to save this figure? (yes/no)')\n\n        # if response == 'yes':\n        #     name = input('Enter a file name (include .jpg, .png, etc.)')\n        #     fig.savefig(name, bbox_inches='tight')\n\n        return None\n</code></pre>"},{"location":"discrepancy/#samba.discrepancy.Bivariate.__init__","title":"<code>__init__(loworder, highorder, error_model='informative', ci=68)</code>","text":"<p>The bivariate BMM method used to construct the mixed model of two series expansions. This class contains the fdagger function and the plotter.</p> Example <p>Bivariate(loworder=5, highorder=10)</p> <p>Parameters:</p> Name Type Description Default <code>loworder</code> <code>(ndarray, int, float)</code> <p>The value of N_s to be  used to truncate the small-g expansion.</p> required <code>highorder</code> <code>(ndarray, int, float)</code> <p>The value of N_l to be  used to truncate the large-g expansion.</p> required <code>error_model</code> <code>str</code> <p>The error model to be used in the calculation.  Options are 'uninformative' and 'informative'. Default is 'informative'. </p> <code>'informative'</code> <code>ci</code> <code>int</code> <p>The value of the credibility interval desired (can be 68 or 95).</p> <code>68</code> <p>Returns:</p> Type Description <p>None.</p> Source code in <code>samba/discrepancy.py</code> <pre><code>def __init__(self, loworder, highorder, error_model='informative', ci=68):\n\n    r'''\n    The bivariate BMM method used to construct the mixed model of two series\n    expansions. This class contains the fdagger function and the plotter.\n\n    Example:\n        Bivariate(loworder=5, highorder=10)\n\n    Parameters:\n        loworder (numpy.ndarray, int, float): The value of N_s to be \n            used to truncate the small-g expansion.\n\n        highorder (numpy.ndarray, int, float): The value of N_l to be \n            used to truncate the large-g expansion.\n\n        error_model (str): The error model to be used in the calculation. \n            Options are 'uninformative' and 'informative'. Default is 'informative'. \n\n        ci (int): The value of the credibility interval desired (can be 68 or 95).\n\n    Returns:\n        None.\n    '''\n\n    #get interval\n    self.ci = ci\n\n    #instantiate the Uncertainties class and error model\n    self.u = Uncertainties(error_model)\n    self.error_model = self.u.error_model\n\n    #check type and assign class variables\n    if isinstance(loworder, float) == True or isinstance(loworder, int) == True:\n        loworder = np.array([loworder])\n\n    if isinstance(highorder, float) == True or isinstance(highorder, int) == True:\n        highorder = np.array([highorder])\n\n    self.loworder = loworder \n    self.highorder = highorder\n\n    #instantiate Models() class here\n    self.m = Models(self.loworder, self.highorder)\n\n    return None\n</code></pre>"},{"location":"discrepancy/#samba.discrepancy.Bivariate.fdagger","title":"<code>fdagger(g, GP_mean=np.zeros([2]), GP_var=np.zeros([2]))</code>","text":"<p>A do-it-all function to determine the pdf of the mixed model. Can use models  indicated by inputting arrays into the loworder and highorder variables, and accept GP mean and variance arrays in the GP_mean and GP_var options.</p> Example <p>Bivariate.fdagger(g=np.linspace(1e-6, 0.5, 100), GP_mean=np.array([]),      GP_var=np.array([]))</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>linspace</code> <p>The linspace over which this calculation is performed.</p> required <code>GP_mean</code> <code>ndarray</code> <p>An array of mean values from a Gaussian process  to be mixed in as a third model (optional).  </p> <code>zeros([2])</code> <code>GP_var</code> <code>ndarray</code> <p>An array of variances from a Gaussian process to  be mixed in as a third model (optional).</p> <code>zeros([2])</code> <p>Returns:</p> Name Type Description <code>mean</code> <code>ndarray</code> <p>The mixed model mean (either including a GP or not  depending on the function arguments).</p> <code>intervals</code> <code>ndarray</code> <p>The credibility interval of the mixed model mean.</p> <code>interval_low</code> <code>ndarray</code> <p>The variance interval for the small-g expansion  (calculated from the next order after the truncation). </p> <code>interval_high</code> <code>ndarray</code> <p>The variance interval for the large-g expansion  (calculated from the next order after the truncation).</p> Source code in <code>samba/discrepancy.py</code> <pre><code>def fdagger(self, g, GP_mean=np.zeros([2]), GP_var=np.zeros([2])): \n\n    r'''\n    A do-it-all function to determine the pdf of the mixed model. Can use models \n    indicated by inputting arrays into the loworder and highorder variables,\n    and accept GP mean and variance arrays in the GP_mean and GP_var options.\n\n    Example:\n        Bivariate.fdagger(g=np.linspace(1e-6, 0.5, 100), GP_mean=np.array([]), \n            GP_var=np.array([]))\n\n    Parameters:\n        g (numpy.linspace): The linspace over which this calculation is performed.\n\n        GP_mean (numpy.ndarray): An array of mean values from a Gaussian process \n            to be mixed in as a third model (optional).  \n\n        GP_var (numpy.ndarray): An array of variances from a Gaussian process to \n            be mixed in as a third model (optional).\n\n    Returns:\n        mean (numpy.ndarray): The mixed model mean (either including a GP or not \n            depending on the function arguments).\n\n        intervals (numpy.ndarray): The credibility interval of the mixed model mean.\n\n        interval_low (numpy.ndarray): The variance interval for the small-g expansion \n            (calculated from the next order after the truncation). \n\n        interval_high (numpy.ndarray): The variance interval for the large-g expansion \n            (calculated from the next order after the truncation).\n    '''\n\n    #check type\n    if isinstance(self.loworder, float) == True or isinstance(self.loworder, int) == True:\n        self.loworder = np.array([self.loworder])\n\n    if isinstance(self.highorder, float) == True or isinstance(self.highorder, int) == True:\n        self.highorder = np.array([self.highorder])\n\n    #uncertainties\n    v_low = np.asarray([self.u.variance_low(g, self.loworder[i]) for i in range(len(self.loworder))])\n    v_high = np.asarray([self.u.variance_high(g, self.highorder[i]) for i in range(len(self.highorder))])\n\n    #calculating models\n    f_low = np.asarray([self.m.low_g(g)[i,:] for i in range(len(self.loworder))])\n    f_high = np.asarray([self.m.high_g(g)[i,:] for i in range(len(self.highorder))])\n\n    #concatenate models and variances\n    if GP_mean.any() and GP_var.any() != 0:\n        f = np.concatenate((f_low, f_high, GP_mean.reshape(-1,1).T), axis=0)\n        v = np.concatenate((v_low, v_high, GP_var.reshape(-1,1).T), axis=0)\n    else:\n        f = np.concatenate((f_low, f_high), axis=0)\n        v = np.concatenate((v_low, v_high), axis=0)\n\n    #initialise arrays\n    mean_n = np.zeros([len(f), len(g)])\n    mean_d = np.zeros([len(f), len(g)])\n    mean = np.zeros([len(g)])\n    var = np.zeros([len(f), len(g)])\n\n    #create fdagger for each value of g\n    for i in range(len(f)):\n        mean_n[i] = f[i]/v[i]\n        mean_d[i] = 1.0/v[i]\n        var[i] = 1.0/v[i]\n\n    #save variances for each model\n    self.var_weights = var/(np.sum(var, axis=0))\n\n    mean_n = np.sum(mean_n, axis=0)\n    mean_d = np.sum(mean_d, axis=0)\n\n    #mean, variance calculation\n    mean = mean_n/mean_d\n    var = 1.0/np.sum(var, axis=0)\n\n    #which credibility interval to use\n    if self.ci == 68:\n        val = 1.0\n    elif self.ci == 95:\n        val = 1.96\n    else:\n        raise ValueError('Please enter either 68 or 95.')\n\n    #initialise credibility intervals\n    intervals = np.zeros([len(g), 2])\n    interval_low = np.zeros([len(self.loworder), len(g), 2])\n    interval_high = np.zeros([len(self.highorder), len(g), 2])\n\n    #calculate credibility intervals\n    intervals[:, 0] = (mean - val * np.sqrt(var))\n    intervals[:, 1] = (mean + val * np.sqrt(var))\n\n    for i in range(len(self.loworder)):\n        interval_low[i,:,0] = (self.m.low_g(g)[i,:] - val * np.sqrt(v_low[i,:]))\n        interval_low[i,:,1] = (self.m.low_g(g)[i,:] + val * np.sqrt(v_low[i,:]))\n\n    for i in range(len(self.highorder)):\n        interval_high[i,:,0] = (self.m.high_g(g)[i,:] - val * np.sqrt(v_high[i,:]))\n        interval_high[i,:,1] = (self.m.high_g(g)[i,:] + val * np.sqrt(v_high[i,:]))\n\n    return mean, intervals, interval_low, interval_high\n</code></pre>"},{"location":"discrepancy/#samba.discrepancy.Bivariate.plot_error_models","title":"<code>plot_error_models(g)</code>","text":"<p>A plotter to compare the uninformative error model results of two models  to the informative error model results for the same two models. Panel a refers to the uninformative error model panel in the subplot, and panel b corresponds to the informative error model panel. </p> Example <p>Bivariate.plot_error_models(g=np.linspace(1e-6, 0.5, 100))</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>linspace</code> <p>The space over which the models are calculated.</p> required <p>Returns:</p> Type Description <p>None.</p> Source code in <code>samba/discrepancy.py</code> <pre><code>def plot_error_models(self, g): \n\n    r'''\n    A plotter to compare the uninformative error model results of two models \n    to the informative error model results for the same two models. Panel a\n    refers to the uninformative error model panel in the subplot, and panel b\n    corresponds to the informative error model panel. \n\n    Example:\n        Bivariate.plot_error_models(g=np.linspace(1e-6, 0.5, 100))\n\n    Parameters:\n        g (numpy.linspace): The space over which the models are calculated.\n\n    Returns:\n        None.\n    '''\n\n    #set up plot configuration\n    fig = plt.figure(figsize=(16,6), dpi=600)\n    gs = fig.add_gridspec(1, 2, hspace=0, wspace=0)\n    (ax1, ax2) = gs.subplots(sharex='col', sharey='row')\n\n    for ax in (ax1, ax2):\n\n        ax.tick_params(axis='x', labelsize=18)\n        ax.tick_params(axis='y', labelsize=18)\n        ax.locator_params(nbins=5)\n        ax.xaxis.set_minor_locator(AutoMinorLocator())\n        ax.yaxis.set_minor_locator(AutoMinorLocator())\n        ax.set_xlim(0.0,1.0)\n        ax.set_xticks([0.1, 0.3, 0.5, 0.7, 0.9])\n        ax.set_ylim(1.0,3.0)\n\n        #labels and true model\n        ax.set_xlabel('g', fontsize=22)\n        ax.set_ylabel('F(g)', fontsize=22)\n        ax.plot(g, self.m.true_model(g), 'k', label='True model')\n\n        #only label outer plot axes\n        ax.label_outer()\n\n    #call fdagger to calculate results (overwrite class variable)\n    self.u = Uncertainties(error_model='uninformative')\n    self.error_model = self.u.error_model\n    mean_u, intervals_u, interval_low_u, interval_high_u = self.fdagger(g)\n    self.u = Uncertainties(error_model='informative')\n    self.error_model = self.u.error_model \n    mean_i, intervals_i, interval_low_i, interval_high_i = self.fdagger(g)\n\n    #plot the small-g expansions and error bands (panel a)\n    ax1.plot(g, self.m.low_g(g)[0,:], 'r--', \\\n        label=r'$f_s$ ($N_s$ = {})'.format(self.loworder[0]))\n    ax1.plot(g, interval_low_u[0, :, 0], 'r', linestyle='dotted',\\\n         label=r'$f_s$ ($N_s$ = {}) {}\\% CI'.format(self.loworder[0], int(self.ci)))\n    ax1.plot(g, interval_low_u[0, :, 1], 'r', linestyle='dotted')\n\n    #plot the small-g expansions and error bands (panel b)\n    ax2.plot(g, self.m.low_g(g)[0,:], 'r--', label=r'$f_s$ ($N_s$ = {})'.format(self.loworder[0]))\n    ax2.plot(g, interval_low_i[0, :, 0], 'r', linestyle='dotted',\\\n            label=r'$f_s$ {}\\% CI'.format(int(self.ci)))\n    ax2.plot(g, interval_low_i[0, :, 1], 'r', linestyle='dotted')\n\n    #plot the large-g expansions and error bands (panel a)\n    ax1.plot(g, self.m.high_g(g)[0,:], 'b--', label=r'$f_l$ ($N_l$ = {})'.format(self.highorder[0]))\n    ax1.plot(g, interval_high_u[0, :, 0], 'b', linestyle='dotted', label=r'$f_l$ {}\\% CI'.format(int(self.ci)))\n    ax1.plot(g, interval_high_u[0, :, 1], 'b', linestyle='dotted')\n\n    #plot the large-g expansions and error bands (panel b)\n    ax2.plot(g, self.m.high_g(g)[0,:], 'b--', label=r'$f_l$ ($N_l$ = {})'.format(self.highorder[0]))\n    ax2.plot(g, interval_high_i[0, :, 0], 'b', linestyle='dotted', \\\n        label=r'$f_l$ {}\\% CI'.format(int(self.ci)))\n    ax2.plot(g, interval_high_i[0, :, 1], 'b', linestyle='dotted')\n\n    #PPD (panel a)\n    ax1.plot(g, mean_u, 'g', label='Mixed model')\n    ax1.plot(g, intervals_u[:,0], 'g', linestyle='dotted', label=r'{}$\\%$ CI'.format(int(self.ci)))\n    ax1.plot(g, intervals_u[:,1], 'g', linestyle='dotted')\n    ax1.fill_between(g, intervals_u[:,0], intervals_u[:,1], color='green', alpha=0.2)\n\n    #PPD (panel b)\n    ax2.plot(g, mean_i, 'g', label='Mixed model')\n    ax2.plot(g, intervals_i[:,0], 'g', linestyle='dotted', label=r'{}$\\%$ CI'.format(int(self.ci)))\n    ax2.plot(g, intervals_i[:,1], 'g', linestyle='dotted')\n    ax2.fill_between(g, intervals_i[:,0], intervals_i[:,1], color='green', alpha=0.2)\n\n    #ax2.legend(bbox_to_anchor=(1.0, 0.5), fontsize=16, loc='center left')\n    ax2.legend(fontsize=16, loc='upper right')\n\n    #label panels\n    ax1.text(0.94, 1.1, '(a)', fontsize=16)\n    ax2.text(0.94, 1.1, '(b)', fontsize=16)\n\n    plt.show()\n\n    #save figure option\n    # response = input('Would you like to save this figure? (yes/no)')\n\n    # if response == 'yes':\n    #     name = input('Enter a file name (include .jpg, .png, etc.)')\n    #     fig.savefig(name, bbox_inches='tight')\n\n    return None\n</code></pre>"},{"location":"discrepancy/#samba.discrepancy.Bivariate.plot_mix","title":"<code>plot_mix(g, plot_fdagger=True, plot_true=True, GP_mean=np.zeros([2]), GP_var=np.zeros([2]))</code>","text":"<p>An all-in-one plotting function that will plot the results of fdagger for N numbers of models, the next orders of the expansion models, and the validation step of the  model mixing in fdagger to test fdagger results.</p> Example <p>Bivariate.plot_mix(g=np.linspace(1e-6, 0.5, 100), plot_fdagger=True)</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>linspace</code> <p>The space over which the models are calculated.</p> required <code>plot_fdagger</code> <code>bool</code> <p>If True, this parameter will allow for the  plotting of fdagger and its credibility interval. </p> <code>True</code> <code>plot_true</code> <code>bool</code> <p>Determines whether or not to plot the true model curve.  Default is True. </p> <code>True</code> <code>GP_mean</code> <code>ndarray</code> <p>The mean array from the GP being included. </p> <code>zeros([2])</code> <code>GP_var</code> <code>ndarray</code> <p>The variance array from the GP being included.</p> <code>zeros([2])</code> <p>Returns:</p> Name Type Description <code>mean</code> <code>ndarray</code> <p>The mean of the mixed model at each point in g.</p> <code>intervals</code> <code>ndarray</code> <p>The values of the credibility intervals at each point in g.</p> Source code in <code>samba/discrepancy.py</code> <pre><code>    def plot_mix(self, g, plot_fdagger=True, plot_true=True, GP_mean=np.zeros([2]), GP_var=np.zeros([2])):\n\n        r'''\n        An all-in-one plotting function that will plot the results of fdagger for N numbers\n        of models, the next orders of the expansion models, and the validation step of the \n        model mixing in fdagger to test fdagger results.\n\n        Example:\n            Bivariate.plot_mix(g=np.linspace(1e-6, 0.5, 100), plot_fdagger=True)\n\n        Parameters:\n            g (numpy.linspace): The space over which the models are calculated.\n\n            plot_fdagger (bool): If True, this parameter will allow for the \n                plotting of fdagger and its credibility interval. \n\n            plot_true (bool): Determines whether or not to plot the true model curve. \n                Default is True. \n\n            GP_mean (numpy.ndarray): The mean array from the GP being included. \n\n            GP_var (numpy.ndarray): The variance array from the GP being included.\n\n        Returns:\n            mean (numpy.ndarray): The mean of the mixed model at each point in g.\n\n            intervals (numpy.ndarray): The values of the credibility intervals at each\n                point in g. \n        '''\n\n        #set up plot configuration\n        fig = plt.figure(figsize=(8,6), dpi=600)\n        ax = plt.axes()\n        ax.tick_params(axis='x', labelsize=18)\n        ax.tick_params(axis='y', labelsize=18)\n        ax.locator_params(nbins=8)\n        ax.xaxis.set_minor_locator(AutoMinorLocator())\n        ax.yaxis.set_minor_locator(AutoMinorLocator())\n\n        #set up x and y limits\n        ax.set_xlim(0.0,1.0)\n       # ax.set_xlim(0.0,0.5)\n        ax.set_ylim(1.2,3.2)\n       # ax.set_ylim(1.0,3.0)\n        ax.set_yticks([1.2, 1.6, 2.0, 2.4, 2.8, 3.2])\n      #  ax.set_ylim(2.0,2.8)\n       # ax.set_yticks([2.0, 2.2, 2.4, 2.6, 2.8])\n\n        #labels and true model\n        ax.set_xlabel('g', fontsize=22)\n        ax.set_ylabel('F(g)', fontsize=22)\n\n        if plot_true is True:\n            ax.plot(g, self.m.true_model(g), 'k', label='True model')\n\n        #call fdagger to calculate results\n        if GP_mean.any() and GP_var.any() != 0:\n            mean, intervals, interval_low, interval_high = self.fdagger(g, GP_mean, GP_var)\n\n        else:\n            mean, intervals, interval_low, interval_high = self.fdagger(g)\n\n        #plot the small-g expansions and error bands\n        for i,j in zip(range(len(self.loworder)), self.loworder):\n            ax.plot(g, self.m.low_g(g)[i,:], 'r--', label=r'$f_s$ ($N_s$ = {})'.format(j))\n\n        for i in range(len(self.loworder)):\n            ax.plot(g, interval_low[i, :, 0], 'r', linestyle='dotted', \\\n                label=r'$f_s$ ($N_s$ = {}) {}\\% CI'.format(self.loworder[i], int(self.ci)))\n            ax.plot(g, interval_low[i, :, 1], 'r', linestyle='dotted')\n\n        #for each large-g order, calculate and plot\n        for i,j in zip(range(len(self.highorder)), self.highorder):\n            ax.plot(g, self.high_g(g)[i,:], 'b--', label=r'$f_l$ ($N_l$ = {})'.format(j))\n\n        for i in range(len(self.highorder)):\n            ax.plot(g, interval_high[i, :, 0], 'b', linestyle='dotted', \\\n                label=r'$f_l$ ($N_l$ = {}) {}\\% CI'.format(self.highorder[i], int(self.ci)))\n            ax.plot(g, interval_high[i, :, 1], 'b', linestyle='dotted')\n\n        # GP\n#         if GP_mean.any() and GP_var.any() != 0:\n#             ax.plot(g, GP_mean, 'm')\n#             ax.fill_between(g, GP_mean-np.sqrt(GP_var), GP_mean+np.sqrt(GP_var), color='m', alpha=0.2)\n\n        if plot_fdagger == True:\n            ax.plot(g, mean, 'g', label='Mean')\n            ax.plot(g, intervals[:,0], 'g', linestyle='dotted', label=r'{}$\\%$ CI'.format(int(self.ci)))\n            ax.plot(g, intervals[:,1], 'g', linestyle='dotted')\n            ax.fill_between(g, intervals[:,0], intervals[:,1], color='green', alpha=0.2)\n\n        ax.legend(fontsize=16, loc='upper right')\n        plt.show()\n\n        # #save figure option\n        # response = input('Would you like to save this figure? (yes/no)')\n\n        # if response == 'yes':\n        #     name = input('Enter a file name (include .jpg, .png, etc.)')\n        #     fig.savefig(name, bbox_inches='tight')\n\n        return mean, intervals \n</code></pre>"},{"location":"discrepancy/#samba.discrepancy.Bivariate.subplot_mix","title":"<code>subplot_mix(g, GP_mean=np.zeros([2]), GP_var=np.zeros([2]), log=False)</code>","text":"<p>An all-in-one plotting function that will plot the results of fdagger for N numbers of models side-by-side with the 2 model case to compare. Currently used to plot the GP results alongside those without the GP; N models case not color-coded yet.  </p> Example <p>Bivariate.subplot_mix(g=np.linspace(1e-6, 0.5, 100))</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>linspace</code> <p>The space over which the models are calculated.</p> required <code>GP_mean</code> <code>ndarray</code> <p>An array of GP PPD results (that MUST be at  input points in g) to be mixed in with the expansions chosen. Optional. </p> <code>zeros([2])</code> <code>GP_var</code> <code>ndarray</code> <p>An array of GP variance results (that MUST be at  input points in g) to be mixed in with the expansions chosen. Optional. </p> <code>zeros([2])</code> <code>log</code> <code>bool</code> <p>A toggle for logscale. Default is False. </p> <code>False</code> <p>Returns:</p> Type Description <p>None.</p> Source code in <code>samba/discrepancy.py</code> <pre><code>def subplot_mix(self, g, GP_mean=np.zeros([2]), GP_var=np.zeros([2]), log=False): \n\n    r'''\n    An all-in-one plotting function that will plot the results of fdagger for N numbers\n    of models side-by-side with the 2 model case to compare. Currently used to plot the GP\n    results alongside those without the GP; N models case not color-coded yet.  \n\n    Example:\n        Bivariate.subplot_mix(g=np.linspace(1e-6, 0.5, 100))\n\n    Parameters:\n        g (numpy.linspace): The space over which the models are calculated.\n\n        GP_mean (numpy.ndarray): An array of GP PPD results (that MUST be at \n            input points in g) to be mixed in with the expansions chosen. Optional. \n\n        GP_var (numpy.ndarray): An array of GP variance results (that MUST be at \n            input points in g) to be mixed in with the expansions chosen. Optional. \n\n        log (bool): A toggle for logscale. Default is False. \n\n    Returns:\n        None.\n    '''\n\n    #set up plot configuration\n    fig = plt.figure(figsize=(16,6), dpi=600)\n    gs = fig.add_gridspec(1, 2, hspace=0, wspace=0)\n    (ax1, ax2) = gs.subplots(sharex='col', sharey='row')\n\n    for ax in (ax1, ax2):\n\n        ax.tick_params(axis='x', labelsize=18)\n        ax.tick_params(axis='y', labelsize=18)\n        ax.locator_params(nbins=5)\n        ax.xaxis.set_minor_locator(AutoMinorLocator())\n        ax.yaxis.set_minor_locator(AutoMinorLocator())\n        ax.set_xlim(0.0,1.0)\n        ax.set_xticks([0.1, 0.3, 0.5, 0.7, 0.9])\n        ax.set_ylim(1.0,3.0)\n\n        #labels and true model\n        ax.set_xlabel('g', fontsize=22)\n        ax.set_ylabel('F(g)', fontsize=22)\n        ax.plot(g, self.m.true_model(g), 'k', label='True model')\n\n        #only label outer plot axes\n        ax.label_outer()\n\n    #log scale option\n    if log is True:\n        for ax in (ax1, ax2):\n            ax.set_yscale('log')\n            ax.set_ylim(1e-2, 10.0)\n\n    #call fdagger to calculate results\n    mean2, intervals2, _, _ = self.fdagger(g)\n\n    if GP_mean.any() and GP_var.any() != 0:\n        mean, intervals, interval_low, interval_high = self.fdagger(g, GP_mean, GP_var)\n\n    else:\n        mean, intervals, interval_low, interval_high = self.fdagger(g)\n\n    #plot the small-g expansions and error bands (panel a)\n    for i,j in zip(range(len(self.loworder)), self.loworder):\n        ax1.plot(g, self.m.low_g(g)[i,:], 'r--', \\\n            label=r'$f_s$ ($N_s$ = {})'.format(j))\n        ax1.plot(g, interval_low[i, :, 0], 'r', linestyle='dotted',\\\n            label=r'$f_s$ {}\\% CI'.format(int(self.ci)))\n        ax1.plot(g, interval_low[i, :, 1], 'r', linestyle='dotted')\n\n    #plot the small-g expansions and error bands (panel b)\n    for i,j in zip(range(len(self.loworder)), self.loworder):\n        ax2.plot(g, self.m.low_g(g)[i,:], 'r--', label=r'$f_s$ ($N_s$ = {})'.format(j))\n        ax2.plot(g, interval_low[i, :, 0], 'r', linestyle='dotted',\\\n                label=r'$f_s$ {}\\% CI'.format(int(self.ci)))\n        ax2.plot(g, interval_low[i, :, 1], 'r', linestyle='dotted')\n\n    #plot the large-g expansions and error bands (panel a)\n    for i,j in zip(range(len(self.highorder)), self.highorder):\n        ax1.plot(g, self.m.high_g(g)[i,:], 'b--', label=r'$f_l$ ($N_l$ = {})'.format(j))\n        ax1.plot(g, interval_high[i, :, 0], 'b', linestyle='dotted', \\\n            label=r'$f_l$ {}\\% CI'.format(int(self.ci)))\n        ax1.plot(g, interval_high[i, :, 1], 'b', linestyle='dotted')\n\n    #plot the large-g expansions and error bands (panel b)\n    for i,j in zip(range(len(self.highorder)), self.highorder):\n        ax2.plot(g, self.m.high_g(g)[i,:], 'b--', label=r'$f_l$ ($N_l$ = {})'.format(j))\n        ax2.plot(g, interval_high[i, :, 0], 'b', linestyle='dotted', \\\n                label=r'$f_l$ {}\\% CI'.format(int(self.ci)))\n        ax2.plot(g, interval_high[i, :, 1], 'b', linestyle='dotted')\n\n    #2 model case\n    ax1.plot(g, mean2, 'g', label='Mixed model')\n    ax1.plot(g, intervals2[:,0], 'g', linestyle='dotted', label=r'{}$\\%$ CI'.format(int(self.ci)))\n    ax1.plot(g, intervals2[:,1], 'g', linestyle='dotted')\n    ax1.fill_between(g, intervals2[:,0], intervals2[:,1], color='green', alpha=0.2)\n\n    #N model case\n    ax2.plot(g, mean, 'g', label='Mixed model')\n    ax2.plot(g, intervals[:,0], 'g', linestyle='dotted', label=r'{}$\\%$ CI'.format(int(self.ci)))\n    ax2.plot(g, intervals[:,1], 'g', linestyle='dotted')\n    ax2.fill_between(g, intervals[:,0], intervals[:,1], color='green', alpha=0.2)\n\n    #ax2.legend(bbox_to_anchor=(1.0, 0.5), fontsize=12, loc='center left')\n    ax2.legend(fontsize=16, loc='upper right')\n\n    #label panels\n    ax1.text(0.94, 1.1, '(a)', fontsize=16)\n    ax2.text(0.94, 1.1, '(b)', fontsize=16)\n\n    plt.show()\n\n    #save figure option\n    # response = input('Would you like to save this figure? (yes/no)')\n\n    # if response == 'yes':\n    #     name = input('Enter a file name (include .jpg, .png, etc.)')\n    #     fig.savefig(name, bbox_inches='tight')\n\n    return None\n</code></pre>"},{"location":"discrepancy/#samba.discrepancy.Bivariate.vertical_plot_fdagger","title":"<code>vertical_plot_fdagger(g1, g2, gp_mean1=np.zeros([2]), gp_mean2=np.zeros([2]), gp_var1=np.zeros([2]), gp_var2=np.zeros([2]))</code>","text":"<p>Vertical panel plotter for the paper to generate two mixed model plots. </p> Example <p>Bivariate.vertical_plot_fdagger(g1=np.linspace(1e-6, 0.5, 100), g2=np.linspace(1e-6,1.0,100), gp_mean1=np.array([]), gp_mean2=np.array([]), gp_var1=np.array([,]), gp_var2=np.array([,]))</p> <p>Parameters:</p> Name Type Description Default <code>g1</code> <code>linspace</code> <p>The space over which the models (and GP) were calculated  for panel a. </p> required <code>g2</code> <code>linspace</code> <p>The space over which the models (and GP) were calculated  for panel b. </p> required <code>gp_mean1</code> <code>ndarray</code> <p>GP mean results to be mixed with the models in panel a.  Optional. </p> <code>zeros([2])</code> <code>gp_mean2</code> <code>ndarray</code> <p>GP mean results to be mixed with the models in panel b.  Optional.</p> <code>zeros([2])</code> <code>gp_var1</code> <code>ndarray</code> <p>GP variance results for panel a. Optional.</p> <code>zeros([2])</code> <code>gp_var2</code> <code>ndarray</code> <p>GP variance results for panel b. Optional. </p> <code>zeros([2])</code> <p>Returns:</p> Type Description <p>None.</p> Source code in <code>samba/discrepancy.py</code> <pre><code>def vertical_plot_fdagger(self, g1, g2, gp_mean1=np.zeros([2]), gp_mean2=np.zeros([2]), gp_var1=np.zeros([2]), gp_var2=np.zeros([2])):\n\n    r'''\n    Vertical panel plotter for the paper to generate two mixed model plots. \n\n    Example:\n        Bivariate.vertical_plot_fdagger(g1=np.linspace(1e-6, 0.5, 100), g2=np.linspace(1e-6,1.0,100),\n        gp_mean1=np.array([]), gp_mean2=np.array([]), gp_var1=np.array([,]), gp_var2=np.array([,]))\n\n    Parameters:\n        g1 (numpy.linspace): The space over which the models (and GP) were calculated \n            for panel a. \n\n        g2 (numpy.linspace): The space over which the models (and GP) were calculated \n            for panel b. \n\n        gp_mean1 (numpy.ndarray): GP mean results to be mixed with the models in panel a. \n            Optional. \n\n        gp_mean2 (numpy.ndarray): GP mean results to be mixed with the models in panel b. \n            Optional.\n\n        gp_var1 (numpy.ndarray): GP variance results for panel a. Optional.\n\n        gp_var2 (numpy.ndarray): GP variance results for panel b. Optional. \n\n    Returns:\n        None.\n    '''\n\n    #set up plot configuration\n    fig = plt.figure(figsize=(8,12), dpi=600)\n    gs = fig.add_gridspec(2,1, hspace=0, wspace=0)\n    (ax1, ax2) = gs.subplots(sharex='col', sharey='row')\n\n    for ax in (ax1, ax2):\n\n        ax.tick_params(axis='x', labelsize=18)\n        ax.tick_params(axis='y', labelsize=18)\n        ax.locator_params(nbins=5)\n        ax.xaxis.set_minor_locator(AutoMinorLocator())\n        ax.yaxis.set_minor_locator(AutoMinorLocator())\n        ax.set_xlim(0.0,1.0)\n        ax.set_ylim(1.2,3.2)\n        ax.set_yticks([1.4, 1.8, 2.2, 2.6, 3.0])\n\n        #labels and true model\n        ax.set_xlabel('g', fontsize=22)\n        ax.set_ylabel('F(g)', fontsize=22)\n        ax.plot(g1, self.m.true_model(g1), 'k', label='True model')\n\n        #only label outer plot axes\n        ax.label_outer()\n\n    #copy class variables to force changes\n    loworder = self.loworder.copy()\n    highorder = self.highorder.copy()\n\n    #call GP &amp; fdagger to calculate results (only works if both are nonzero)\n    if gp_mean1.any() and gp_mean2.any() != 0:\n\n        #mixed results (panel a)\n        self.loworder = np.array([loworder[0]])\n        self.highorder = np.array([highorder[0]])\n        self.m = Models(self.loworder, self.highorder)\n        mean_1, intervals_1, interval_low_1, interval_high_1 = self.fdagger(g1, \\\n            GP_mean=gp_mean1, GP_var=gp_var1) \n\n        #mixed results (panel b)\n        self.loworder = np.array([loworder[1]])\n        self.highorder = np.array([highorder[1]])\n        self.m = Models(self.loworder, self.highorder)\n        mean_2, intervals_2, interval_low_2, interval_high_2 = self.fdagger(g2, \\\n            GP_mean=gp_mean2, GP_var=gp_var2)\n\n    else:\n        #mixed results (panel a)\n        self.loworder = np.array([loworder[0]])\n        self.highorder = np.array([highorder[0]])\n        self.m = Models(self.loworder, self.highorder)\n        mean_1, intervals_1, interval_low_1, interval_high_1 = self.fdagger(g1)\n\n        #mixed results (panel b)\n        self.loworder = np.array([loworder[1]])\n        self.highorder = np.array([highorder[1]])\n        self.m = Models(self.loworder, self.highorder)\n        mean_2, intervals_2, interval_low_2, interval_high_2 = self.fdagger(g2)\n\n    #plot the small-g expansions and error bands (panel a)\n    self.loworder = np.array([loworder[0]])\n    self.m = Models(self.loworder, self.highorder)\n    ax1.plot(g1, self.m.low_g(g1)[0,:], 'r--', \\\n        label=r'$f_s$ ($N_s$ = {})'.format(self.loworder[0]))\n    ax1.plot(g1, interval_low_1[0, :, 0], 'r', linestyle='dotted',\\\n         label=r'$f_s$ {}\\% CI'.format(int(self.ci)))\n    ax1.plot(g1, interval_low_1[0, :, 1], 'r', linestyle='dotted')\n\n    #plot the small-g expansions and error bands (panel b)\n    self.loworder = np.array([loworder[1]])\n    self.m = Models(self.loworder, self.highorder)\n    ax2.plot(g2, self.m.low_g(g2)[0,:], color='r', linestyle='dashed',\\\n            label=r'$f_s$ ($N_s$ = {})'.format(self.loworder[0]))\n    ax2.plot(g2, interval_low_2[0, :, 0], color='r', linestyle='dotted', \\\n        label=r'$f_s$ {}\\% CI'.format(int(self.ci)))\n    ax2.plot(g2, interval_low_2[0, :, 1], color='r', linestyle='dotted')\n\n    #plot the large-g expansions and error bands (panel a)\n    self.highorder = np.array([highorder[0]])\n    self.m = Models(self.loworder, self.highorder)\n    ax1.plot(g1, self.m.high_g(g1)[0,:], 'b--', label=r'$f_l$ ($N_l$ = {})'.format(self.highorder[0]))\n    ax1.plot(g1, interval_high_1[0, :, 0], 'b', linestyle='dotted', label=r'$f_l$ {}\\% CI'.format(int(self.ci)))\n    ax1.plot(g1, interval_high_1[0, :, 1], 'b', linestyle='dotted')\n\n    #plot the large-g expansions and error bands (panel b)\n    self.highorder = np.array([highorder[1]])\n    self.m = Models(self.loworder, self.highorder)\n    ax2.plot(g2, self.m.high_g(g2)[0,:], color='b', linestyle='dashed', \\\n            label=r'$f_l$ ($N_l$ = {})'.format(self.highorder[0]))\n    ax2.plot(g2, interval_high_2[0, :, 0], color='b', linestyle='dotted', \\\n        label=r'$f_l$ {}\\% CI'.format(int(self.ci)))\n    ax2.plot(g2, interval_high_2[0, :, 1], color='b', linestyle='dotted')\n\n    #uninformative model case\n    ax1.plot(g1, mean_1, 'g', label='Mixed model')\n    ax1.plot(g1, intervals_1[:,0], 'g', linestyle='dotted', label=r'{}$\\%$ CI'.format(int(self.ci)))\n    ax1.plot(g1, intervals_1[:,1], 'g', linestyle='dotted')\n    ax1.fill_between(g1, intervals_1[:,0], intervals_1[:,1], color='green', alpha=0.2)\n\n    #informative model case\n    ax2.plot(g2, mean_2, 'g', label='Mixed model')\n    ax2.plot(g2, intervals_2[:,0], 'g', linestyle='dotted', label=r'{}$\\%$ CI'.format(int(self.ci)))\n    ax2.plot(g2, intervals_2[:,1], 'g', linestyle='dotted')\n    ax2.fill_between(g2, intervals_2[:,0], intervals_2[:,1], color='green', alpha=0.2)\n\n    #add panel labels\n    ax1.text(0.94, 1.3, '(a)', fontsize=18)\n    ax2.text(0.94, 1.3, '(b)', fontsize=18)\n\n    ax2.legend(fontsize=18, loc='upper right')\n    ax1.legend(fontsize=18, loc='upper right')\n\n    plt.show()\n\n    #save figure option\n    # response = input('Would you like to save this figure? (yes/no)')\n\n    # if response == 'yes':\n    #     name = input('Enter a file name (include .jpg, .png, etc.)')\n    #     fig.savefig(name, bbox_inches='tight')\n\n    return None\n</code></pre>"},{"location":"fprdat/","title":"Fractional Power of Rational function method (FPR) data","text":"<p>This class contains the data for the results of the FPR method used in Honda. It obtains a very precise mean function, but is unable to generate the uncertainty bands that Bayesian model mixing can achieve. It is used  as a comparison in the paper published with this package.</p>"},{"location":"fprdat/#samba.fprdat.FPR","title":"<code>FPR</code>","text":"<p>               Bases: <code>Models</code></p> Source code in <code>samba/fprdat.py</code> <pre><code>class FPR(Models):\n\n    def __init__(self, g, loworder, highorder):\n\n        r'''\n        A class to calculate the FPR method curves for comparison\n        to the mixed models in the three BMM methods of this package.\n\n        Example:\n            FPR(g=np.linspace(1e-6,1.0,100), loworder=np.array([5]),\n                highorder=np.array([5]))\n\n        Parameters:\n            g (numpy.linspace): The input space array over which the models are \n                mixed.\n\n            loworder (numpy.ndarray): The highest order considered in the small-g \n                expansion.\n\n            highorder (numpy.ndarray): The highest order considered in the large-g \n                expansion.\n\n        Returns:\n            None.\n        '''\n\n        self.g = g\n        self.loworder = loworder\n        self.highorder = highorder \n\n        #instantiate Models() class here\n        self.m = Models(self.loworder, self.highorder)\n\n        return None\n\n\n    def fprset(self, key):\n\n        r'''\n        Call the proper FPR function desired and obtain \n        an array of the results in the input space, g. \n\n        Example: \n            FPR.fprset(key='(2,4)^(1/8)')\n\n        Parameters:\n            key (str): The preferred FPR function. Enter a key in the\n                convention: '(m,n)^(\\alpha)', where m,n are orders\n                less than or equal to N_s and N_l (loworder, highorder\n                in the other classes). \\alpha is the value the FPR is \n                raised to in Eq. (2.7) (Honda 2014). \n\n        Returns:\n            fpr (numpy.ndarray): Results of the FPR function in an array. \n        '''\n\n        #if statement for calling the proper FPR function\n        fpr = np.zeros(len(self.g))\n\n        self.keyvalue = key\n\n        if key == '(0,0)^(1/2)':\n\n            #FPR[1/(2*1),0,0,g_]\n            fpr = np.sqrt(2 * np.pi * np.sqrt(1/(1 + (8 * self.g * np.pi)/sp.gamma(1/4)**2)))  \n\n        elif key == '(1,1)^(1/2)':  \n\n            #FPR[1/(2*1),1,1,g_] \n            fpr = np.sqrt(2 * np.pi * sp.gamma(1/4)) * np.sqrt((2 * np.pi * sp.gamma(-(1/4)) + 8 * \\\n                  self.g * np.pi * sp.gamma(1/4) + sp.gamma(1/4)**3) / (64 * self.g**2 * np.pi**2 + 2 * \\\n                  np.pi * sp.gamma(-(1/4)) * sp.gamma(1/4) + 8 * self.g * \\\n                  np.pi * sp.gamma(1/4)**2 + sp.gamma(1/4)**4))\n\n        elif key == '(2,2)^(1/2)':\n            #FPR[1/(2*1),2,2,g_] \n            fpr = 2.5066282746310002 * np.sqrt((1. + 10.1531607808241 * self.g + 37.91166947810798 * \\\n                  self.g**2) / (1. + 10.1531607808241 * self.g + 43.91166947810798 * \\\n                  self.g**2 + 72.48541321812907 * self.g**3))\n\n        elif key == '(3,3)^(1/2)':\n            #FPR[1/(2*1),3,3,g_] \n            fpr = 2.5066282746310002 * np.sqrt((1. + 16.030388229931486 * self.g + 110.26077245790317 * \\\n                  self.g**2 + 324.0187225963292 * self.g**3) / (1. + 16.030388229931486 * \\\n                  self.g + 116.26077245790317 * self.g**2 + 420.2010519759181 * \\\n                  self.g**3 + 619.509278307239 * self.g**4))\n\n        elif key == '(4,4)^(1/2)':\n            #FPR[1/(2*1),4,4,g_] \n            fpr = 2.5066282746310002 * np.sqrt(( 1. + 22.874503929271544 * self.g + 238.23905635876318 * \\\n                  self.g**2 + 1303.4929582331083 * self.g**3 + 3224.5631655188554 * self.g**4)/ \\\n                  ( 1. + 22.874503929271544 * self.g + 244.23905635876318 * self.g**2 + 1440.7399818087376 * \\\n                  self.g**3 + 4575.997503671434 * self.g**4 + 6165.220279617642 * self.g**5))\n\n        elif key == '(1,1)^(1/6)':\n            #FPR[1/(2*3),1,1,g_] \n            fpr = 2.5066282746310002 * (1/( 1. + 7.086913042848253 * self.g**2 + 6.989291097242859 * \\\n                  self.g**3))**(1/6)\n\n        elif key == '(2,2)^(1/6)':\n            #FPR[1/(2*3),2,2,g_] \n            fpr = 2.5066282746310002 * (( 1. + 3.7875802399388747 * self.g)/( 1. + 3.7875802399388747 * \\\n                  self.g + 18.000000000000007 * self.g**2 + 33.83154290049999 * self.g**3 + 26.47250085109775 * \\\n                  self.g**4))**(1/6)\n\n        elif key == '(3,3)^(1/6)':\n            #FPR[1/(2*3),3,3,g_] \n            fpr = 2.5066282746310002 * (( 1. + 8.099704591178746 * self.g + 28.252523713142793 * \\\n                  self.g**2)/( 1. + 8.099704591178746 * self.g + 46.25252371314279 * \\\n                  self.g**2 + 145.7946826412175 * self.g**3 + 256.83437198547387 * \\\n                  self.g**4 + 197.46511246291166 * self.g**5))**(1/6)\n\n        elif key == '(4,4)^(1/6)':\n            #FPR[1/(2*3),4,4,g_]\n            fpr = 2.5066282746310002 * (( 1. + 13.470628417091639 * self.g + 87.53888949233587 * \\\n                  self.g**2 + 255.1535591483978 * self.g**3)/( 1. + 13.470628417091639 * \\\n                  self.g + 105.53888949233587 * self.g**2 + 497.6248706560473 * self.g**3 + 1449.7000108620534 * \\\n                  self.g**4 + 2420.0858672492423 * self.g**5 + 1783.3424993857257 * self.g**6))**(1/6)\n\n        elif key == '(2,2)^(1/10)':\n            #FPR[1/(2*5),2,2,g_] \n            fpr = 2.5066282746310002 * (1/( 1. + 30. * self.g**2 + 32.14821200212872 * \\\n                  self.g**3 + 43.17787580118569 * self.g**4 + 25.54986136647706 * self.g**5))**(1/10)\n\n        elif key == '(4,4)^(1/10)':\n            #FPR[1/(2*5),4,4,g_] \n            fpr = 2.5066282746310002 * (( 1. + 5.072505666220409 * self.g + 14.43685326985559 * \\\n                  self.g**2)/( 1. + 5.072505666220409 * self.g + 44.43685326985559 * self.g**2 + 152.17516998661225 * \\\n                  self.g**3 + 403.10559809566917 * self.g**4 + 708.6889005862954 * self.g**5 + 752.9544739983669 * \\\n                  self.g**6 + 368.85959961298136 * self.g**7))**(1/10)\n\n        elif key == '(3,3)^(1/14)':\n            #FPR[1/(2*7),3,3,g_] \n            fpr = 2.5066282746310002 * (1/( 1. + 42. * self.g**2+155.75843284764994 * \\\n                  self.g**4 + 239.21559267499774 * self.g**5 + 220.9758065100799 * \\\n                  self.g**6 + 93.39937438057375 * self.g**7))**(1/14)\n\n        elif key == '(4,4)^(1/18)':\n            #FPR[1/(2*9),4,4,g_] \n            fpr = 2.5066282746310002 * (1/( 1. + 54. * self.g**2+594. * self.g**4+780.7879756756589 * \\\n                  self.g**5 + 1294.340979801729 * self.g**6 + 1475.3510504866329 * self.g**7 + 1038.5911468627608 * \\\n                  self.g**8 + 341.42819835916043 * self.g**9))**(1/18)\n\n        else:\n            raise KeyError('The key provided does not match any in the FPR database.')\n\n        return fpr\n\n    #at the moment, this is very specific to the paper plot---disassemble later for package\n    def fpr_plot(self, mean, intervals, fpr_keys=None, ci=68):\n\n        r'''\n        A plotter for the overlay of the GP results and the FPR results\n        from Honda (2014). \n\n        Example:\n            FPR.fpr_plot(mean=np.array(), intervals=np.array([,]), \n                fpr_keys=['(3,3)^(1/6)'], ci=95)\n\n        Parameters:\n            mean (numpy.ndarray): A PPD mean to be compared to the FPR \n                results.\n\n            intervals (numpy.ndarray): A 2D array to plot a UQ band around \n                the PPD. \n\n            fpr_keys (list): A list of strings of fpr keys to be read in \n                by the function and calculated using the fprset()\n                function above.\n\n            ci (int): The uncertainty calculated on the expansions. Can\n                be either 68 or 95. \n\n        Returns:\n            None.\n        '''\n\n        #set up plot configuration\n        fig, ax1 = plt.subplots(figsize=(8,6), dpi=600)\n        ax1.tick_params(axis='x', labelsize=18)\n        ax1.tick_params(axis='y', labelsize=18)\n        ax1.locator_params(nbins=8)\n        ax1.xaxis.set_minor_locator(AutoMinorLocator())\n        ax1.xaxis.set_label_coords(.5, -.05)\n        ax1.yaxis.set_minor_locator(AutoMinorLocator())\n        ax1.yaxis.set_label_coords(-.05, .5)\n\n        #set up x and y limits\n        ax1.set_xlim(0,1)\n        ax1.set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1])\n        ax1.set_ylim(1.2,3.2)\n        ax1.set_yticks([1.2, 1.6, 2.0, 2.4, 2.8, 3.2])\n\n        #labels and true model\n        ax1.set_xlabel('g', fontsize=22)\n        ax1.set_ylabel('F(g)', fontsize=22)\n        ax1.plot(self.g, self.m.true_model(self.g), 'k', label='True model')\n\n        #unpack ci\n        self.ci = ci \n\n        #plot the small-g expansions and error bands\n        ax1.plot(self.g, self.m.low_g(self.g)[0,:], 'r--', label=r'$f_s$ ($N_s$ = {})'.format(self.m.loworder[0]))\n\n        #plot the large-g expansions and error bands\n        ax1.plot(self.g, self.m.high_g(self.g)[0,:], 'b--', label=r'$f_l$ ($N_l$ = {})'.format(self.m.highorder[0]))\n\n        #plot the GP results (mixed model)\n        ax1.plot(self.g, mean, 'g', label='Mean')\n        ax1.plot(self.g, intervals[:,0], 'g', linestyle='dotted', label=r'{}$\\%$ CI'.format(int(self.ci)))\n        ax1.plot(self.g, intervals[:,1], 'g', linestyle='dotted')\n        ax1.fill_between(self.g, intervals[:,0], intervals[:,1], color='green', alpha=0.2)\n\n        #FPR results\n        if fpr_keys is not None:\n            ax1.set_prop_cycle(cycler('color', ['darkviolet', 'deepskyblue', 'darkorange', 'gold']))\n            for k in fpr_keys:\n                mn = k[0:5]\n                alpha = k[6:]\n                fpr = self.fprset(k)\n                ax1.plot(self.g, fpr, linestyle='dashed', label=r'$F_{{{}}}^{{{}}} (g)$'.format(mn, alpha))\n\n        ax1.legend(fontsize=16, loc='upper right')\n\n        #inset plot parameters\n        x1 = 0.26\n        x2 = 0.31\n        y1 = 2.15\n        y2 = 2.25\n        axins = zoomed_inset_axes(ax1, 6, loc=9) \n        axins.plot(self.g, self.m.true_model(self.g), 'k', label='True model')\n        axins.plot(self.g, self.m.low_g(self.g)[0,:], 'r--', label=r'$f_s$ ($N_s$ = {})'.format(self.m.loworder[0]))\n        axins.plot(self.g, self.m.high_g(self.g)[0,:], 'b--', label=r'$f_l$ ($N_l$ = {})'.format(self.m.highorder[0]))\n        axins.plot(self.g, mean, 'g', label='Mean')\n        axins.plot(self.g, intervals[:,0], 'g', linestyle='dotted', label=r'{}$\\%$ interval'.format(int(self.ci)))\n        axins.plot(self.g, intervals[:,1], 'g', linestyle='dotted')\n        axins.fill_between(self.g, intervals[:,0], intervals[:,1], color='green', alpha=0.2)\n        if fpr_keys is not None:\n            axins.set_prop_cycle(cycler('color', ['darkviolet', 'deepskyblue', 'darkorange', 'gold']))\n        for k in fpr_keys:\n                mn = k[0:5]\n                alpha = k[6:]\n                fpr = self.fprset(k)\n                axins.plot(self.g, fpr, linestyle='dashed', label=r'$F_{{{}}}^{{{}}} (g)$'.format(mn, alpha))\n\n        axins.set_xlim(x1, x2)\n        axins.set_ylim(y1, y2)\n        mark_inset(ax1, axins, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\")\n        plt.draw()\n        plt.show()\n\n        #save figure option\n        # response = input('Would you like to save this figure? (yes/no)')\n\n        # if response == 'yes':\n        #     name = input('Enter a file name (include .jpg, .png, etc.)')\n        #     fig.savefig(name, bbox_inches='tight')\n\n        return None\n</code></pre>"},{"location":"fprdat/#samba.fprdat.FPR.__init__","title":"<code>__init__(g, loworder, highorder)</code>","text":"<p>A class to calculate the FPR method curves for comparison to the mixed models in the three BMM methods of this package.</p> Example <p>FPR(g=np.linspace(1e-6,1.0,100), loworder=np.array([5]),     highorder=np.array([5]))</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>linspace</code> <p>The input space array over which the models are  mixed.</p> required <code>loworder</code> <code>ndarray</code> <p>The highest order considered in the small-g  expansion.</p> required <code>highorder</code> <code>ndarray</code> <p>The highest order considered in the large-g  expansion.</p> required <p>Returns:</p> Type Description <p>None.</p> Source code in <code>samba/fprdat.py</code> <pre><code>def __init__(self, g, loworder, highorder):\n\n    r'''\n    A class to calculate the FPR method curves for comparison\n    to the mixed models in the three BMM methods of this package.\n\n    Example:\n        FPR(g=np.linspace(1e-6,1.0,100), loworder=np.array([5]),\n            highorder=np.array([5]))\n\n    Parameters:\n        g (numpy.linspace): The input space array over which the models are \n            mixed.\n\n        loworder (numpy.ndarray): The highest order considered in the small-g \n            expansion.\n\n        highorder (numpy.ndarray): The highest order considered in the large-g \n            expansion.\n\n    Returns:\n        None.\n    '''\n\n    self.g = g\n    self.loworder = loworder\n    self.highorder = highorder \n\n    #instantiate Models() class here\n    self.m = Models(self.loworder, self.highorder)\n\n    return None\n</code></pre>"},{"location":"fprdat/#samba.fprdat.FPR.fpr_plot","title":"<code>fpr_plot(mean, intervals, fpr_keys=None, ci=68)</code>","text":"<p>A plotter for the overlay of the GP results and the FPR results from Honda (2014). </p> Example <p>FPR.fpr_plot(mean=np.array(), intervals=np.array([,]),      fpr_keys=['(3,3)^(1/6)'], ci=95)</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>ndarray</code> <p>A PPD mean to be compared to the FPR  results.</p> required <code>intervals</code> <code>ndarray</code> <p>A 2D array to plot a UQ band around  the PPD. </p> required <code>fpr_keys</code> <code>list</code> <p>A list of strings of fpr keys to be read in  by the function and calculated using the fprset() function above.</p> <code>None</code> <code>ci</code> <code>int</code> <p>The uncertainty calculated on the expansions. Can be either 68 or 95. </p> <code>68</code> <p>Returns:</p> Type Description <p>None.</p> Source code in <code>samba/fprdat.py</code> <pre><code>def fpr_plot(self, mean, intervals, fpr_keys=None, ci=68):\n\n    r'''\n    A plotter for the overlay of the GP results and the FPR results\n    from Honda (2014). \n\n    Example:\n        FPR.fpr_plot(mean=np.array(), intervals=np.array([,]), \n            fpr_keys=['(3,3)^(1/6)'], ci=95)\n\n    Parameters:\n        mean (numpy.ndarray): A PPD mean to be compared to the FPR \n            results.\n\n        intervals (numpy.ndarray): A 2D array to plot a UQ band around \n            the PPD. \n\n        fpr_keys (list): A list of strings of fpr keys to be read in \n            by the function and calculated using the fprset()\n            function above.\n\n        ci (int): The uncertainty calculated on the expansions. Can\n            be either 68 or 95. \n\n    Returns:\n        None.\n    '''\n\n    #set up plot configuration\n    fig, ax1 = plt.subplots(figsize=(8,6), dpi=600)\n    ax1.tick_params(axis='x', labelsize=18)\n    ax1.tick_params(axis='y', labelsize=18)\n    ax1.locator_params(nbins=8)\n    ax1.xaxis.set_minor_locator(AutoMinorLocator())\n    ax1.xaxis.set_label_coords(.5, -.05)\n    ax1.yaxis.set_minor_locator(AutoMinorLocator())\n    ax1.yaxis.set_label_coords(-.05, .5)\n\n    #set up x and y limits\n    ax1.set_xlim(0,1)\n    ax1.set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1])\n    ax1.set_ylim(1.2,3.2)\n    ax1.set_yticks([1.2, 1.6, 2.0, 2.4, 2.8, 3.2])\n\n    #labels and true model\n    ax1.set_xlabel('g', fontsize=22)\n    ax1.set_ylabel('F(g)', fontsize=22)\n    ax1.plot(self.g, self.m.true_model(self.g), 'k', label='True model')\n\n    #unpack ci\n    self.ci = ci \n\n    #plot the small-g expansions and error bands\n    ax1.plot(self.g, self.m.low_g(self.g)[0,:], 'r--', label=r'$f_s$ ($N_s$ = {})'.format(self.m.loworder[0]))\n\n    #plot the large-g expansions and error bands\n    ax1.plot(self.g, self.m.high_g(self.g)[0,:], 'b--', label=r'$f_l$ ($N_l$ = {})'.format(self.m.highorder[0]))\n\n    #plot the GP results (mixed model)\n    ax1.plot(self.g, mean, 'g', label='Mean')\n    ax1.plot(self.g, intervals[:,0], 'g', linestyle='dotted', label=r'{}$\\%$ CI'.format(int(self.ci)))\n    ax1.plot(self.g, intervals[:,1], 'g', linestyle='dotted')\n    ax1.fill_between(self.g, intervals[:,0], intervals[:,1], color='green', alpha=0.2)\n\n    #FPR results\n    if fpr_keys is not None:\n        ax1.set_prop_cycle(cycler('color', ['darkviolet', 'deepskyblue', 'darkorange', 'gold']))\n        for k in fpr_keys:\n            mn = k[0:5]\n            alpha = k[6:]\n            fpr = self.fprset(k)\n            ax1.plot(self.g, fpr, linestyle='dashed', label=r'$F_{{{}}}^{{{}}} (g)$'.format(mn, alpha))\n\n    ax1.legend(fontsize=16, loc='upper right')\n\n    #inset plot parameters\n    x1 = 0.26\n    x2 = 0.31\n    y1 = 2.15\n    y2 = 2.25\n    axins = zoomed_inset_axes(ax1, 6, loc=9) \n    axins.plot(self.g, self.m.true_model(self.g), 'k', label='True model')\n    axins.plot(self.g, self.m.low_g(self.g)[0,:], 'r--', label=r'$f_s$ ($N_s$ = {})'.format(self.m.loworder[0]))\n    axins.plot(self.g, self.m.high_g(self.g)[0,:], 'b--', label=r'$f_l$ ($N_l$ = {})'.format(self.m.highorder[0]))\n    axins.plot(self.g, mean, 'g', label='Mean')\n    axins.plot(self.g, intervals[:,0], 'g', linestyle='dotted', label=r'{}$\\%$ interval'.format(int(self.ci)))\n    axins.plot(self.g, intervals[:,1], 'g', linestyle='dotted')\n    axins.fill_between(self.g, intervals[:,0], intervals[:,1], color='green', alpha=0.2)\n    if fpr_keys is not None:\n        axins.set_prop_cycle(cycler('color', ['darkviolet', 'deepskyblue', 'darkorange', 'gold']))\n    for k in fpr_keys:\n            mn = k[0:5]\n            alpha = k[6:]\n            fpr = self.fprset(k)\n            axins.plot(self.g, fpr, linestyle='dashed', label=r'$F_{{{}}}^{{{}}} (g)$'.format(mn, alpha))\n\n    axins.set_xlim(x1, x2)\n    axins.set_ylim(y1, y2)\n    mark_inset(ax1, axins, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\")\n    plt.draw()\n    plt.show()\n\n    #save figure option\n    # response = input('Would you like to save this figure? (yes/no)')\n\n    # if response == 'yes':\n    #     name = input('Enter a file name (include .jpg, .png, etc.)')\n    #     fig.savefig(name, bbox_inches='tight')\n\n    return None\n</code></pre>"},{"location":"fprdat/#samba.fprdat.FPR.fprset","title":"<code>fprset(key)</code>","text":"<p>Call the proper FPR function desired and obtain  an array of the results in the input space, g. </p> Example <p>FPR.fprset(key='(2,4)^(1/8)')</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The preferred FPR function. Enter a key in the convention: '(m,n)^(\\alpha)', where m,n are orders less than or equal to N_s and N_l (loworder, highorder in the other classes). \\alpha is the value the FPR is  raised to in Eq. (2.7) (Honda 2014). </p> required <p>Returns:</p> Name Type Description <code>fpr</code> <code>ndarray</code> <p>Results of the FPR function in an array.</p> Source code in <code>samba/fprdat.py</code> <pre><code>def fprset(self, key):\n\n    r'''\n    Call the proper FPR function desired and obtain \n    an array of the results in the input space, g. \n\n    Example: \n        FPR.fprset(key='(2,4)^(1/8)')\n\n    Parameters:\n        key (str): The preferred FPR function. Enter a key in the\n            convention: '(m,n)^(\\alpha)', where m,n are orders\n            less than or equal to N_s and N_l (loworder, highorder\n            in the other classes). \\alpha is the value the FPR is \n            raised to in Eq. (2.7) (Honda 2014). \n\n    Returns:\n        fpr (numpy.ndarray): Results of the FPR function in an array. \n    '''\n\n    #if statement for calling the proper FPR function\n    fpr = np.zeros(len(self.g))\n\n    self.keyvalue = key\n\n    if key == '(0,0)^(1/2)':\n\n        #FPR[1/(2*1),0,0,g_]\n        fpr = np.sqrt(2 * np.pi * np.sqrt(1/(1 + (8 * self.g * np.pi)/sp.gamma(1/4)**2)))  \n\n    elif key == '(1,1)^(1/2)':  \n\n        #FPR[1/(2*1),1,1,g_] \n        fpr = np.sqrt(2 * np.pi * sp.gamma(1/4)) * np.sqrt((2 * np.pi * sp.gamma(-(1/4)) + 8 * \\\n              self.g * np.pi * sp.gamma(1/4) + sp.gamma(1/4)**3) / (64 * self.g**2 * np.pi**2 + 2 * \\\n              np.pi * sp.gamma(-(1/4)) * sp.gamma(1/4) + 8 * self.g * \\\n              np.pi * sp.gamma(1/4)**2 + sp.gamma(1/4)**4))\n\n    elif key == '(2,2)^(1/2)':\n        #FPR[1/(2*1),2,2,g_] \n        fpr = 2.5066282746310002 * np.sqrt((1. + 10.1531607808241 * self.g + 37.91166947810798 * \\\n              self.g**2) / (1. + 10.1531607808241 * self.g + 43.91166947810798 * \\\n              self.g**2 + 72.48541321812907 * self.g**3))\n\n    elif key == '(3,3)^(1/2)':\n        #FPR[1/(2*1),3,3,g_] \n        fpr = 2.5066282746310002 * np.sqrt((1. + 16.030388229931486 * self.g + 110.26077245790317 * \\\n              self.g**2 + 324.0187225963292 * self.g**3) / (1. + 16.030388229931486 * \\\n              self.g + 116.26077245790317 * self.g**2 + 420.2010519759181 * \\\n              self.g**3 + 619.509278307239 * self.g**4))\n\n    elif key == '(4,4)^(1/2)':\n        #FPR[1/(2*1),4,4,g_] \n        fpr = 2.5066282746310002 * np.sqrt(( 1. + 22.874503929271544 * self.g + 238.23905635876318 * \\\n              self.g**2 + 1303.4929582331083 * self.g**3 + 3224.5631655188554 * self.g**4)/ \\\n              ( 1. + 22.874503929271544 * self.g + 244.23905635876318 * self.g**2 + 1440.7399818087376 * \\\n              self.g**3 + 4575.997503671434 * self.g**4 + 6165.220279617642 * self.g**5))\n\n    elif key == '(1,1)^(1/6)':\n        #FPR[1/(2*3),1,1,g_] \n        fpr = 2.5066282746310002 * (1/( 1. + 7.086913042848253 * self.g**2 + 6.989291097242859 * \\\n              self.g**3))**(1/6)\n\n    elif key == '(2,2)^(1/6)':\n        #FPR[1/(2*3),2,2,g_] \n        fpr = 2.5066282746310002 * (( 1. + 3.7875802399388747 * self.g)/( 1. + 3.7875802399388747 * \\\n              self.g + 18.000000000000007 * self.g**2 + 33.83154290049999 * self.g**3 + 26.47250085109775 * \\\n              self.g**4))**(1/6)\n\n    elif key == '(3,3)^(1/6)':\n        #FPR[1/(2*3),3,3,g_] \n        fpr = 2.5066282746310002 * (( 1. + 8.099704591178746 * self.g + 28.252523713142793 * \\\n              self.g**2)/( 1. + 8.099704591178746 * self.g + 46.25252371314279 * \\\n              self.g**2 + 145.7946826412175 * self.g**3 + 256.83437198547387 * \\\n              self.g**4 + 197.46511246291166 * self.g**5))**(1/6)\n\n    elif key == '(4,4)^(1/6)':\n        #FPR[1/(2*3),4,4,g_]\n        fpr = 2.5066282746310002 * (( 1. + 13.470628417091639 * self.g + 87.53888949233587 * \\\n              self.g**2 + 255.1535591483978 * self.g**3)/( 1. + 13.470628417091639 * \\\n              self.g + 105.53888949233587 * self.g**2 + 497.6248706560473 * self.g**3 + 1449.7000108620534 * \\\n              self.g**4 + 2420.0858672492423 * self.g**5 + 1783.3424993857257 * self.g**6))**(1/6)\n\n    elif key == '(2,2)^(1/10)':\n        #FPR[1/(2*5),2,2,g_] \n        fpr = 2.5066282746310002 * (1/( 1. + 30. * self.g**2 + 32.14821200212872 * \\\n              self.g**3 + 43.17787580118569 * self.g**4 + 25.54986136647706 * self.g**5))**(1/10)\n\n    elif key == '(4,4)^(1/10)':\n        #FPR[1/(2*5),4,4,g_] \n        fpr = 2.5066282746310002 * (( 1. + 5.072505666220409 * self.g + 14.43685326985559 * \\\n              self.g**2)/( 1. + 5.072505666220409 * self.g + 44.43685326985559 * self.g**2 + 152.17516998661225 * \\\n              self.g**3 + 403.10559809566917 * self.g**4 + 708.6889005862954 * self.g**5 + 752.9544739983669 * \\\n              self.g**6 + 368.85959961298136 * self.g**7))**(1/10)\n\n    elif key == '(3,3)^(1/14)':\n        #FPR[1/(2*7),3,3,g_] \n        fpr = 2.5066282746310002 * (1/( 1. + 42. * self.g**2+155.75843284764994 * \\\n              self.g**4 + 239.21559267499774 * self.g**5 + 220.9758065100799 * \\\n              self.g**6 + 93.39937438057375 * self.g**7))**(1/14)\n\n    elif key == '(4,4)^(1/18)':\n        #FPR[1/(2*9),4,4,g_] \n        fpr = 2.5066282746310002 * (1/( 1. + 54. * self.g**2+594. * self.g**4+780.7879756756589 * \\\n              self.g**5 + 1294.340979801729 * self.g**6 + 1475.3510504866329 * self.g**7 + 1038.5911468627608 * \\\n              self.g**8 + 341.42819835916043 * self.g**9))**(1/18)\n\n    else:\n        raise KeyError('The key provided does not match any in the FPR database.')\n\n    return fpr\n</code></pre>"},{"location":"gaussianprocess/","title":"Method 3: Multivariate model mixing with a Gaussian process","text":"<p>This method uses the same framework as the previous method, but now includes a Gaussian process (GP) in the mixing.</p> <p>A diagnostic tool that helps with determining whether or not our mixed model result is reasonable is the Mahalanobis distance, calculated as</p> <p>$$ D^{2}_{MD} = (\\mathbf{y} - \\mathbf{m})^{T}\\textit{K}^{-1}(\\mathbf{y} - \\mathbf{m}), $$</p> <p>and given in the functions below.</p>"},{"location":"gaussianprocess/#samba.gaussprocess.GP","title":"<code>GP</code>","text":"<p>               Bases: <code>Bivariate</code></p> Source code in <code>samba/gaussprocess.py</code> <pre><code>class GP(Bivariate):\n\n    def __init__(self, g, loworder, highorder, kernel=\"RBF\", nu=None, ci=68, error_model='informative'):\n\n        r'''\n        A class that will pull from the Models class to perform GP emulation on \n        two models from the small-g expansion region to the large-g expansion region. \n        The parameter settings of the kernel will be set by the user in this \n        initial function. This class 'wraps' the scikit learn package. \n\n        Example:\n            GP(g=np.linspace(1e-6,1.0,100), loworder=5, highorder=2, kernel=\"Matern\",\n                ci=68, error_model='informative')\n\n        Parameters:\n            g (numpy linspace): The linspace across the coupling constant space \n                used for the GP.\n\n            highorder (numpy.ndarray, float, int): The truncation order of the \n                large-g expansion. \n\n            kernel (str): The type of kernel the user wishes to use. Default is \n                the RBF kernel; possible choices are RBF, Matern, and Rational \n                Quadratic. \n\n            nu (float): The value of the Matern kernel used, if kernel=\"Matern\". \n                Otherwise, default is None.\n\n            ci (int): The uncertainty interval to use. Must be 68 or 95. \n\n            error_model (str): The error model to be used in the calculation. \n                Options are 'uninformative' and 'informative'. Default is 'informative'. \n\n        Returns:\n            None.\n        ''' \n\n        #set up the prediction array as a class variable for use later\n        self.gpredict = np.copy(g)\n\n        #extract uncertainty interval for later use\n        self.ci = ci \n\n        #check type and assign class variables\n        if isinstance(loworder, float) == True or isinstance(loworder, int) == True:\n            loworder = np.array([loworder])\n\n        if isinstance(highorder, float) == True or isinstance(highorder, int) == True:\n            highorder = np.array([highorder])\n\n        self.loworder = loworder \n        self.highorder = highorder \n\n        #Models(), Uncertainties()\n        self.m = Models(self.loworder, self.highorder)\n        self.u = Uncertainties(error_model)\n\n        #instantiate the class variable error_model for ease class crossing\n        self.error_model = self.u.error_model\n\n        #integral length\n        self.gint = np.empty([])\n\n        #kernel set-up for the rest of the class (one-dimensional)\n        kconstant = kernels.ConstantKernel(1.0)\n\n        if kernel == \"RBF\":\n            k = kernels.RBF(length_scale=0.5, length_scale_bounds=(1e-5,1e5))\n        elif kernel == \"Matern\":\n            if nu is None:\n                raise ValueError('Matern kernel must be supplied a value for nu.')\n            else:\n                k = kernels.Matern(length_scale=0.4, length_scale_bounds=(1e-5,1e5), nu=nu)\n        elif kernel == \"Rational Quadratic\":\n            k = kernels.RationalQuadratic(length_scale=1.0, alpha=1)\n        else:\n            raise ValueError('Please choose an available kernel.')\n\n        self.kern = kconstant * k\n\n        return None\n\n\n    def training(self, error=True, method=2, plot=True):\n\n        r'''\n        A function that links the model data and the training function in \n        scikit learn, and plots the training data using GP.plot_training().\n\n        Example:\n            GP.training(error=False, method=3)\n\n        Parameters:\n            error (bool): A boolean variable to toggle use of a truncation error model \n                in the kernel during training. Default is True.\n\n            method (int): The method used for determining the training points. Options: \n                1,2,3. For an extensive explanation of the methods, see the paper.\n\n            plot (bool): Option to plot the training set with series expansions and \n                true model. Default is True. \n\n        Returns:\n            sk (object): The object storing all training information from the sklearn \n                regression performed on the data.\n        '''\n\n        #first set the method\n        self.method = method \n\n        #call the training set generator function\n        gs, datas, sigmas = self.training_set()\n\n#         ### specific test ###\n#         # split up the set\n#         gs = np.array([gs[1], gs[2], gs[3]])\n#         datas = np.array([datas[1], datas[2], datas[3]])\n#         sigmas = np.array([sigmas[1], sigmas[2], sigmas[3]])\n\n        #make a gs class variable for weights use\n        self.gs = gs \n        self.datas = datas\n        self.sigmas = sigmas\n\n        #make column vectors for the regressor\n        gc = gs.reshape(-1,1)\n        datac = datas.reshape(-1,1)\n\n        #take the data point uncertainty into the kernel \n        if error == True:\n            self.alpha = np.square(sigmas)\n        else:\n            self.alpha = 1e-12\n\n        #use GPR and kernel to train\n        m = GaussianProcessRegressor(kernel=self.kern, alpha=self.alpha, n_restarts_optimizer=20, normalize_y=True)\n\n        #fit the GP to the training data\n        self.sk = m.fit(gc, datac)\n\n        #print the optimized parameters for the user\n        print('Gaussian process parameters: {}'.format(m.kernel_))\n\n        #plot the results\n        if plot is True:\n            self.plot_training(gs, datas, sigmas)\n\n        return self.sk\n\n\n    def validate(self, plot=True, run_taweret=False):\n\n        r'''\n        A wrapper function for scikit learn's GP prediction function. This will \n        predict the GP results with an interval and plot against the expansions\n        using GP.plot_validate().\n\n        Example:\n            GP.validate()\n\n        Parameters:\n            plot (bool): The option to plot the GP mean and variance over the testing\n                set and true model. Default is True. \n\n        Returns:\n            meanp (numpy.ndarray): The mean array of the GP prediction results.\n\n            sigp (numpy.ndarray): The standard deviation array of the GP prediction \n                results. \n\n            cov (numpy.ndarray): The covariance matrix of the GP prediction results. \n        '''\n\n        #make the prediction values into a column vector\n        self.gpred = self.gpredict.reshape(-1,1)\n\n        #predict the results for the validation data\n        self.meanp, self.sigp = self.sk.predict(self.gpred, return_std=True)\n        _, self.cov = self.sk.predict(self.gpred, return_cov=True)\n\n        # issues right here when running wrapped in Taweret\n        if run_taweret is False:\n            self.meanp = self.meanp #[:,0]\n\n        #calculate the interval for the predictions\n        if self.ci == 68:\n            factor = 1.0\n        elif self.ci == 95:\n            factor = 1.96\n        intervals = np.zeros([len(self.meanp), 2])\n        intervals[:,0] = self.meanp - factor*self.sigp\n        intervals[:,1] = self.meanp + factor*self.sigp\n\n        #plot the results\n        if plot is True:\n            self.plot_validate(intervals)\n\n        return self.meanp, self.sigp, self.cov\n\n\n    def plot_training(self, gs, datas, sigmas):\n\n        r'''\n        A simple plotter to plot the trained GP results and models, \n        as well as the points at which the GP was trained. \n\n        Example:\n            GP.plot_training(gs=np.array([]), datas=np.array([]),\n                sigmas=np.array([]))\n\n        Parameters:\n            gs (numpy.ndarray): Points chosen by GP.training_set() in input \n                space g.\n\n            datas (numpy.ndarray): Corresponding values of the series expansions \n                at gs.\n\n            sigmas (numpy.ndarray): Corresponding error model results at each \n                training point.\n\n        Returns:\n            None.\n        '''\n\n        #set up the plot\n        fig = plt.figure(figsize=(8,6), dpi=600)\n        ax = plt.axes()\n        fig.patch.set_facecolor('white')\n        ax.tick_params(axis='x', labelsize=18)\n        ax.tick_params(axis='y', labelsize=18)\n        ax.locator_params(nbins=8)\n        ax.xaxis.set_minor_locator(AutoMinorLocator())\n        ax.yaxis.set_minor_locator(AutoMinorLocator())\n        ax.set_xlim(0.0, max(self.gpredict))\n        ax.set_ylim(1.0,3.0)\n        ax.set_xlabel('g', fontsize=22)\n        ax.set_ylabel('F(g)', fontsize=22)\n        ax.set_title('F(g): training set', fontsize=22)\n        ax.plot(self.gpredict, self.m.true_model(self.gpredict), 'k', label='True model')\n\n        #plot the data\n        ax.errorbar(self.gtrlow, self.datatrlow, yerr=self.lowsigma, color='red', fmt='o', markersize=4, \\\n                    capsize=4, label=r'$f_s$ ($N_s$ = {}) data'.format(self.loworder[0]))\n        ax.errorbar(self.gtrhigh, self.datatrhigh, yerr=self.highsigma, color='blue', fmt='o', markersize=4, \\\n                    capsize=4, label=r'$f_l$ ($N_l$ = {}) data'.format(self.highorder[0]))\n\n        #plot the chosen training points over the whole training set\n        ax.errorbar(gs, datas, yerr=sigmas, color='black', fmt='o', markersize=4, capsize=4, label='Training data')\n\n        ax.legend(fontsize=18, loc='upper right')\n        plt.show()\n\n        #save figure option\n        # response = input('Would you like to save this figure? (yes/no)')\n\n        # if response == 'yes':\n        #     name = input('Enter a file name (include .jpg, .png, etc.)')\n        #     fig.savefig(name, bbox_inches='tight')\n\n        return None\n\n\n    def plot_validate(self, intervals):\n\n        r'''\n        A simple plotter to show the results of the GP \n        predictions at new points in g. \n\n        Example:\n            GP.plot_validate(intervals=np.array([,]))\n\n        Parameters:\n            intervals (numpy.ndarray): The uncertainty band around the \n                prediction set.\n\n        Returns:\n            None.\n        '''\n\n        #plot the results\n        fig = plt.figure(figsize=(8,6), dpi=600)\n        ax = plt.axes()\n        fig.patch.set_facecolor('white')\n        ax.tick_params(axis='x', labelsize=18)\n        ax.tick_params(axis='y', labelsize=18)\n        ax.locator_params(nbins=8)\n        ax.xaxis.set_minor_locator(AutoMinorLocator())\n        ax.yaxis.set_minor_locator(AutoMinorLocator())\n        ax.set_xlim(0.0, max(self.gpredict))\n        ax.set_ylim(1.0,3.0)\n        ax.set_xlabel('g', fontsize=22)\n        ax.set_ylabel('F(g)', fontsize=22)\n        ax.set_title('F(g): GP predictions', fontsize=22)\n        ax.plot(self.gpredict, self.m.true_model(self.gpredict), 'k', label='True model')\n\n        #plot the data\n        ax.errorbar(self.gtrlow, self.datatrlow, self.lowsigma, color=\"red\", fmt='o', markersize=4, \\\n            capsize=4, alpha = 0.4, label=r\"$f_s$ ($N_s$ = {})\".format(self.loworder[0]), zorder=1)\n        ax.errorbar(self.gtrhigh, self.datatrhigh, self.highsigma, color=\"blue\", fmt='o', markersize=4, \\\n             capsize=4, alpha=0.4, label=r\"$f_l$ ($N_l$ = {})\".format(self.highorder[0]), zorder=1)\n        ax.plot(self.gpred, self.meanp, 'g', label='Predictions', zorder=2)\n        ax.plot(self.gpred, intervals[:,0], color='green', linestyle='dotted', label=r'{}$\\%$ CI'.format(self.ci), zorder=2)\n        ax.plot(self.gpred, intervals[:,1], color='green', linestyle='dotted', zorder=2)\n        ax.fill_between(self.gpred[:,0], intervals[:,0], intervals[:,1], color='green', alpha=0.3, zorder=10)\n\n        ax.legend(fontsize=18, loc='upper right')\n        plt.show()\n\n        #save figure option\n        # response = input('Would you like to save this figure? (yes/no)')\n\n        # if response == 'yes':\n        #     name = input('Enter a file name (include .jpg, .png, etc.)')\n        #     fig.savefig(name, bbox_inches='tight')\n\n        return None\n\n\n    def training_set(self):\n\n        r'''\n        An internal function to calculate the necessary training data set from\n        the input prediction set. \n\n        Example:\n            GP.training_set() \n\n        Parameters:\n            None. \n\n        Returns:\n        gs (numpy.ndarray): The modified array of input values for the training. \n\n        datas (numpy.ndarray): The modified array of data values for the training. \n\n        sigmas (numpy.ndarray): The modified array of the truncation errors for \n            the training. \n        '''\n\n        #set up the training set from the prediction set (offset by midpoint)\n        self.midpoint = (self.gpredict[1] - self.gpredict[0]) / 2.0\n        gtrainingset = np.linspace(min(self.gpredict)+self.midpoint, max(self.gpredict)+self.midpoint, len(self.gpredict))\n\n        #stop the training set, negative curvature\n        if self.loworder[0] % 4 == 2 or self.loworder[0] % 4 == 3:\n            for i in range(len(gtrainingset)):\n                if self.m.low_g(gtrainingset[i]) &lt; -1.0:\n                    lowindex = i-1\n                    break\n\n        #stop the training set, positive curvature\n        elif self.loworder[0] % 4 == 0 or self.loworder[0] % 4 == 1:\n            for i in range(len(gtrainingset)):\n                if self.m.low_g(gtrainingset[i]) &gt; 3.0:\n                    lowindex = i-1\n                    break\n\n        #stop the training set, even orders (positive curvature)\n        if self.highorder[0] % 2 == 0:\n            for i in range(len(gtrainingset)):\n                if self.m.high_g(gtrainingset[i]) &gt; 3.0:\n                    highindex = i+1\n                else:\n                    break\n\n        #stop the training set, odd orders (negative curvature)\n        else:\n            for i in range(len(gtrainingset)):\n                if self.m.high_g(gtrainingset[i]) &lt; -1.0:\n                    highindex = i+1\n                else:\n                    break\n\n        #slice the training set for the two models\n        self.gtrlow = gtrainingset[:lowindex]\n        self.gtrhigh = gtrainingset[highindex:]\n\n        #calculate the data at each point\n        self.datatrlow = self.m.low_g(self.gtrlow)[0,:]\n        self.datatrhigh = self.m.high_g(self.gtrhigh)[0,:]\n\n        #calculate the variance at each point from the next term\n        lowvariance = self.u.variance_low(self.gtrlow, self.loworder[0])\n        self.lowsigma = np.sqrt(lowvariance)\n        highvariance = self.u.variance_high(self.gtrhigh, self.highorder[0])\n        self.highsigma = np.sqrt(highvariance)\n\n        #find the values of g in the other set to determine location of points\n        index_ghigh = (np.where(self.gtrhigh == self.gtrlow[-1])[0])[0]\n\n        #value of g at the optimal red points\n        pt1 = 0.0656575\n        pt2 = 0.1161625\n\n        #method 1: using g=0.6 as a training point\n        pttest = 0.6  \n        indexptest = self.nearest_value(self.gtrhigh, pttest) \n\n        #method 3: finding based on error (5%)\n        for i in range(len(self.gtrhigh)-1, -1, -1):\n            if self.highsigma[i] &gt;= 0.05*self.datatrhigh[i]:\n                indexerror = i\n                break \n\n        #find the values in the training array closest to the points\n        indexpt1 = self.nearest_value(self.gtrlow, pt1)\n        indexpt2 = self.nearest_value(self.gtrlow, pt2)\n\n        #create two points on either side (highpoint = 20)\n        glowtr = np.array([self.gtrlow[indexpt1], self.gtrlow[indexpt2]])\n        datalowtr = np.array([self.datatrlow[indexpt1], self.datatrlow[indexpt2]])\n        sigmalowtr = np.array([self.lowsigma[indexpt1], self.lowsigma[indexpt2]])\n\n        #choose training points depending on method entered\n        if self.method == 1:\n            ghightr = np.array([self.gtrhigh[indexptest], self.gtrhigh[-1]])\n            datahightr = np.array([self.datatrhigh[indexptest], self.datatrhigh[-1]])\n            sigmahightr = np.array([self.highsigma[indexptest], self.highsigma[-1]])\n\n        elif self.method == 2:\n            ghightr = np.array([self.gtrhigh[index_ghigh], self.gtrhigh[-1]])\n            datahightr = np.array([self.datatrhigh[index_ghigh], self.datatrhigh[-1]])\n            sigmahightr = np.array([self.highsigma[index_ghigh], self.highsigma[-1]])\n\n        elif self.method == 3:\n            ghightr = np.array([self.gtrhigh[indexerror], self.gtrhigh[-1]])\n            datahightr = np.array([self.datatrhigh[indexerror], self.datatrhigh[-1]])\n            sigmahightr = np.array([self.highsigma[indexerror], self.highsigma[-1]])\n\n        #concatenate these arrays and send back\n        gtr = np.concatenate((glowtr, ghightr))\n        datatr = np.concatenate((datalowtr, datahightr))\n        sigmatr = np.concatenate((sigmalowtr, sigmahightr))\n\n        return gtr, datatr, sigmatr \n\n\n    def MD_set(self, pts=3, plot=False):\n\n        r'''\n        Takes the training set of points and uses them to cut the\n        testing set to their limits. This reduces the MD calculation\n        to the region of interest.  \n\n        Example:\n            GP.MD_set()\n\n        Parameters:\n            pts (int): The number of points to use to calculate the Mahalanobis\n                distance. Can be any number up to the size of self.gpredict. \n\n            plot (bool): The option to plot the MD points across the input space. \n                Default is False. \n\n        Returns:\n            md_g (numpy.ndarray): The input values used in the MD calculation.\n\n            md_mean (numpy.ndarray): The mean values from the GP corresponding \n                to the md_g points.\n\n            md_sig (numpy.ndarray): The error bars corresponding to the md_g \n                points.\n\n            md_cov (numpy.ndarray): The covariance matrix corresponding to the md_g \n                points.\n        '''\n\n        #import the GP mean, cov, and errors for the prediction set\n        GP_mean = self.meanp\n        GP_err = self.sigp\n        GP_cov = self.cov\n\n        #calculate the variance at each expansion point from the next term\n        lowvar = self.u.variance_low(self.gpredict, self.loworder[0])\n        lowerr = np.sqrt(lowvar)\n        highvar = self.u.variance_high(self.gpredict, self.highorder[0])\n        hierr = np.sqrt(highvar)\n\n        #compare the values and choose where the gap is\n        for i in range(len(lowerr)):\n            if GP_err[i] &lt; lowerr[i]:\n                index_lowerr = i\n                break\n\n        for i in range(len(hierr)-1, -1, -1):\n            if GP_err[i] &lt; hierr[i]: \n                index_hierr = i \n                break\n\n        #cut the GP array into the gap\n        md_g = self.gpredict[index_lowerr:index_hierr]\n        self.gint = md_g.copy()\n        md_mean = GP_mean[index_lowerr:index_hierr]\n        md_sig = GP_err[index_lowerr:index_hierr]\n        md_cov = GP_cov[index_lowerr:index_hierr, index_lowerr:index_hierr]\n\n        #select points in g\n        self.lenpts = pts\n        points = self.create_points(int(self.lenpts), md_g[0], md_g[-1])\n        #print('Location of MD points in g: ', points)\n\n        #find the indices\n        indices = np.zeros([self.lenpts])\n        for i in range(self.lenpts):\n            indices[i] = self.nearest_value(md_g, points[i])\n\n        #convert to integer array\n        indices = indices.astype(int)\n\n        #pick the points out of the arrays\n        md_g = md_g[indices]\n        md_mean = md_mean[indices]\n        md_sig = md_sig[indices]\n        md_cov = md_cov[np.ix_(indices, indices)]\n\n        #plot the check the location of the points\n        if plot is True:\n            plt.xlim(0.,1.)\n            plt.plot(md_g, np.ones(len(md_g)), 'k.')\n\n        return md_g, md_mean, md_sig, md_cov\n\n\n    def md_squared(self, md_g, md_mean, md_cov, n_curves=1000):\n\n        r'''\n        A wrapper for the Mahalanobis distance calculation for the\n        reference distribution and the GP curve. To calculate the \n        Cholesky decomposition or to perform an SVD analysis, consult\n        GP.mahalanobis() below. \n\n        Example:\n            GP.md_squared(md_g=np.linspace, md_mean=np.array([]), \n                          md_cov=np.array([,]), n_curves=1000)\n\n        Parameters:\n            md_g (numpy.linspace): The points in input space g from the GP.MD_set() \n                function. \n\n            md_mean (numpy.ndarray): The values of the GP mean at the md_g points. \n\n            md_cov (numpy.ndarray): The values of the GP covariance matrix at the \n                md_g points. \n\n            n_curves (int): The number of curves from the reference distribution that\n                are drawn for the MD^2 calculation (md_ref). \n\n        Returns:\n            md_gp (float): The individual MD^2 value for the GP curve. \n\n        md_ref (numpy.ndarray): The array of MD^2 values from the reference \n            distribution.\n        '''\n\n        #calculate the ref distribution MDs\n        dist = self.ref_dist(md_mean, md_cov)\n        y = self.sample_ref(dist, n_curves)\n        md = np.ones([n_curves])\n        for i in range(n_curves):\n            md[i] = self.mahalanobis(y[:,i].T, md_mean, inv=md_cov, chol=False, svd=False)\n\n        #MD^2 (ref)\n        md_ref = md**2.0 \n\n        #calculate the GP MD \n        fval = self.m.true_model(md_g)\n        mdgp = self.mahalanobis(fval.T, md_mean, inv=md_cov, chol=False, svd=False)\n\n        #MD^2 (GP)\n        md_gp = mdgp**2.0\n\n        return md_gp, md_ref\n\n\n    def md_plotter(self, md_gp, md_ref, md_mean=None, md_cov=None, hist=True, box=False):\n\n        r'''\n        A plotting function that allows the Mahalanobis distance\n        to be plotted using either a histogram or a box and whisker\n        plot, or both. \n\n        Box and whisker plot code heavily drawn from J. Melendez' gsum\n        code (https://github.com/buqeye/gsum).\n\n        Example:\n            GP.md_plotter(md_gp=np.array([]), md_ref=np.array([]),\n            hist=False, box=True)\n\n        Parameters:\n            md_gp (float): The MD^2 value for the GP curve. \n\n            md_ref (numpy.ndarray): The array of MD^2 values for the reference\n                distribution.\n\n            md_mean (numpy.ndarray): The values of the GP mean at the md_g points. \n                Only used for box and whisker option; default is None. \n\n            md_cov (numpy.ndarray): The values of the GP covariance matrix at the \n                md_g points. Only used for box and whisker option; default is None.\n\n            hist (bool): Toggle for plotting a histogram. Default is True. \n\n            box (bool): Toggle for plotting a box plot. Default is False. \n\n        Returns:\n            None.\n        '''\n\n        title = 'Mahalanobis Distance'\n        xlabel = r'$\\mathrm{D}_{\\mathrm{MD}}^{2}$'\n\n        #histogram option\n        if hist is True:\n            fig = plt.figure(figsize=(8,6), dpi=600)\n            fig.patch.set_facecolor('white')\n            ax = plt.axes()\n            ax.set_xlabel(xlabel, fontsize=18)\n            ax.set_title(title, fontsize=22)\n            ax.set_xlim(0.0, max(md_ref))\n            ax.hist(md_ref, bins=50, density=True, histtype='bar', facecolor='black', \\\n                    ec='white', label='Reference distribution')\n            ax.plot(md_gp, 0.0, marker='o', color='r', markersize=10)\n\n            #add chi-squared to histogram\n            n = 200\n            x = np.linspace(0.0, max(md_ref), n)\n            ax.plot(x, stats.chi2.pdf(x, df=self.lenpts), 'r', linewidth=2, label=r'$\\chi^2$ (df={})'.format(self.lenpts))\n\n            #include legend\n            legend = True\n\n        #box-and-whisker option\n        if box is True:\n\n            dist = self.ref_dist(md_mean, md_cov)\n\n            legend = False\n\n            #set up the figure\n            fig = plt.figure(figsize=(8,6), dpi=100)\n            ax = plt.axes()\n            ax.set_xlabel(xlabel, fontsize=18)\n\n            #reference distribution (using chi2, NOT md_ref)\n            boxartist = self.ref_boxplot(dist, ax=ax, patch_artist=True, widths=0.8)\n            gray = 'gray'\n            for box in boxartist['boxes']:\n                box.update(dict(facecolor='lightgrey', edgecolor=gray))\n            for whisk in boxartist[\"whiskers\"]:\n                whisk.update(dict(color=gray))\n            for cap in boxartist[\"caps\"]:\n                cap.update(dict(color=gray))\n            for med in boxartist[\"medians\"]:\n                med.update(dict(color=gray))\n\n            #ax.boxplot(md_ref, showfliers=False)\n            ax.get_xaxis().set_ticks([])\n            ax.tick_params(direction='in')\n            ax.set_ylim(0,20)\n            ax.set_aspect(0.25)\n            sns.despine(offset=0, bottom=True, ax=ax)\n\n            #plot the individual GP MD value\n            ax.plot(1.0, md_gp, color='red', marker='o', markersize=10)\n\n        #finish up plot\n        if legend is True:\n            ax.legend(loc='upper right', fontsize=18)\n\n        plt.show()\n\n        return None\n\n\n    @staticmethod\n    def mahalanobis(y, mean, inv=None, chol=False, svd=False):\n\n        r'''\n        A diagnostic testing function that can calculate the Mahalanobis \n        distance for a given set of mean, covariance data and a vector. \n\n        Uses: 1). Calculate the MD of the predictions of the GP using\n                  the inverse covariance matrix (usual method);\n              2). Calculate the MD of the predictions to construct a \n                  reference distribution using the inverse covariance\n                  matrix (usual method);\n              3). Calculate the Cholesky decomposition of the MD\n                  information;\n              4). Perform an SVD analysis and send back the MD \n                  calculated via SVD. \n\n        Example:\n            GP.MD(y=np.array([]), mean=np.array([]), inv=numpy.ndarray([]),\n                chol=False, svd=False)\n\n        Parameters:\n            y (numpy.ndarray): An array of predicted values from the emulator.\n\n            mean (numpy.ndarray): An array of true values from the true model \n                (simulator).\n\n            inv (numpy.ndarray): The covariance matrix to be inverted in the \n                MD calculation.\n\n            chol (bool): The option to calculate the Cholesky decomposition\n                of the data. \n\n            svd (bool): An option to perform the SVD analysis of the MD data.\n                To use, must also have a covariance matrix sent to inv. \n\n        Returns:\n            md (float): (if calculating MD) The Mahalanobis distance. \n\n            chol_decomp (numpy.ndarray): (if calculating Cholesky decomposition) \n                The Cholesky decomposition results. \n\n            svderrs (numpy.ndarray): (if calculating SVD) The SVD errors at each   \n                point in the MD testing set. \n\n            svd_md (float) (if calculating SVD) The Mahalanobis distance. \n        '''\n\n        y = np.atleast_2d(y)\n\n        #cholesky option (solves for Cholesky decomposition)\n        if (inv is not None) and (chol is True):\n\n            chol = cholesky(inv)\n            errs = scl.solve_triangular(chol, (y-mean).T, lower=True).T\n            chol_decomp = np.linalg.norm(errs, axis=-1)\n\n            return chol_decomp \n\n        #SVD option\n        if (svd is True) and (inv is not None):\n\n            #perform SVD\n            _, s, vh = np.linalg.svd(inv)\n            print('Eigenvalues: ',s)\n            sinv = np.linalg.inv(np.diag(s))   #inverse of eigenvalue matrix\n            one = vh @ (y-mean).T\n            svd_md = np.squeeze(one.T @ sinv @ one)\n            print('MD^2 (SVD): ', svd_md)\n\n            #SVD errors\n            svderrs = np.zeros([len(s)])\n            for i in range(len(s)):\n                svderrs[i] = np.square(1.0/np.sqrt(s[i]) * np.dot(vh[i,:],(y-mean).T))\n\n            return svderrs, svd_md\n\n        #inverse option (normal MD calculation)\n        if (chol is False) and (svd is False) and (inv is not None):\n\n            md = np.squeeze(np.sqrt(np.diag((y - mean) @ np.linalg.inv(inv) @ (y - mean).T)))\n\n            return md\n\n        #if nothing is selected\n        if (inv is None):\n            raise ValueError('Please input a covariance matrix.')\n\n\n    @staticmethod\n    def nearest_value(array, value):\n\n        r'''\n        A static method to find the index of the nearest value\n        of an array to a desired value. \n\n        Example:\n            GP.nearest_value(array=numpy.ndarray, value=5)\n\n        Parameters:\n            array (numpy.ndarray): The array of values to search. \n\n            value (int): The desired value to search the array for. \n\n        Returns:\n        index (int): The index of the nearest value of the array\n            to the desired value. \n        '''\n\n        #calculate the difference between each point\n        abs_val = np.abs(array - value)\n\n        #find the smallest difference in the array\n        index = abs_val.argmin()\n\n        return index\n\n\n    @staticmethod\n    def ref_dist(mean, cov):\n\n        r'''\n        Constructs a multivariate normal distribution to act\n        as a reference distribution for the Mahalanobis distance\n        calculation. \n\n        Example:\n            Diagnostics.ref_dist(mean=np.array([]), cov=np.array([]))\n\n        Parameters:\n            mean (numpy.ndarray): The mean of the GP (given by the \n                prediction set). \n\n            cov (numpy.ndarray): The covariance matrix of the GP \n                (given by the prediction set). \n\n        Returns:\n            dist (object): A multivariate normal distribution that can \n                be used to generate samples for the reference distribution. \n        '''\n\n        dist = stats.multivariate_normal(mean=mean, cov=cov)\n\n        return dist\n\n\n    @staticmethod\n    def sample_ref(dist, n_curves):\n\n        r'''\n        Generate some sample curves from the reference distribution.\n\n        Example:\n            Diagnostics.sample_ref(dist, n_curves=10)\n\n        Parameters:\n            dist (object): The reference distribution object. \n\n            n_curves (int): The number of draws from the reference \n                distribution.\n\n        Returns:\n            samples (numpy.ndarray): The array of curves from the \n                distribution. \n        '''\n\n        samples = dist.rvs(n_curves).T\n\n        return samples\n\n\n    @staticmethod\n    def create_points(N, a, b):\n\n        r'''\n        A code to create a given number of points from a \n        linspace evenly from points a to b. \n\n        Example:\n            GP.create_points(N=3, a=0.0, b=1.0)\n\n        Parameters:\n            N (int): The number of points desired.\n\n            a (float, int): The left endpoint of the region of \n                interest. \n\n            b (float, int): The right endpoint of the region of \n                interest. \n\n        Returns:\n            pts (numpy.ndarray): The resulting array of points. \n        '''\n\n        #create the linspace with endpoints\n        pts_array = np.linspace(a, b, N+2)\n\n        #remove the first and last point\n        pts = pts_array[1:-1]\n\n        return pts\n\n\n    @staticmethod \n    def ref_boxplot(dist, q1=0.25, q3=0.75, whislo=0.025, whishi=0.975, ax=None, **kwargs):\n\n        r'''\n        Taken from the gsum code written by J. Melendez (https://github.com/buqeye/gsum).\n        '''\n\n        stat_dict = [{'med': dist.median(), 'q1': dist.ppf(q1), 'q3': dist.ppf(q3),\n                      'whislo': dist.ppf(whislo), 'whishi': dist.ppf(whishi)}]\n\n        return ax.bxp(stat_dict, showfliers=False, **kwargs)\n</code></pre>"},{"location":"gaussianprocess/#samba.gaussprocess.GP.MD_set","title":"<code>MD_set(pts=3, plot=False)</code>","text":"<p>Takes the training set of points and uses them to cut the testing set to their limits. This reduces the MD calculation to the region of interest.  </p> Example <p>GP.MD_set()</p> <p>Parameters:</p> Name Type Description Default <code>pts</code> <code>int</code> <p>The number of points to use to calculate the Mahalanobis distance. Can be any number up to the size of self.gpredict. </p> <code>3</code> <code>plot</code> <code>bool</code> <p>The option to plot the MD points across the input space.  Default is False. </p> <code>False</code> <p>Returns:</p> Name Type Description <code>md_g</code> <code>ndarray</code> <p>The input values used in the MD calculation.</p> <code>md_mean</code> <code>ndarray</code> <p>The mean values from the GP corresponding  to the md_g points.</p> <code>md_sig</code> <code>ndarray</code> <p>The error bars corresponding to the md_g  points.</p> <code>md_cov</code> <code>ndarray</code> <p>The covariance matrix corresponding to the md_g  points.</p> Source code in <code>samba/gaussprocess.py</code> <pre><code>def MD_set(self, pts=3, plot=False):\n\n    r'''\n    Takes the training set of points and uses them to cut the\n    testing set to their limits. This reduces the MD calculation\n    to the region of interest.  \n\n    Example:\n        GP.MD_set()\n\n    Parameters:\n        pts (int): The number of points to use to calculate the Mahalanobis\n            distance. Can be any number up to the size of self.gpredict. \n\n        plot (bool): The option to plot the MD points across the input space. \n            Default is False. \n\n    Returns:\n        md_g (numpy.ndarray): The input values used in the MD calculation.\n\n        md_mean (numpy.ndarray): The mean values from the GP corresponding \n            to the md_g points.\n\n        md_sig (numpy.ndarray): The error bars corresponding to the md_g \n            points.\n\n        md_cov (numpy.ndarray): The covariance matrix corresponding to the md_g \n            points.\n    '''\n\n    #import the GP mean, cov, and errors for the prediction set\n    GP_mean = self.meanp\n    GP_err = self.sigp\n    GP_cov = self.cov\n\n    #calculate the variance at each expansion point from the next term\n    lowvar = self.u.variance_low(self.gpredict, self.loworder[0])\n    lowerr = np.sqrt(lowvar)\n    highvar = self.u.variance_high(self.gpredict, self.highorder[0])\n    hierr = np.sqrt(highvar)\n\n    #compare the values and choose where the gap is\n    for i in range(len(lowerr)):\n        if GP_err[i] &lt; lowerr[i]:\n            index_lowerr = i\n            break\n\n    for i in range(len(hierr)-1, -1, -1):\n        if GP_err[i] &lt; hierr[i]: \n            index_hierr = i \n            break\n\n    #cut the GP array into the gap\n    md_g = self.gpredict[index_lowerr:index_hierr]\n    self.gint = md_g.copy()\n    md_mean = GP_mean[index_lowerr:index_hierr]\n    md_sig = GP_err[index_lowerr:index_hierr]\n    md_cov = GP_cov[index_lowerr:index_hierr, index_lowerr:index_hierr]\n\n    #select points in g\n    self.lenpts = pts\n    points = self.create_points(int(self.lenpts), md_g[0], md_g[-1])\n    #print('Location of MD points in g: ', points)\n\n    #find the indices\n    indices = np.zeros([self.lenpts])\n    for i in range(self.lenpts):\n        indices[i] = self.nearest_value(md_g, points[i])\n\n    #convert to integer array\n    indices = indices.astype(int)\n\n    #pick the points out of the arrays\n    md_g = md_g[indices]\n    md_mean = md_mean[indices]\n    md_sig = md_sig[indices]\n    md_cov = md_cov[np.ix_(indices, indices)]\n\n    #plot the check the location of the points\n    if plot is True:\n        plt.xlim(0.,1.)\n        plt.plot(md_g, np.ones(len(md_g)), 'k.')\n\n    return md_g, md_mean, md_sig, md_cov\n</code></pre>"},{"location":"gaussianprocess/#samba.gaussprocess.GP.__init__","title":"<code>__init__(g, loworder, highorder, kernel='RBF', nu=None, ci=68, error_model='informative')</code>","text":"<p>A class that will pull from the Models class to perform GP emulation on  two models from the small-g expansion region to the large-g expansion region.  The parameter settings of the kernel will be set by the user in this  initial function. This class 'wraps' the scikit learn package. </p> Example <p>GP(g=np.linspace(1e-6,1.0,100), loworder=5, highorder=2, kernel=\"Matern\",     ci=68, error_model='informative')</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>numpy linspace</code> <p>The linspace across the coupling constant space  used for the GP.</p> required <code>highorder</code> <code>(ndarray, float, int)</code> <p>The truncation order of the  large-g expansion. </p> required <code>kernel</code> <code>str</code> <p>The type of kernel the user wishes to use. Default is  the RBF kernel; possible choices are RBF, Matern, and Rational  Quadratic. </p> <code>'RBF'</code> <code>nu</code> <code>float</code> <p>The value of the Matern kernel used, if kernel=\"Matern\".  Otherwise, default is None.</p> <code>None</code> <code>ci</code> <code>int</code> <p>The uncertainty interval to use. Must be 68 or 95. </p> <code>68</code> <code>error_model</code> <code>str</code> <p>The error model to be used in the calculation.  Options are 'uninformative' and 'informative'. Default is 'informative'. </p> <code>'informative'</code> <p>Returns:</p> Type Description <p>None.</p> Source code in <code>samba/gaussprocess.py</code> <pre><code>def __init__(self, g, loworder, highorder, kernel=\"RBF\", nu=None, ci=68, error_model='informative'):\n\n    r'''\n    A class that will pull from the Models class to perform GP emulation on \n    two models from the small-g expansion region to the large-g expansion region. \n    The parameter settings of the kernel will be set by the user in this \n    initial function. This class 'wraps' the scikit learn package. \n\n    Example:\n        GP(g=np.linspace(1e-6,1.0,100), loworder=5, highorder=2, kernel=\"Matern\",\n            ci=68, error_model='informative')\n\n    Parameters:\n        g (numpy linspace): The linspace across the coupling constant space \n            used for the GP.\n\n        highorder (numpy.ndarray, float, int): The truncation order of the \n            large-g expansion. \n\n        kernel (str): The type of kernel the user wishes to use. Default is \n            the RBF kernel; possible choices are RBF, Matern, and Rational \n            Quadratic. \n\n        nu (float): The value of the Matern kernel used, if kernel=\"Matern\". \n            Otherwise, default is None.\n\n        ci (int): The uncertainty interval to use. Must be 68 or 95. \n\n        error_model (str): The error model to be used in the calculation. \n            Options are 'uninformative' and 'informative'. Default is 'informative'. \n\n    Returns:\n        None.\n    ''' \n\n    #set up the prediction array as a class variable for use later\n    self.gpredict = np.copy(g)\n\n    #extract uncertainty interval for later use\n    self.ci = ci \n\n    #check type and assign class variables\n    if isinstance(loworder, float) == True or isinstance(loworder, int) == True:\n        loworder = np.array([loworder])\n\n    if isinstance(highorder, float) == True or isinstance(highorder, int) == True:\n        highorder = np.array([highorder])\n\n    self.loworder = loworder \n    self.highorder = highorder \n\n    #Models(), Uncertainties()\n    self.m = Models(self.loworder, self.highorder)\n    self.u = Uncertainties(error_model)\n\n    #instantiate the class variable error_model for ease class crossing\n    self.error_model = self.u.error_model\n\n    #integral length\n    self.gint = np.empty([])\n\n    #kernel set-up for the rest of the class (one-dimensional)\n    kconstant = kernels.ConstantKernel(1.0)\n\n    if kernel == \"RBF\":\n        k = kernels.RBF(length_scale=0.5, length_scale_bounds=(1e-5,1e5))\n    elif kernel == \"Matern\":\n        if nu is None:\n            raise ValueError('Matern kernel must be supplied a value for nu.')\n        else:\n            k = kernels.Matern(length_scale=0.4, length_scale_bounds=(1e-5,1e5), nu=nu)\n    elif kernel == \"Rational Quadratic\":\n        k = kernels.RationalQuadratic(length_scale=1.0, alpha=1)\n    else:\n        raise ValueError('Please choose an available kernel.')\n\n    self.kern = kconstant * k\n\n    return None\n</code></pre>"},{"location":"gaussianprocess/#samba.gaussprocess.GP.create_points","title":"<code>create_points(N, a, b)</code>  <code>staticmethod</code>","text":"<p>A code to create a given number of points from a  linspace evenly from points a to b. </p> Example <p>GP.create_points(N=3, a=0.0, b=1.0)</p> <p>Parameters:</p> Name Type Description Default <code>N</code> <code>int</code> <p>The number of points desired.</p> required <code>a</code> <code>(float, int)</code> <p>The left endpoint of the region of  interest. </p> required <code>b</code> <code>(float, int)</code> <p>The right endpoint of the region of  interest. </p> required <p>Returns:</p> Name Type Description <code>pts</code> <code>ndarray</code> <p>The resulting array of points.</p> Source code in <code>samba/gaussprocess.py</code> <pre><code>@staticmethod\ndef create_points(N, a, b):\n\n    r'''\n    A code to create a given number of points from a \n    linspace evenly from points a to b. \n\n    Example:\n        GP.create_points(N=3, a=0.0, b=1.0)\n\n    Parameters:\n        N (int): The number of points desired.\n\n        a (float, int): The left endpoint of the region of \n            interest. \n\n        b (float, int): The right endpoint of the region of \n            interest. \n\n    Returns:\n        pts (numpy.ndarray): The resulting array of points. \n    '''\n\n    #create the linspace with endpoints\n    pts_array = np.linspace(a, b, N+2)\n\n    #remove the first and last point\n    pts = pts_array[1:-1]\n\n    return pts\n</code></pre>"},{"location":"gaussianprocess/#samba.gaussprocess.GP.mahalanobis","title":"<code>mahalanobis(y, mean, inv=None, chol=False, svd=False)</code>  <code>staticmethod</code>","text":"<p>A diagnostic testing function that can calculate the Mahalanobis  distance for a given set of mean, covariance data and a vector. </p> 1). Calculate the MD of the predictions of the GP using <p>the inverse covariance matrix (usual method);</p> <pre><code>  2). Calculate the MD of the predictions to construct a \n      reference distribution using the inverse covariance\n      matrix (usual method);\n  3). Calculate the Cholesky decomposition of the MD\n      information;\n  4). Perform an SVD analysis and send back the MD \n      calculated via SVD.\n</code></pre> Example <p>GP.MD(y=np.array([]), mean=np.array([]), inv=numpy.ndarray([]),     chol=False, svd=False)</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ndarray</code> <p>An array of predicted values from the emulator.</p> required <code>mean</code> <code>ndarray</code> <p>An array of true values from the true model  (simulator).</p> required <code>inv</code> <code>ndarray</code> <p>The covariance matrix to be inverted in the  MD calculation.</p> <code>None</code> <code>chol</code> <code>bool</code> <p>The option to calculate the Cholesky decomposition of the data. </p> <code>False</code> <code>svd</code> <code>bool</code> <p>An option to perform the SVD analysis of the MD data. To use, must also have a covariance matrix sent to inv. </p> <code>False</code> <p>Returns:</p> Name Type Description <code>md</code> <code>float</code> <p>(if calculating MD) The Mahalanobis distance. </p> <code>chol_decomp</code> <code>ndarray</code> <p>(if calculating Cholesky decomposition)  The Cholesky decomposition results. </p> <code>svderrs</code> <code>ndarray</code> <p>(if calculating SVD) The SVD errors at each  point in the MD testing set. </p> <p>svd_md (float) (if calculating SVD) The Mahalanobis distance.</p> Source code in <code>samba/gaussprocess.py</code> <pre><code>@staticmethod\ndef mahalanobis(y, mean, inv=None, chol=False, svd=False):\n\n    r'''\n    A diagnostic testing function that can calculate the Mahalanobis \n    distance for a given set of mean, covariance data and a vector. \n\n    Uses: 1). Calculate the MD of the predictions of the GP using\n              the inverse covariance matrix (usual method);\n          2). Calculate the MD of the predictions to construct a \n              reference distribution using the inverse covariance\n              matrix (usual method);\n          3). Calculate the Cholesky decomposition of the MD\n              information;\n          4). Perform an SVD analysis and send back the MD \n              calculated via SVD. \n\n    Example:\n        GP.MD(y=np.array([]), mean=np.array([]), inv=numpy.ndarray([]),\n            chol=False, svd=False)\n\n    Parameters:\n        y (numpy.ndarray): An array of predicted values from the emulator.\n\n        mean (numpy.ndarray): An array of true values from the true model \n            (simulator).\n\n        inv (numpy.ndarray): The covariance matrix to be inverted in the \n            MD calculation.\n\n        chol (bool): The option to calculate the Cholesky decomposition\n            of the data. \n\n        svd (bool): An option to perform the SVD analysis of the MD data.\n            To use, must also have a covariance matrix sent to inv. \n\n    Returns:\n        md (float): (if calculating MD) The Mahalanobis distance. \n\n        chol_decomp (numpy.ndarray): (if calculating Cholesky decomposition) \n            The Cholesky decomposition results. \n\n        svderrs (numpy.ndarray): (if calculating SVD) The SVD errors at each   \n            point in the MD testing set. \n\n        svd_md (float) (if calculating SVD) The Mahalanobis distance. \n    '''\n\n    y = np.atleast_2d(y)\n\n    #cholesky option (solves for Cholesky decomposition)\n    if (inv is not None) and (chol is True):\n\n        chol = cholesky(inv)\n        errs = scl.solve_triangular(chol, (y-mean).T, lower=True).T\n        chol_decomp = np.linalg.norm(errs, axis=-1)\n\n        return chol_decomp \n\n    #SVD option\n    if (svd is True) and (inv is not None):\n\n        #perform SVD\n        _, s, vh = np.linalg.svd(inv)\n        print('Eigenvalues: ',s)\n        sinv = np.linalg.inv(np.diag(s))   #inverse of eigenvalue matrix\n        one = vh @ (y-mean).T\n        svd_md = np.squeeze(one.T @ sinv @ one)\n        print('MD^2 (SVD): ', svd_md)\n\n        #SVD errors\n        svderrs = np.zeros([len(s)])\n        for i in range(len(s)):\n            svderrs[i] = np.square(1.0/np.sqrt(s[i]) * np.dot(vh[i,:],(y-mean).T))\n\n        return svderrs, svd_md\n\n    #inverse option (normal MD calculation)\n    if (chol is False) and (svd is False) and (inv is not None):\n\n        md = np.squeeze(np.sqrt(np.diag((y - mean) @ np.linalg.inv(inv) @ (y - mean).T)))\n\n        return md\n\n    #if nothing is selected\n    if (inv is None):\n        raise ValueError('Please input a covariance matrix.')\n</code></pre>"},{"location":"gaussianprocess/#samba.gaussprocess.GP.md_plotter","title":"<code>md_plotter(md_gp, md_ref, md_mean=None, md_cov=None, hist=True, box=False)</code>","text":"<p>A plotting function that allows the Mahalanobis distance to be plotted using either a histogram or a box and whisker plot, or both. </p> <p>Box and whisker plot code heavily drawn from J. Melendez' gsum code (https://github.com/buqeye/gsum).</p> Example <p>GP.md_plotter(md_gp=np.array([]), md_ref=np.array([]), hist=False, box=True)</p> <p>Parameters:</p> Name Type Description Default <code>md_gp</code> <code>float</code> <p>The MD^2 value for the GP curve. </p> required <code>md_ref</code> <code>ndarray</code> <p>The array of MD^2 values for the reference distribution.</p> required <code>md_mean</code> <code>ndarray</code> <p>The values of the GP mean at the md_g points.  Only used for box and whisker option; default is None. </p> <code>None</code> <code>md_cov</code> <code>ndarray</code> <p>The values of the GP covariance matrix at the  md_g points. Only used for box and whisker option; default is None.</p> <code>None</code> <code>hist</code> <code>bool</code> <p>Toggle for plotting a histogram. Default is True. </p> <code>True</code> <code>box</code> <code>bool</code> <p>Toggle for plotting a box plot. Default is False. </p> <code>False</code> <p>Returns:</p> Type Description <p>None.</p> Source code in <code>samba/gaussprocess.py</code> <pre><code>def md_plotter(self, md_gp, md_ref, md_mean=None, md_cov=None, hist=True, box=False):\n\n    r'''\n    A plotting function that allows the Mahalanobis distance\n    to be plotted using either a histogram or a box and whisker\n    plot, or both. \n\n    Box and whisker plot code heavily drawn from J. Melendez' gsum\n    code (https://github.com/buqeye/gsum).\n\n    Example:\n        GP.md_plotter(md_gp=np.array([]), md_ref=np.array([]),\n        hist=False, box=True)\n\n    Parameters:\n        md_gp (float): The MD^2 value for the GP curve. \n\n        md_ref (numpy.ndarray): The array of MD^2 values for the reference\n            distribution.\n\n        md_mean (numpy.ndarray): The values of the GP mean at the md_g points. \n            Only used for box and whisker option; default is None. \n\n        md_cov (numpy.ndarray): The values of the GP covariance matrix at the \n            md_g points. Only used for box and whisker option; default is None.\n\n        hist (bool): Toggle for plotting a histogram. Default is True. \n\n        box (bool): Toggle for plotting a box plot. Default is False. \n\n    Returns:\n        None.\n    '''\n\n    title = 'Mahalanobis Distance'\n    xlabel = r'$\\mathrm{D}_{\\mathrm{MD}}^{2}$'\n\n    #histogram option\n    if hist is True:\n        fig = plt.figure(figsize=(8,6), dpi=600)\n        fig.patch.set_facecolor('white')\n        ax = plt.axes()\n        ax.set_xlabel(xlabel, fontsize=18)\n        ax.set_title(title, fontsize=22)\n        ax.set_xlim(0.0, max(md_ref))\n        ax.hist(md_ref, bins=50, density=True, histtype='bar', facecolor='black', \\\n                ec='white', label='Reference distribution')\n        ax.plot(md_gp, 0.0, marker='o', color='r', markersize=10)\n\n        #add chi-squared to histogram\n        n = 200\n        x = np.linspace(0.0, max(md_ref), n)\n        ax.plot(x, stats.chi2.pdf(x, df=self.lenpts), 'r', linewidth=2, label=r'$\\chi^2$ (df={})'.format(self.lenpts))\n\n        #include legend\n        legend = True\n\n    #box-and-whisker option\n    if box is True:\n\n        dist = self.ref_dist(md_mean, md_cov)\n\n        legend = False\n\n        #set up the figure\n        fig = plt.figure(figsize=(8,6), dpi=100)\n        ax = plt.axes()\n        ax.set_xlabel(xlabel, fontsize=18)\n\n        #reference distribution (using chi2, NOT md_ref)\n        boxartist = self.ref_boxplot(dist, ax=ax, patch_artist=True, widths=0.8)\n        gray = 'gray'\n        for box in boxartist['boxes']:\n            box.update(dict(facecolor='lightgrey', edgecolor=gray))\n        for whisk in boxartist[\"whiskers\"]:\n            whisk.update(dict(color=gray))\n        for cap in boxartist[\"caps\"]:\n            cap.update(dict(color=gray))\n        for med in boxartist[\"medians\"]:\n            med.update(dict(color=gray))\n\n        #ax.boxplot(md_ref, showfliers=False)\n        ax.get_xaxis().set_ticks([])\n        ax.tick_params(direction='in')\n        ax.set_ylim(0,20)\n        ax.set_aspect(0.25)\n        sns.despine(offset=0, bottom=True, ax=ax)\n\n        #plot the individual GP MD value\n        ax.plot(1.0, md_gp, color='red', marker='o', markersize=10)\n\n    #finish up plot\n    if legend is True:\n        ax.legend(loc='upper right', fontsize=18)\n\n    plt.show()\n\n    return None\n</code></pre>"},{"location":"gaussianprocess/#samba.gaussprocess.GP.md_squared","title":"<code>md_squared(md_g, md_mean, md_cov, n_curves=1000)</code>","text":"<p>A wrapper for the Mahalanobis distance calculation for the reference distribution and the GP curve. To calculate the  Cholesky decomposition or to perform an SVD analysis, consult GP.mahalanobis() below. </p> Example <p>GP.md_squared(md_g=np.linspace, md_mean=np.array([]),                md_cov=np.array([,]), n_curves=1000)</p> <p>Parameters:</p> Name Type Description Default <code>md_g</code> <code>linspace</code> <p>The points in input space g from the GP.MD_set()  function. </p> required <code>md_mean</code> <code>ndarray</code> <p>The values of the GP mean at the md_g points. </p> required <code>md_cov</code> <code>ndarray</code> <p>The values of the GP covariance matrix at the  md_g points. </p> required <code>n_curves</code> <code>int</code> <p>The number of curves from the reference distribution that are drawn for the MD^2 calculation (md_ref). </p> <code>1000</code> <p>Returns:</p> Name Type Description <code>md_gp</code> <code>float</code> <p>The individual MD^2 value for the GP curve. </p> <p>md_ref (numpy.ndarray): The array of MD^2 values from the reference      distribution.</p> Source code in <code>samba/gaussprocess.py</code> <pre><code>def md_squared(self, md_g, md_mean, md_cov, n_curves=1000):\n\n    r'''\n    A wrapper for the Mahalanobis distance calculation for the\n    reference distribution and the GP curve. To calculate the \n    Cholesky decomposition or to perform an SVD analysis, consult\n    GP.mahalanobis() below. \n\n    Example:\n        GP.md_squared(md_g=np.linspace, md_mean=np.array([]), \n                      md_cov=np.array([,]), n_curves=1000)\n\n    Parameters:\n        md_g (numpy.linspace): The points in input space g from the GP.MD_set() \n            function. \n\n        md_mean (numpy.ndarray): The values of the GP mean at the md_g points. \n\n        md_cov (numpy.ndarray): The values of the GP covariance matrix at the \n            md_g points. \n\n        n_curves (int): The number of curves from the reference distribution that\n            are drawn for the MD^2 calculation (md_ref). \n\n    Returns:\n        md_gp (float): The individual MD^2 value for the GP curve. \n\n    md_ref (numpy.ndarray): The array of MD^2 values from the reference \n        distribution.\n    '''\n\n    #calculate the ref distribution MDs\n    dist = self.ref_dist(md_mean, md_cov)\n    y = self.sample_ref(dist, n_curves)\n    md = np.ones([n_curves])\n    for i in range(n_curves):\n        md[i] = self.mahalanobis(y[:,i].T, md_mean, inv=md_cov, chol=False, svd=False)\n\n    #MD^2 (ref)\n    md_ref = md**2.0 \n\n    #calculate the GP MD \n    fval = self.m.true_model(md_g)\n    mdgp = self.mahalanobis(fval.T, md_mean, inv=md_cov, chol=False, svd=False)\n\n    #MD^2 (GP)\n    md_gp = mdgp**2.0\n\n    return md_gp, md_ref\n</code></pre>"},{"location":"gaussianprocess/#samba.gaussprocess.GP.nearest_value","title":"<code>nearest_value(array, value)</code>  <code>staticmethod</code>","text":"<p>A static method to find the index of the nearest value of an array to a desired value. </p> Example <p>GP.nearest_value(array=numpy.ndarray, value=5)</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>The array of values to search. </p> required <code>value</code> <code>int</code> <p>The desired value to search the array for. </p> required <p>index (int): The index of the nearest value of the array     to the desired value.</p> Source code in <code>samba/gaussprocess.py</code> <pre><code>@staticmethod\ndef nearest_value(array, value):\n\n    r'''\n    A static method to find the index of the nearest value\n    of an array to a desired value. \n\n    Example:\n        GP.nearest_value(array=numpy.ndarray, value=5)\n\n    Parameters:\n        array (numpy.ndarray): The array of values to search. \n\n        value (int): The desired value to search the array for. \n\n    Returns:\n    index (int): The index of the nearest value of the array\n        to the desired value. \n    '''\n\n    #calculate the difference between each point\n    abs_val = np.abs(array - value)\n\n    #find the smallest difference in the array\n    index = abs_val.argmin()\n\n    return index\n</code></pre>"},{"location":"gaussianprocess/#samba.gaussprocess.GP.plot_training","title":"<code>plot_training(gs, datas, sigmas)</code>","text":"<p>A simple plotter to plot the trained GP results and models,  as well as the points at which the GP was trained. </p> Example <p>GP.plot_training(gs=np.array([]), datas=np.array([]),     sigmas=np.array([]))</p> <p>Parameters:</p> Name Type Description Default <code>gs</code> <code>ndarray</code> <p>Points chosen by GP.training_set() in input  space g.</p> required <code>datas</code> <code>ndarray</code> <p>Corresponding values of the series expansions  at gs.</p> required <code>sigmas</code> <code>ndarray</code> <p>Corresponding error model results at each  training point.</p> required <p>Returns:</p> Type Description <p>None.</p> Source code in <code>samba/gaussprocess.py</code> <pre><code>def plot_training(self, gs, datas, sigmas):\n\n    r'''\n    A simple plotter to plot the trained GP results and models, \n    as well as the points at which the GP was trained. \n\n    Example:\n        GP.plot_training(gs=np.array([]), datas=np.array([]),\n            sigmas=np.array([]))\n\n    Parameters:\n        gs (numpy.ndarray): Points chosen by GP.training_set() in input \n            space g.\n\n        datas (numpy.ndarray): Corresponding values of the series expansions \n            at gs.\n\n        sigmas (numpy.ndarray): Corresponding error model results at each \n            training point.\n\n    Returns:\n        None.\n    '''\n\n    #set up the plot\n    fig = plt.figure(figsize=(8,6), dpi=600)\n    ax = plt.axes()\n    fig.patch.set_facecolor('white')\n    ax.tick_params(axis='x', labelsize=18)\n    ax.tick_params(axis='y', labelsize=18)\n    ax.locator_params(nbins=8)\n    ax.xaxis.set_minor_locator(AutoMinorLocator())\n    ax.yaxis.set_minor_locator(AutoMinorLocator())\n    ax.set_xlim(0.0, max(self.gpredict))\n    ax.set_ylim(1.0,3.0)\n    ax.set_xlabel('g', fontsize=22)\n    ax.set_ylabel('F(g)', fontsize=22)\n    ax.set_title('F(g): training set', fontsize=22)\n    ax.plot(self.gpredict, self.m.true_model(self.gpredict), 'k', label='True model')\n\n    #plot the data\n    ax.errorbar(self.gtrlow, self.datatrlow, yerr=self.lowsigma, color='red', fmt='o', markersize=4, \\\n                capsize=4, label=r'$f_s$ ($N_s$ = {}) data'.format(self.loworder[0]))\n    ax.errorbar(self.gtrhigh, self.datatrhigh, yerr=self.highsigma, color='blue', fmt='o', markersize=4, \\\n                capsize=4, label=r'$f_l$ ($N_l$ = {}) data'.format(self.highorder[0]))\n\n    #plot the chosen training points over the whole training set\n    ax.errorbar(gs, datas, yerr=sigmas, color='black', fmt='o', markersize=4, capsize=4, label='Training data')\n\n    ax.legend(fontsize=18, loc='upper right')\n    plt.show()\n\n    #save figure option\n    # response = input('Would you like to save this figure? (yes/no)')\n\n    # if response == 'yes':\n    #     name = input('Enter a file name (include .jpg, .png, etc.)')\n    #     fig.savefig(name, bbox_inches='tight')\n\n    return None\n</code></pre>"},{"location":"gaussianprocess/#samba.gaussprocess.GP.plot_validate","title":"<code>plot_validate(intervals)</code>","text":"<p>A simple plotter to show the results of the GP  predictions at new points in g. </p> Example <p>GP.plot_validate(intervals=np.array([,]))</p> <p>Parameters:</p> Name Type Description Default <code>intervals</code> <code>ndarray</code> <p>The uncertainty band around the  prediction set.</p> required <p>Returns:</p> Type Description <p>None.</p> Source code in <code>samba/gaussprocess.py</code> <pre><code>def plot_validate(self, intervals):\n\n    r'''\n    A simple plotter to show the results of the GP \n    predictions at new points in g. \n\n    Example:\n        GP.plot_validate(intervals=np.array([,]))\n\n    Parameters:\n        intervals (numpy.ndarray): The uncertainty band around the \n            prediction set.\n\n    Returns:\n        None.\n    '''\n\n    #plot the results\n    fig = plt.figure(figsize=(8,6), dpi=600)\n    ax = plt.axes()\n    fig.patch.set_facecolor('white')\n    ax.tick_params(axis='x', labelsize=18)\n    ax.tick_params(axis='y', labelsize=18)\n    ax.locator_params(nbins=8)\n    ax.xaxis.set_minor_locator(AutoMinorLocator())\n    ax.yaxis.set_minor_locator(AutoMinorLocator())\n    ax.set_xlim(0.0, max(self.gpredict))\n    ax.set_ylim(1.0,3.0)\n    ax.set_xlabel('g', fontsize=22)\n    ax.set_ylabel('F(g)', fontsize=22)\n    ax.set_title('F(g): GP predictions', fontsize=22)\n    ax.plot(self.gpredict, self.m.true_model(self.gpredict), 'k', label='True model')\n\n    #plot the data\n    ax.errorbar(self.gtrlow, self.datatrlow, self.lowsigma, color=\"red\", fmt='o', markersize=4, \\\n        capsize=4, alpha = 0.4, label=r\"$f_s$ ($N_s$ = {})\".format(self.loworder[0]), zorder=1)\n    ax.errorbar(self.gtrhigh, self.datatrhigh, self.highsigma, color=\"blue\", fmt='o', markersize=4, \\\n         capsize=4, alpha=0.4, label=r\"$f_l$ ($N_l$ = {})\".format(self.highorder[0]), zorder=1)\n    ax.plot(self.gpred, self.meanp, 'g', label='Predictions', zorder=2)\n    ax.plot(self.gpred, intervals[:,0], color='green', linestyle='dotted', label=r'{}$\\%$ CI'.format(self.ci), zorder=2)\n    ax.plot(self.gpred, intervals[:,1], color='green', linestyle='dotted', zorder=2)\n    ax.fill_between(self.gpred[:,0], intervals[:,0], intervals[:,1], color='green', alpha=0.3, zorder=10)\n\n    ax.legend(fontsize=18, loc='upper right')\n    plt.show()\n\n    #save figure option\n    # response = input('Would you like to save this figure? (yes/no)')\n\n    # if response == 'yes':\n    #     name = input('Enter a file name (include .jpg, .png, etc.)')\n    #     fig.savefig(name, bbox_inches='tight')\n\n    return None\n</code></pre>"},{"location":"gaussianprocess/#samba.gaussprocess.GP.ref_boxplot","title":"<code>ref_boxplot(dist, q1=0.25, q3=0.75, whislo=0.025, whishi=0.975, ax=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Taken from the gsum code written by J. Melendez (https://github.com/buqeye/gsum).</p> Source code in <code>samba/gaussprocess.py</code> <pre><code>@staticmethod \ndef ref_boxplot(dist, q1=0.25, q3=0.75, whislo=0.025, whishi=0.975, ax=None, **kwargs):\n\n    r'''\n    Taken from the gsum code written by J. Melendez (https://github.com/buqeye/gsum).\n    '''\n\n    stat_dict = [{'med': dist.median(), 'q1': dist.ppf(q1), 'q3': dist.ppf(q3),\n                  'whislo': dist.ppf(whislo), 'whishi': dist.ppf(whishi)}]\n\n    return ax.bxp(stat_dict, showfliers=False, **kwargs)\n</code></pre>"},{"location":"gaussianprocess/#samba.gaussprocess.GP.ref_dist","title":"<code>ref_dist(mean, cov)</code>  <code>staticmethod</code>","text":"<p>Constructs a multivariate normal distribution to act as a reference distribution for the Mahalanobis distance calculation. </p> Example <p>Diagnostics.ref_dist(mean=np.array([]), cov=np.array([]))</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>ndarray</code> <p>The mean of the GP (given by the  prediction set). </p> required <code>cov</code> <code>ndarray</code> <p>The covariance matrix of the GP  (given by the prediction set). </p> required <p>Returns:</p> Name Type Description <code>dist</code> <code>object</code> <p>A multivariate normal distribution that can  be used to generate samples for the reference distribution.</p> Source code in <code>samba/gaussprocess.py</code> <pre><code>@staticmethod\ndef ref_dist(mean, cov):\n\n    r'''\n    Constructs a multivariate normal distribution to act\n    as a reference distribution for the Mahalanobis distance\n    calculation. \n\n    Example:\n        Diagnostics.ref_dist(mean=np.array([]), cov=np.array([]))\n\n    Parameters:\n        mean (numpy.ndarray): The mean of the GP (given by the \n            prediction set). \n\n        cov (numpy.ndarray): The covariance matrix of the GP \n            (given by the prediction set). \n\n    Returns:\n        dist (object): A multivariate normal distribution that can \n            be used to generate samples for the reference distribution. \n    '''\n\n    dist = stats.multivariate_normal(mean=mean, cov=cov)\n\n    return dist\n</code></pre>"},{"location":"gaussianprocess/#samba.gaussprocess.GP.sample_ref","title":"<code>sample_ref(dist, n_curves)</code>  <code>staticmethod</code>","text":"<p>Generate some sample curves from the reference distribution.</p> Example <p>Diagnostics.sample_ref(dist, n_curves=10)</p> <p>Parameters:</p> Name Type Description Default <code>dist</code> <code>object</code> <p>The reference distribution object. </p> required <code>n_curves</code> <code>int</code> <p>The number of draws from the reference  distribution.</p> required <p>Returns:</p> Name Type Description <code>samples</code> <code>ndarray</code> <p>The array of curves from the  distribution.</p> Source code in <code>samba/gaussprocess.py</code> <pre><code>@staticmethod\ndef sample_ref(dist, n_curves):\n\n    r'''\n    Generate some sample curves from the reference distribution.\n\n    Example:\n        Diagnostics.sample_ref(dist, n_curves=10)\n\n    Parameters:\n        dist (object): The reference distribution object. \n\n        n_curves (int): The number of draws from the reference \n            distribution.\n\n    Returns:\n        samples (numpy.ndarray): The array of curves from the \n            distribution. \n    '''\n\n    samples = dist.rvs(n_curves).T\n\n    return samples\n</code></pre>"},{"location":"gaussianprocess/#samba.gaussprocess.GP.training","title":"<code>training(error=True, method=2, plot=True)</code>","text":"<p>A function that links the model data and the training function in  scikit learn, and plots the training data using GP.plot_training().</p> Example <p>GP.training(error=False, method=3)</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>bool</code> <p>A boolean variable to toggle use of a truncation error model  in the kernel during training. Default is True.</p> <code>True</code> <code>method</code> <code>int</code> <p>The method used for determining the training points. Options:  1,2,3. For an extensive explanation of the methods, see the paper.</p> <code>2</code> <code>plot</code> <code>bool</code> <p>Option to plot the training set with series expansions and  true model. Default is True. </p> <code>True</code> <p>Returns:</p> Name Type Description <code>sk</code> <code>object</code> <p>The object storing all training information from the sklearn  regression performed on the data.</p> Source code in <code>samba/gaussprocess.py</code> <pre><code>    def training(self, error=True, method=2, plot=True):\n\n        r'''\n        A function that links the model data and the training function in \n        scikit learn, and plots the training data using GP.plot_training().\n\n        Example:\n            GP.training(error=False, method=3)\n\n        Parameters:\n            error (bool): A boolean variable to toggle use of a truncation error model \n                in the kernel during training. Default is True.\n\n            method (int): The method used for determining the training points. Options: \n                1,2,3. For an extensive explanation of the methods, see the paper.\n\n            plot (bool): Option to plot the training set with series expansions and \n                true model. Default is True. \n\n        Returns:\n            sk (object): The object storing all training information from the sklearn \n                regression performed on the data.\n        '''\n\n        #first set the method\n        self.method = method \n\n        #call the training set generator function\n        gs, datas, sigmas = self.training_set()\n\n#         ### specific test ###\n#         # split up the set\n#         gs = np.array([gs[1], gs[2], gs[3]])\n#         datas = np.array([datas[1], datas[2], datas[3]])\n#         sigmas = np.array([sigmas[1], sigmas[2], sigmas[3]])\n\n        #make a gs class variable for weights use\n        self.gs = gs \n        self.datas = datas\n        self.sigmas = sigmas\n\n        #make column vectors for the regressor\n        gc = gs.reshape(-1,1)\n        datac = datas.reshape(-1,1)\n\n        #take the data point uncertainty into the kernel \n        if error == True:\n            self.alpha = np.square(sigmas)\n        else:\n            self.alpha = 1e-12\n\n        #use GPR and kernel to train\n        m = GaussianProcessRegressor(kernel=self.kern, alpha=self.alpha, n_restarts_optimizer=20, normalize_y=True)\n\n        #fit the GP to the training data\n        self.sk = m.fit(gc, datac)\n\n        #print the optimized parameters for the user\n        print('Gaussian process parameters: {}'.format(m.kernel_))\n\n        #plot the results\n        if plot is True:\n            self.plot_training(gs, datas, sigmas)\n\n        return self.sk\n</code></pre>"},{"location":"gaussianprocess/#samba.gaussprocess.GP.training_set","title":"<code>training_set()</code>","text":"<p>An internal function to calculate the necessary training data set from the input prediction set. </p> Example <p>GP.training_set() </p> <p>Returns: gs (numpy.ndarray): The modified array of input values for the training. </p> <p>datas (numpy.ndarray): The modified array of data values for the training. </p> <p>sigmas (numpy.ndarray): The modified array of the truncation errors for      the training.</p> Source code in <code>samba/gaussprocess.py</code> <pre><code>def training_set(self):\n\n    r'''\n    An internal function to calculate the necessary training data set from\n    the input prediction set. \n\n    Example:\n        GP.training_set() \n\n    Parameters:\n        None. \n\n    Returns:\n    gs (numpy.ndarray): The modified array of input values for the training. \n\n    datas (numpy.ndarray): The modified array of data values for the training. \n\n    sigmas (numpy.ndarray): The modified array of the truncation errors for \n        the training. \n    '''\n\n    #set up the training set from the prediction set (offset by midpoint)\n    self.midpoint = (self.gpredict[1] - self.gpredict[0]) / 2.0\n    gtrainingset = np.linspace(min(self.gpredict)+self.midpoint, max(self.gpredict)+self.midpoint, len(self.gpredict))\n\n    #stop the training set, negative curvature\n    if self.loworder[0] % 4 == 2 or self.loworder[0] % 4 == 3:\n        for i in range(len(gtrainingset)):\n            if self.m.low_g(gtrainingset[i]) &lt; -1.0:\n                lowindex = i-1\n                break\n\n    #stop the training set, positive curvature\n    elif self.loworder[0] % 4 == 0 or self.loworder[0] % 4 == 1:\n        for i in range(len(gtrainingset)):\n            if self.m.low_g(gtrainingset[i]) &gt; 3.0:\n                lowindex = i-1\n                break\n\n    #stop the training set, even orders (positive curvature)\n    if self.highorder[0] % 2 == 0:\n        for i in range(len(gtrainingset)):\n            if self.m.high_g(gtrainingset[i]) &gt; 3.0:\n                highindex = i+1\n            else:\n                break\n\n    #stop the training set, odd orders (negative curvature)\n    else:\n        for i in range(len(gtrainingset)):\n            if self.m.high_g(gtrainingset[i]) &lt; -1.0:\n                highindex = i+1\n            else:\n                break\n\n    #slice the training set for the two models\n    self.gtrlow = gtrainingset[:lowindex]\n    self.gtrhigh = gtrainingset[highindex:]\n\n    #calculate the data at each point\n    self.datatrlow = self.m.low_g(self.gtrlow)[0,:]\n    self.datatrhigh = self.m.high_g(self.gtrhigh)[0,:]\n\n    #calculate the variance at each point from the next term\n    lowvariance = self.u.variance_low(self.gtrlow, self.loworder[0])\n    self.lowsigma = np.sqrt(lowvariance)\n    highvariance = self.u.variance_high(self.gtrhigh, self.highorder[0])\n    self.highsigma = np.sqrt(highvariance)\n\n    #find the values of g in the other set to determine location of points\n    index_ghigh = (np.where(self.gtrhigh == self.gtrlow[-1])[0])[0]\n\n    #value of g at the optimal red points\n    pt1 = 0.0656575\n    pt2 = 0.1161625\n\n    #method 1: using g=0.6 as a training point\n    pttest = 0.6  \n    indexptest = self.nearest_value(self.gtrhigh, pttest) \n\n    #method 3: finding based on error (5%)\n    for i in range(len(self.gtrhigh)-1, -1, -1):\n        if self.highsigma[i] &gt;= 0.05*self.datatrhigh[i]:\n            indexerror = i\n            break \n\n    #find the values in the training array closest to the points\n    indexpt1 = self.nearest_value(self.gtrlow, pt1)\n    indexpt2 = self.nearest_value(self.gtrlow, pt2)\n\n    #create two points on either side (highpoint = 20)\n    glowtr = np.array([self.gtrlow[indexpt1], self.gtrlow[indexpt2]])\n    datalowtr = np.array([self.datatrlow[indexpt1], self.datatrlow[indexpt2]])\n    sigmalowtr = np.array([self.lowsigma[indexpt1], self.lowsigma[indexpt2]])\n\n    #choose training points depending on method entered\n    if self.method == 1:\n        ghightr = np.array([self.gtrhigh[indexptest], self.gtrhigh[-1]])\n        datahightr = np.array([self.datatrhigh[indexptest], self.datatrhigh[-1]])\n        sigmahightr = np.array([self.highsigma[indexptest], self.highsigma[-1]])\n\n    elif self.method == 2:\n        ghightr = np.array([self.gtrhigh[index_ghigh], self.gtrhigh[-1]])\n        datahightr = np.array([self.datatrhigh[index_ghigh], self.datatrhigh[-1]])\n        sigmahightr = np.array([self.highsigma[index_ghigh], self.highsigma[-1]])\n\n    elif self.method == 3:\n        ghightr = np.array([self.gtrhigh[indexerror], self.gtrhigh[-1]])\n        datahightr = np.array([self.datatrhigh[indexerror], self.datatrhigh[-1]])\n        sigmahightr = np.array([self.highsigma[indexerror], self.highsigma[-1]])\n\n    #concatenate these arrays and send back\n    gtr = np.concatenate((glowtr, ghightr))\n    datatr = np.concatenate((datalowtr, datahightr))\n    sigmatr = np.concatenate((sigmalowtr, sigmahightr))\n\n    return gtr, datatr, sigmatr \n</code></pre>"},{"location":"gaussianprocess/#samba.gaussprocess.GP.validate","title":"<code>validate(plot=True, run_taweret=False)</code>","text":"<p>A wrapper function for scikit learn's GP prediction function. This will  predict the GP results with an interval and plot against the expansions using GP.plot_validate().</p> Example <p>GP.validate()</p> <p>Parameters:</p> Name Type Description Default <code>plot</code> <code>bool</code> <p>The option to plot the GP mean and variance over the testing set and true model. Default is True. </p> <code>True</code> <p>Returns:</p> Name Type Description <code>meanp</code> <code>ndarray</code> <p>The mean array of the GP prediction results.</p> <code>sigp</code> <code>ndarray</code> <p>The standard deviation array of the GP prediction  results. </p> <code>cov</code> <code>ndarray</code> <p>The covariance matrix of the GP prediction results.</p> Source code in <code>samba/gaussprocess.py</code> <pre><code>def validate(self, plot=True, run_taweret=False):\n\n    r'''\n    A wrapper function for scikit learn's GP prediction function. This will \n    predict the GP results with an interval and plot against the expansions\n    using GP.plot_validate().\n\n    Example:\n        GP.validate()\n\n    Parameters:\n        plot (bool): The option to plot the GP mean and variance over the testing\n            set and true model. Default is True. \n\n    Returns:\n        meanp (numpy.ndarray): The mean array of the GP prediction results.\n\n        sigp (numpy.ndarray): The standard deviation array of the GP prediction \n            results. \n\n        cov (numpy.ndarray): The covariance matrix of the GP prediction results. \n    '''\n\n    #make the prediction values into a column vector\n    self.gpred = self.gpredict.reshape(-1,1)\n\n    #predict the results for the validation data\n    self.meanp, self.sigp = self.sk.predict(self.gpred, return_std=True)\n    _, self.cov = self.sk.predict(self.gpred, return_cov=True)\n\n    # issues right here when running wrapped in Taweret\n    if run_taweret is False:\n        self.meanp = self.meanp #[:,0]\n\n    #calculate the interval for the predictions\n    if self.ci == 68:\n        factor = 1.0\n    elif self.ci == 95:\n        factor = 1.96\n    intervals = np.zeros([len(self.meanp), 2])\n    intervals[:,0] = self.meanp - factor*self.sigp\n    intervals[:,1] = self.meanp + factor*self.sigp\n\n    #plot the results\n    if plot is True:\n        self.plot_validate(intervals)\n\n    return self.meanp, self.sigp, self.cov\n</code></pre>"},{"location":"mixing/","title":"Method 1: Linear model mixing","text":"<p>This method is derived from Coleman's thesis, and  uses a mixing function with hyperparameters to be estimated using data, to construct the mixed model. There are several possible mixing functions given in the code below for a user to play with and build off of to write their own mixing function. There are also priors to choose from (in the <code>priors.py</code> file) for the hyperparameters of each mixing function.</p> <p>Once the mixing function has been chosen, and data supplied or simulated, the user can construct the mixed model by sampling the parameter space using the sampler wrapper below, and then building the posterior predictive distribution (PPD). This is given as</p> <p>$$ p(\\tilde y(g)|\\theta, \\mathbf{D}) = \\sum_{j=1}^{M} \\alpha(g; \\theta_{j}) F^{N_s}(g) + (1 - \\alpha(g; \\theta_{j})) F^{N_l}_{l}(g), $$</p> <p>where $\\alpha(g; \\theta_{j})$ is the chosen mixing function with hyperparameters $\\theta_{j}$.</p>"},{"location":"mixing/#samba.mixing.LMM","title":"<code>LMM</code>","text":"<p>               Bases: <code>Models</code>, <code>Uncertainties</code></p> Source code in <code>samba/mixing.py</code> <pre><code>class LMM(Models, Uncertainties): \n\n\n    def __init__(self, loworder, highorder, error_model='informative'):\n\n        r'''\n        This class is designed with all of the necessary functions for creating \n        a data set, plotting it along with the true model, and calculating expansions \n        of specific orders of the true model to mix. Dependent on the Models class to \n        run the expansion functions. \n\n        Example:            \n            LMM(loworder=np.array([2]), highorder=np.array([2]), error_model='informative')\n\n        Parameters:\n            loworder (numpy.ndarray, int): The truncation order to which we calculate \n                the small-g expansion. \n\n            highorder (numpy.ndarray, int): The truncation order to which we calculate the \n                large-g expansion. \n\n            error_model (str): The error model chosen for this calculation. Can be either \n                'uninformative' or 'informative'. Default is 'informative'. \n\n        Returns:\n            None.\n        '''    \n\n        #check type and create class variables\n        if isinstance(loworder, float) == True or isinstance(loworder, int) == True:\n            loworder = np.array([loworder])\n\n        if isinstance(highorder, float) == True or isinstance(highorder, int) == True:\n            highorder = np.array([highorder])\n\n        self.loworder = loworder \n        self.highorder = highorder\n\n        #instantiate the Models() and Priors() classes here\n        self.m = Models(self.loworder, self.highorder)\n        self.u = Uncertainties(error_model)\n        self.p = Priors()\n\n        return None\n\n\n    def add_data(self, g_true, g_data, data=None, sigma=None, error=None, plot=True):\n\n        r'''\n        A data generation function that generates data based on the g_data linspace \n        provided (with the number of points chosen by the user) and the error desired on \n        each point (also input by the user), or accepts the user's input of an array of data \n        and standard deviations of the data points. \n\n        Example:\n            LMM.add_data(g_true=np.linspace(0.0, 0.5, 100), g_data=np.linspace(0.0, 0.5, 20),\n            error=0.01, plot=False)\n\n        Parameters:\n            g_true (linspace): The linspace desired for the true model to be calculated.\n\n            g_data (linspace): The linspace input for the data to be generated within. \n\n            data (numpy.ndarray): The data array entered by the user; if user wishes to \n                generate data, this remains set to None.\n\n            sigma (numpy.ndarray): The standard deviation array entered by the user; if \n                user wishes to generate data, this will remain set to None. \n\n            error (float): The error to put on the data set if the data set is not being \n                given by the user. Enter in decimal form (0.01 = 1%). Default is None. \n\n            plot (bool): The option to plot the data. Default is True. \n\n        Returns:\n            data (numpy.ndarray): The array of data (generated or entered by the user).\n\n            sigma (numpy.ndarray): The standard deviation at each data point (generated or \n                entered by the user).\n        '''\n\n        #if user has an array of data, skip data generation\n        if data is None:\n\n            if error is None:\n                raise ValueError('Please enter a error in decimal form for the data set generation.')\n            elif error &lt; 0.0 or error &gt; 1.0:\n                raise ValueError('Error must be between 0.0 and 1.0.')\n\n            #generate fake data  \n            data = self.true_model(g_data)\n            rand = np.random.RandomState()\n            var = error*rand.randn(len(g_data))\n            data = data*(1 + var)\n\n            #calculate standard deviation\n            sigma = error*data\n\n        #plot the data and true model\n        if plot is True:\n            self.plot_data(g_true, g_data, data)\n\n        return data, sigma\n\n\n    def plot_data(self, g_true, g_data, data):\n\n        r'''\n        The plotting function to display the generated data and true model. \n\n        Example:\n            LMM.plot_data(g_true=np.linspace(0.0, 0.5, 100), g_data=np.linspace(0.0, 0.5, 20), \n            data=np.array([]))\n\n        Parameters:\n            g_true (linspace): The linspace desired for the true model to be calculated.\n\n            g_data (linspace): The linspace over which the data was generated. \n\n            data (numpy.ndarray): The array of data generated using the LMM.add_data function.\n\n        Returns:\n            None.\n\n        '''\n\n        #set up the plot\n        fig = plt.figure(figsize=(8,6), dpi=600)\n        ax = plt.axes()\n        ax.tick_params(axis='x', labelsize=18)\n        ax.tick_params(axis='y', labelsize=18)\n        ax.locator_params(nbins=6)\n        ax.xaxis.set_minor_locator(AutoMinorLocator())\n        ax.yaxis.set_minor_locator(AutoMinorLocator())\n        ax.set_xlim(min(g_true), max(g_true))\n        ax.set_ylim(1.2,3.2)\n\n        ax.plot(g_data, data, 'k.', label='Data set')\n        ax.plot(g_true, self.m.true_model(g_true), 'k', label='True model')\n\n        ax.legend(fontsize=18)\n        plt.show()\n\n        #save figure option\n        # response = input('Would you like to save this figure? (yes/no)')\n\n        # if response == 'yes':\n        #     name = input('Enter a file name (include .jpg, .png, etc.)')\n        #     fig.savefig(name, bbox_inches='tight')\n\n        return None\n\n\n    def likelihood_low(self, g_data, data, sigma, siglow):\n\n        r'''\n        The likelihood function for the data using the small-g expansion as the model in the \n        chi-squared.\n\n        Example:\n            LMM.likelihood_low(g_data=np.linspace(0.0, 0.5, 20), data=np.array(), \n                sigma=np.array(), loworder=5)\n\n        Parameters:\n            g_data (linspace): A linspace used to generate data points. \n\n            data (numpy.ndarray): An array of data points generated or supplied by \n                the user.\n\n            sigma (numpy.ndarray): An array of standard deviations at each point in 'data'. \n\n        Returns:\n            An array of the likelihood calculated at each data point. \n        '''\n\n        #set up the uncertainties using experimental &amp; theory errors\n        sigma_t = np.sqrt(sigma**2.0 + siglow**2.0)\n\n        prelow = (np.sqrt(2.0 * np.pi) * sigma_t)**(-1.0)\n        insidelow = -0.5 * ((data - self.m.low_g(g_data))/(sigma_t))**2.0\n\n        return prelow*np.exp(insidelow)\n\n\n    def likelihood_high(self, g_data, data, sigma, sighigh):\n\n        r'''\n        The likelihood function for the data using the large-g expansion as the model in the \n        chi-squared.\n\n        Example:\n            LMM.likelihood_high(g_data=np.linspace(0.0, 0.5, 20), data=np.array(), \n                sigma=np.array(), highorder=23)\n\n        Parameters:\n            g_data (linspace): A linspace used to generate data points. \n\n            data (numpy.ndarray): An array of data points generated or supplied by \n                the user.\n\n            sigma (numpy.ndarray): An array of standard deviations at each point \n                in 'data'. \n\n        Returns:\n            An array of the likelihood calculated at each data point. \n        '''\n\n        #set up the uncertainties using experimental &amp; theory errors\n        sigma_t = np.sqrt(sigma**2.0 + sighigh**2.0)\n\n        prehigh = (np.sqrt(2.0 * np.pi) * sigma_t)**(-1.0)\n        insidehigh = -0.5 * ((data - self.m.high_g(g_data))/(sigma_t))**2.0\n\n        return prehigh*np.exp(insidehigh)\n\n\n    def sampler_mix(self, params, g_data, data, sigma, siglow, sighigh):\n\n        r'''\n        The model mixing function sent to the sampler to find the values of the parameters in the \n        selected mixing function. \n\n        Example:\n            emcee.EnsembleSampler(nwalkers, self.sampler_mix,\n                args=[g_data, data, sigma])\n\n        Parameters:\n            params (numpy.ndarray): The parameters that are being determined by the \n                sampler, in an array (not input).\n\n            g_data (linspace): The linspace used to generate the data.\n\n            data (numpy.ndarray): An array of data either generated or supplied by \n                the user. \n\n            sigma (numpy.ndarray): An array of standard deviations for each data point.\n\n        Returns:\n            mixed_results (numpy.ndarray): The results of the mixing function for the \n                entire linspace in g, in an array format.\n        '''\n\n        #set up arrays\n        mixed_likelihood = np.empty([len(g_data)])\n        log_ml = np.empty([len(g_data)])\n\n        #test prior first\n        logprior = self.p.lpdf(params)\n\n        if math.isnan(logprior) == True or np.isinf(-logprior) == True:\n            return -np.inf\n\n        else:\n\n            #likelihood mixing\n            for i in range(len(g_data)):\n                mixed_likelihood[i] = self.f(params, g_data[i]) * \\\n                                    LMM.likelihood_low(self, g_data[i], data[i], sigma[i], siglow[i]) \\\n                                    + (1.0- self.f(params, g_data[i])) * \\\n                                    LMM.likelihood_high(self, g_data[i], data[i], sigma[i], sighigh[i])\n\n                if mixed_likelihood[i] &lt;= 0.0:\n                    return -np.inf\n\n                log_ml[i] = np.log(mixed_likelihood[i])\n\n            total_lml = np.sum(log_ml)\n\n            #add the priors\n            mixed_results = total_lml + self.p.lpdf(params)\n\n            return mixed_results\n\n\n    def mixed_model(self, g_data, data, sigma, mixing_function='cosine', nsteps=1000):\n\n        r'''\n        A function that will run the emcee ensemble sampler for a given mixed model to determine at least one\n        unknown parameter in the mixing function selected. The function asks the user to decide which mixing\n        function to use, and runs the subsequent code to use the correct one. Functions sent to the sampler are\n        static methods defined at the end of this class.\n\n        Example:\n            LMM.mixed_model(g_data=np.linspace(0.0, 0.5, 20), data=np.array(), sigma=np.array(),\n                mixing_function='cosine', nsteps=3000)\n\n        Parameters:\n            g_data (linspace): The linspace over which the data was generated.\n\n            data (numpy.ndarray): An array of data points, either generated or supplied by \n                the user.\n\n            sigma (numpy.ndarray): An array of standard deviations at each data point.\n\n            mixing_function (str): The name of the mixing function to use for the LMM method. \n                Default is the piecewise cosine. \n\n            nsteps (int): The number of steps per walker for the sampler to use. \n\n        Returns:\n            sampler_mixed (object): The sampler results, contained in a sampler object, from \n                the determination of the unknown parameter. \n\n            emcee_trace_mixed (numpy.ndarray): The trace of each parameter, with burnin samples \n                extracted.\n        '''\n\n        #dictionary of LMM functions\n        self.function_mappings = {\n            'step': self.step,\n            'logistic': self.logistic,\n            'cdf': self.cdf,\n            'cosine': self.switchcos,\n        }\n\n        #ask user which mixing function to use\n        self.choice = mixing_function\n\n        #determine number of hyperparameters \n        if self.choice == 'step':\n            self.ndim = 1\n        elif self.choice == 'logistic' or self.choice == 'cdf':\n            self.ndim = 2 \n        elif self.choice == 'cosine':\n            self.ndim = 3\n        else:\n            raise ValueError('Mixing function requested is not found. Enter one of the valid options.')\n\n        #theory errors via error models\n        siglow = np.sqrt(self.u.variance_low(g_data, self.loworder[0]))\n        sighigh = np.sqrt(self.u.variance_high(g_data, self.highorder[0]))\n\n        #set up sampler\n        nwalkers = 2*int(3*self.ndim + 1)\n\n        #show total samples while running\n        total_samples = nwalkers * nsteps\n        print('Using {} walkers with {} steps each, for a total of {} samples.'.format(nwalkers, nsteps, total_samples))\n\n        #set starting points per parameter\n        starting_points = np.zeros((nwalkers, self.ndim))\n\n        #generalise for ndim=1,2,...!=3 and specify for 3\n        if self.ndim == 3:\n            starting_points[:,0] = np.random.uniform(0.12, 0.18, nwalkers)\n            starting_points[:,2] = np.random.uniform(0.19, 0.24, nwalkers)\n            starting_points[:,1] = np.random.uniform(0.25, 0.30, nwalkers)\n        else:\n            for i in range(self.ndim):\n                starting_points[:,i] = np.random.uniform(0.0, 1.0, nwalkers)\n\n        #set the mixing function\n        self.f = self._select_function(self.choice)\n\n        #call emcee\n        sampler_mixed = emcee.EnsembleSampler(nwalkers, self.ndim, self.sampler_mix, \\\n                                            args=[g_data, data, sigma, siglow, sighigh])\n        now = time.time()\n        sampler_mixed.run_mcmc(starting_points, nsteps)\n        stop = time.time()\n        print('Calculation finished!')\n\n        #time calculation\n        elapsed = int(stop - now)\n        if elapsed / 60 &lt; 1.0:\n            print(f\"Duration = {elapsed} sec.\")\n        elif elapsed / 60 &gt;= 1.0:\n            minutes = int(elapsed / 60)\n            seconds = int(elapsed - 60*minutes)\n            print(f\"Duration = {minutes} min, {seconds} sec.\")\n\n        #find the trace\n        emcee_trace_mixed = self.burnin_trace(sampler_mixed, nsteps)\n\n        return sampler_mixed, emcee_trace_mixed\n\n\n    def ppd(self, trace, param_values, g_data, g, data, ci, plot=True):\n\n        r'''\n        A function to calculate the posterior predictive distribution (PPD) \n        for any chosen mixing function defined in this class. \n\n        Example:\n            LMM.ppd(trace, param_values=np.array([]),g_data=np.linspace(1e-6,1.0,10), \n                g_ppd=np.linspace(0.0, 0.5, 100), ci=68)\n\n        Parameters:\n            trace (numpy.ndarray): The trace of each of the parameters from \n                the sampler.\n\n            param_values (numpy.ndarray): The mean, median, or MAP values of the \n                parameters. \n\n            g_data (numpy.linspace): The linspace in g from which the data set \n                was calculated.\n\n            g (numpy.linspace): The linspace over which the PPD result will be \n                calculated.\n\n            data (numpy.ndarray): The data set used to calculate the mixed model. \n\n            ci (int): The desired credibility interval. Can be either 68 or 95.\n\n            plot (bool): The option to plot the PPD result with the series expansions\n                and true model. Default is True. \n\n        Returns:\n            switch_med_results (numpy.ndarray): The array of median values from the \n                PPD at each point in g.\n\n            switch_g_intervals (numpy.ndarray): The array of credibility interval values \n                for the median results of the PPD.\n        '''\n\n        #convert list to array\n        trace = np.asarray(trace)\n\n        #check interval\n        if ci == 68:\n            ci = 0.68\n        elif ci == 95:\n            ci = 0.95\n\n        if self.choice != 'step':\n            result_array = np.empty([len(g), len(trace[0].T)])\n        elif self.choice == 'step':\n            result_array = np.empty([len(g), len(trace)])\n        gmax = max(g)\n\n        #determine which mixing function was used\n        if self.choice == 'step':\n            for i in range(len(g)):\n                for j in range(len(trace)):\n                    result_array[i,j] = self.step(trace[j], g[i]) * self.m.low_g(g[i]) \\\n                        + (1.0 - self.step(trace[j], g[i])) \\\n                            * self.m.high_g(g[i])\n\n        elif self.choice == 'logistic' or self.choice == 'cdf':\n\n            for i in range(len(g)):\n                for j in range(len(trace[0].T)):\n\n                    if (self.m.low_g(g[i]) - self.m.high_g(g[i]))\\\n                    &gt; 0.1 and g[i] &gt; (0.25*gmax):\n                        result_array[i,j] = self.m.high_g(g[i])\n\n                    elif (self.m.low_g(g[i]) - self.m.high_g(g[i])) &gt; 0.1:\n                        result_array[i,j] = self.m.low_g(g[i])\n\n                    else:\n                        params = np.array([trace[0, j], trace[1, j]])\n\n                        result_array[i,j] = self.f(params, g[i])*self.m.low_g(g[i]) \\\n                                        + (1.0 - self.f(params, g[i])) \\\n                                        *self.m.high_g(g[i])\n\n        elif self.choice == 'cosine':\n\n            params = np.array([np.mean(trace[0,:]), np.mean(trace[1,:]), np.mean(trace[2,:])])\n\n            for i in range(len(g)):\n                for j in range(len(trace[0].T)):\n\n                    params = np.array([trace[0, j], trace[1, j], trace[2, j]])\n\n                    result_array[i,j] = self.switchcos(params, g[i]) * self.m.low_g(g[i]) \\\n                                    + (1.0 - self.switchcos(params, g[i])) \\\n                                    * self.m.high_g(g[i])\n\n        #define the credibility intervals\n        switch_med_results = np.empty([len(g)])\n        switch_g_intervals = np.empty([len(g), 2])\n\n        for i in range(len(g)):\n            switch_med_results[i] = statistics.median(result_array[i,:])\n            switch_g_intervals[i, :] = self.hpd_interval(result_array[i,:], ci)  # this is what I need (put in M-R curves)\n\n        #plot the PPD results\n        if plot is True:\n            self.plot_ppd(param_values, g_data, g, data, switch_med_results, switch_g_intervals, percent=68)\n\n        return switch_med_results, switch_g_intervals\n\n\n    def plot_ppd(self, results, g_data, g_ppd, data, ppd_results, ppd_intervals, percent):\n\n        r'''\n        A plotting function that can be used to plot the posterior predictive distribution (PPD) results (mean and \n        credible interval) obtained from calling the functions above in the main code, as well as data generated, \n        the true model, and the small- and large-g expansions chosen for the mixed model calculation. \n\n        Example:\n            LMM.plot_ppd(g_data=np.linspace(0.0, 0.5, 20), g_true=np.linspace(0.0, 0.5, 100), \n                g_ppd=np.linspace(0.0, 0.5, 200), data=np.array(), ppd_results=np.array(), \n                ppd_intervals=np.array(), percent=68)\n\n        Parameters:\n            results (numpy.ndarray): The mean or the median of the estimated parameters \n                from the posterior draws. \n\n            g_data (linspace): The linspace used to generate the data.\n\n            g_ppd (linspace): The linspace chosen to calculate the PPD over. \n\n            data (numpy.ndarray): An array of data either generated or supplied by \n                the user.\n\n            ppd_results (numpy.ndarray): An array of the mean of the PPD at each point \n                in the g_ppd linspace.\n\n            ppd_intervals (numpy.ndarray): A 2D array of the credibility interval calculated \n                for the PPD (containing both bounds).\n\n            percent (int): The percent credibility interval calculated for the variable \n                ppd_intervals (used in the plot legend). \n\n        Returns:\n            None.\n        '''\n\n        fig = plt.figure(figsize=(8,6), dpi=600)\n        ax = plt.axes()\n        ax.tick_params(axis='x', labelsize=18)\n        ax.tick_params(axis='y', labelsize=18)\n        ax.locator_params(nbins=6)\n        ax.xaxis.set_minor_locator(AutoMinorLocator())\n        ax.yaxis.set_minor_locator(AutoMinorLocator())\n        ax.set_xlabel('g', fontsize=22)\n        ax.set_ylabel('F(g)', fontsize=22)\n\n        ax.set_xlim(0.0, 1.0)\n        ax.set_ylim(1.2, 3.2)\n        ax.set_yticks([1.2, 1.6, 2.0, 2.4, 2.8, 3.2])\n\n        #plot the data and true model\n        ax.plot(g_data, data, 'k.', label='Data set')  \n        ax.plot(g_ppd, self.m.true_model(g_ppd), 'k', label='True model')\n\n        #plot the expansions\n        ax.plot(g_ppd, self.m.low_g(g_ppd)[0,:], 'r--', label=r'$f_s$ ($N_s$ = {})'.format(self.loworder[0]))\n        ax.plot(g_ppd, self.m.high_g(g_ppd)[0,:], 'b--', label=r'$f_l$ ($N_l$ = {})'.format(self.highorder[0]))\n\n        #plot the PPD results from mixing\n        ax.plot(g_ppd, ppd_results, 'g', label='Mixed model')\n        ax.plot(g_ppd, ppd_intervals[:,0], 'g', linestyle='dotted', label=r'{}\\% CI (HPD)'.format(percent))\n        ax.plot(g_ppd, ppd_intervals[:,1], 'g', linestyle='dotted')\n        ax.fill_between(g_ppd, ppd_intervals[:,0], ppd_intervals[:,1], color='green', alpha=0.2)\n\n        #parameter results (vertical lines)\n        if self.choice == 'step':\n            ax.axvline(x=results, color='darkviolet', alpha=0.35, label=r'$\\theta_{1}$')\n        else:\n            ax.axvline(x=results[0], color='darkviolet', alpha=0.35, label=r\"$\\theta_{1}$, $\\theta_{2}$, $\\theta_{3}$\")\n            ax.axvline(x=results[1], color='darkviolet', alpha=0.35)\n\n        if len(results) == 3:\n            ax.axvline(x=results[2], color='darkviolet', alpha=0.35)\n\n        ax.legend(fontsize=18, loc='upper right')\n        plt.show()\n\n        # answer = input('Would you like to save the plot to a file (yes/no)?')\n\n        # if answer == 'yes':\n        #     name = input('Enter a file name to use (include file type as .png, .pdf, etc.).')\n        #     fig.savefig(name, bbox_inches='tight')\n\n        return None\n\n\n    def _select_function(self, x):\n\n        r'''\n        A function that selects the proper mixing function given the input from the user\n        in the function 'LMM.mixed_model'. \n\n        *Internal function only for use inside the LMM class.*\n\n        Example:\n            LMM._select_function(x=self.choice)\n\n        Parameters:\n            x (string): The string that is input by the user to select one of the \n                available mixing functions. \n\n        Returns:\n            self.function_mappings[x]: The correct function label for the chosen mixing \n                function, converted from string to object.\n        '''\n        while True:\n            try:\n                return self.function_mappings[x]\n            except KeyError:\n                print('Invalid function, try again.')\n\n\n    def _autocorrelation(self, chain, max_lag=100):\n\n        r'''\n        Borrowed from Christian Forssen's nuclear TALENT school work on \n        'Learning from Data: Bayesian Methods and Machine Learning' course\n        in June 2019 (see https://github.com/NuclearTalent/Bayes2019). \n\n        *Internal function, only for use inside the LMM class.*\n\n        Example:\n            LMM._autocorrelation(chain, max_lag=200)\n\n        Parameters:\n            chain (numpy.ndarray): The array of samples from the emcee chain object that \n                are returned by the sampler. \n\n            max_lag (int): The maximum lagtime for the autocorrelation length. \n\n        Returns:\n            acors (numpy.ndarray): The array of autocorrelation calculated.\n        '''\n\n        #determine the autocorrelation length\n        acors = np.empty(max_lag+1)\n        if max_lag &gt; len(chain)/5:\n            warnings.warn('max_lag is more than one fifth the chain length')\n\n        # Create a copy of the chain with average zero\n        chain1d = chain - np.average(chain)\n        for lag in range(max_lag+1):\n            unshifted = None\n            shifted = chain1d[lag:]\n            if 0 == lag:\n                unshifted = chain1d\n            else:\n                unshifted = chain1d[:-lag]\n            normalization = np.sqrt(np.dot(unshifted, unshifted))\n            normalization *= np.sqrt(np.dot(shifted, shifted))\n            acors[lag] = np.dot(unshifted, shifted) / normalization \n\n        return acors\n\n\n    def burnin_trace(self, sampler_object, nsteps):\n\n        r'''\n        A small function to take the burn-in samples off of the sampler chain from the \n        LMM.mixed_model function, and to send back the trace of the sampler chain to \n        LMM.mixed_model.\n\n        Example:\n            LMM.burnin_trace(sampler_object=sampler_mixed, nsteps=3000)\n\n        Parameters:\n            sampler_object (object): The chain sent back by the emcee sampler after \n                it finishes running through the samples and walkers.\n\n            nsteps (int): The number of steps per walker.\n\n        Returns:\n            emcee_trace_mixed (numpy.ndarray): The trace of the sampler chain with \n                the user's desired number of burn-in samples removed.\n        '''\n\n        nburnin = int((1/15) * nsteps)\n\n        #throw out the burn-in and reshape again\n        emcee_trace_mixed = sampler_object.chain[:, nburnin:, :].reshape(-1, self.ndim).T\n\n        return emcee_trace_mixed\n\n\n    def stats_chain(self, chain, plot=True):\n\n        r'''\n        Calculates the autocorrelation time and thins the samples\n        accordingly for a better estimate of the mean, median, and MAP values. \n\n        Example: \n            LMM.stats_chain(chain=emcee.object, plot=False)\n\n        Parameters:\n            chain (object): The object resulting from sampling the parameters\n                using emcee. The chain of samples must be extracted\n                from it. \n\n            plot (bool): The option to plot the traces of the sample\n                chains and the corner plot of the parameter\n                distributions. Default is True.  \n\n        Returns:\n            thin (numpy.ndarray): The array of thinned samples per parameter. \n                Used externally to calculate the MAP values.\n\n            median_results (numpy.ndarray): Each of the median parameter \n                values found from the sampling.\n\n            mean_results (numpy.ndarray): Each of the mean parameter values found \n                from the sampling. \n        '''\n\n        #retrieve the chain\n        chain_result = chain.chain[:,:,:]\n\n        #\"quick and dirty\" method; will finish later more generally\n        if self.ndim == 1:\n\n            #set up arrays\n            chain1 = chain_result[:,:,0]\n\n            #flatten each individual array\n            flat1 = chain1.flatten()\n\n            #call autocorrelation to find the lengths\n            post_acors1 = self._autocorrelation(flat1, max_lag=200)\n\n            #determine the autocorrelation time\n            post_rho1 = post_acors1[25:35]\n\n            post_y = np.arange(10)\n            post_x1 = -np.log(post_rho1)\n\n            #linear fits\n            p1, _ = np.polyfit(post_x1, post_y, 1, cov=True)\n\n            #thin the samples given the determined autocorrelation time\n            thin1 = []\n            time = p1[0]\n\n            time = int(time)\n\n            for i in range(len(flat1)):\n                if i % time == 0:\n                    thin1.append(flat1[i])\n\n            #array thinned samples\n            thin = np.array(thin1)\n\n            #call stats_trace for plots\n            if plot is True:\n                _, _ = LMM.stats_trace(self, thin)\n\n            #median calculation\n            median_1 = statistics.median(thin)\n\n            #mean calculation\n            mean_1 = np.mean(thin)\n\n            #arrays\n            mean_results = np.array([mean_1])\n            median_results = np.array([median_1])\n\n            return thin, mean_results, median_results\n\n        else:\n\n            #set up lists and arrays\n            chains = []\n            post_acors = []\n            post_rho = []\n            post_x = []\n            p = []\n\n            post_y = np.arange(10)\n\n            #create list using parameter arrays and flatten\n            for i in range(self.ndim):\n                chains.append(chain_result[:,:,i].flatten())\n\n            #max lag determination\n            max_lag = 200\n            if max_lag &gt; int(0.2*len(chains[0])):\n                max_lag = int(0.15*len(chains[0]))\n\n            #determine autocorrelation\n            for i in range(len(chains)):\n                post_acors.append(self._autocorrelation(chains[i], max_lag=max_lag))\n                post_rho.append(post_acors[i][25:35])\n                post_x.append(-np.log(post_rho[i]))\n                p_temp, _ = np.polyfit(post_x[i], post_y, 1, cov=True)\n                p.append(p_temp[0])\n\n        #thin the samples based on the results above\n        thin = []\n\n        #get the autocorrelation time for all parameters\n        for i in range(len(chains)-1):\n            if p[i] &gt; p[i+1]:\n                time = int(p[i])\n            else:\n                time = int(p[i+1])\n\n        #thin the samples for each parameter  ---&gt; double counting, need to separate params correctly\n        for i in range(self.ndim):\n            thin_temp = []\n            for j in range(len(chains[0])):\n                if j % time == 0:\n                    thin_temp.append(chains[i][j])\n\n            thin.append(thin_temp)\n\n        #plot the traces\n        if plot is True:\n                _, _ = LMM.stats_trace(self, thin)\n\n        #median, mean calculations\n        median = []\n        mean = []\n\n        for i in range(len(thin)):\n            median.append(statistics.median(thin[i]))\n            mean.append(np.mean(thin[i]))\n\n        return thin, mean, median\n\n\n    def MAP_values(self, thin, g, g_data, data, sigma, plot=True):\n\n        r'''\n        A function to calculate the MAP values of sampled distributions \n        of parameters. Will calculate for as many parameters as are present \n        and return results in an array. \n\n        Example:\n            LMM.MAP_values(thin=np.array([]), g_data=np.linspace(),\n                g=np.linspace(), data=np.array([]), sigma=np.array([]))\n\n        Parameters:\n            thin (numpy.ndarray): The array of thinned samples from the \n                stats_chain() function.\n\n            g (numpy.linspace): The input space over which the mixing \n                is calculated.\n\n            g_data (numpy.linspace): The array of input points in g for \n                the data set. \n\n            data (numpy.ndarray): The data set being used for the mixing \n                calculation.\n\n            sigma (numpy.ndarray): The data error set being used for the \n                mixing calculation.\n\n            plot (bool): The option to plot the weights over the input space \n                in g. Default is True. \n\n        Returns:\n            map_values (numpy.ndarray): The MAP values of each parameter. \n        '''\n\n        #calculate theory error needed \n        siglow = np.sqrt(self.u.variance_low(g_data, self.loworder[0]))\n        sighigh = np.sqrt(self.u.variance_high(g_data, self.highorder[0]))\n\n        #find the posterior using the parameters\n        thetas = np.asarray(thin)\n\n        if self.choice == 'step':\n            posterior = np.zeros(len(thetas))\n            for i in range(len(thetas)):\n                posterior[i] = self.sampler_mix(thetas[i], g_data, data, sigma, siglow, sighigh)\n\n            #MAP value for parameter\n            theta_index = np.argmax(posterior)\n            map_values = np.array([thetas[theta_index]])\n\n        else:\n            posterior = np.zeros([len(thetas[0,:])])\n            for i in range(len(thetas[0,:])):\n                posterior[i] = self.sampler_mix(thetas[:,i], g_data, data, sigma, siglow, sighigh)\n\n            #MAP value calculation\n            theta_index = np.argmax(posterior)\n            map_values = thetas[:, theta_index]\n\n        #plot the weight function and MAP values\n        if plot is True:\n            self.plot_MAP(g, map_values)\n\n        return map_values\n\n\n    def plot_MAP(self, g, map_values):\n\n        r'''\n        A simple rough plotter to plot the weight/mixing function\n        for the LMM method using the mixing function calculated at\n        the points in g and the MAP values of its parameters. \n\n        Example:\n            LMM.plot_MAP(g=np.linspace(), map_values=numpy.ndarray([]))\n\n        Parameters:\n            g (numpy.linspace): The input space over which the mixing \n                is calculated.\n\n            map_values (numpy.ndarray): The results of the MAP_values() \n                function (MAP values of each parameter in the mixing function \n                selected).\n\n        Returns:\n            None.\n        '''\n\n        #set up figure\n        fig = plt.figure(figsize=(8,6), dpi=600)\n        ax = plt.axes()\n        ax.tick_params(axis='x', labelsize=18)\n        ax.tick_params(axis='y', labelsize=18)\n        ax.set_xlim(0.0, 1.0)\n        ax.set_ylim(0.0,1.0)\n        ax.set_xlabel('g', fontsize=22)\n        ax.set_ylabel(r'$\\alpha$(g; $\\theta$)', fontsize=22)\n\n        #solve the mixing function first and plot\n        switch = np.zeros([len(g)])\n        for i in range(len(g)):\n            switch[i] = self.f(map_values, g[i])\n        ax.plot(g, switch, 'k', linewidth=3, label=r'$\\alpha(g; \\theta)$') \n\n        #plot the MAP value lines\n        if len(map_values) == 1:\n            ax.axvline(x=map_values, color='darkorange', linestyle='dashed', label=r'$\\theta_{1}$')\n\n        elif len(map_values) == 2:\n            ax.axvline(x=map_values[0], color='darkorange', linestyle='dashed', label=r'$\\theta_{1}$')\n            ax.axvline(x=map_values[1], color='darkviolet', linestyle='dashdot', label=r'$\\theta_{2}$')\n\n        elif len(map_values) == 3:\n            ax.axvline(x=map_values[0], color='darkorange', linestyle='dashed', label=r'$\\theta_{1}$')\n            ax.axvline(x=map_values[2], color='darkviolet', linestyle='dashdot', label=r'$\\theta_{2}$')\n            ax.axvline(x=map_values[1], color='darkgreen', linestyle='dashed', label=r'$\\theta_{3}$')\n\n        ax.legend(loc='upper right', fontsize=18)\n\n        #fig.savefig('mixing_plot_MAP_5v5.pdf', bbox_inches='tight')\n\n        plt.show()\n\n        return None \n\n\n    def stats_trace(self, trace):\n\n        r'''\n        A function to calculate the mean and credible intervals corresponding to\n        each parameter. The trace plots for each parameter are plotted. \n\n        Example:\n            LMM.stats_trace(trace=np.array([]))\n\n        Parameters:\n            trace (numpy.ndarray): The trace from the sampler object that was generated \n                when estimating the parameters of the mixing function.\n\n        Returns:\n            mean (numpy.ndarray): The array of mean values for each parameter.\n\n            ci (numpy.ndarray): The array of sets of credible interval bounds for \n                each parameter. \n        '''\n\n        #calculate mean and credible intervals\n        mean = []\n        ci = []\n\n        if self.ndim == 1:\n            mean.append(np.mean(trace))\n            ci.append(self.hpd_interval(trace, 0.95))\n        else:\n            for i in range(self.ndim):\n                mean.append(np.mean(trace[i]))\n                ci.append(self.hpd_interval(trace[i], 0.95))\n\n        mean = np.asarray(mean)\n        ci = np.asarray(ci)\n\n        #plot traces with mean and credible intervals\n        fig, ax = plt.subplots(self.ndim,1,figsize=(7,4*self.ndim), dpi=600)\n\n        if self.ndim == 1:\n            ax.plot(trace, 'k')\n            ax.set_ylabel('Parameter 1', fontsize=22)\n            ax.set_title('Trace plot: Parameter 1', fontsize=22)\n\n            ax.axhline(y=mean, color='b', linestyle='solid', label='Mean')\n            ax.axhline(y=ci[0, 0], color='b', linestyle='dashed')\n            ax.axhline(y=ci[0, 1], color='b', linestyle='dashed')\n\n            #plot the median over the mean\n            med = []\n            med.append(np.median(trace[0]))\n            med = np.asarray(med)\n\n            ax.axhline(y=med, color='r', linestyle='solid', label='Median')\n            ax.legend(loc='upper right')\n\n            plt.show()\n\n\n        if self.ndim &gt; 1:\n            for irow in range(self.ndim):\n                ax[irow].plot(trace[irow], 'k')\n                ax[irow].set_ylabel('Parameter {0}'.format(irow+1), fontsize=22)\n                ax[irow].set_title('Trace plot: Parameter {0}'.format(irow+1), fontsize=22)\n\n                ax[irow].axhline(y=mean[irow], color='b', linestyle='solid', label='Mean')\n                ax[irow].axhline(y=ci[irow, 0], color='b', linestyle='dashed')\n                ax[irow].axhline(y=ci[irow, 1], color='b', linestyle='dashed')\n\n            #plot the median over the mean\n            med = []\n\n            for i in range(self.ndim):\n                med.append(np.median(trace[i]))\n\n            med = np.asarray(med)\n\n            for irow in range(self.ndim):\n                ax[irow].axhline(y=med[irow], color='r', linestyle='solid', label='Median')\n\n                ax[irow].legend(loc='upper right')\n\n            plt.show()\n\n            #corner plots for hyperparameter posteriors\n            fig, axs = plt.subplots(self.ndim,self.ndim, figsize=(8,8), dpi=600)\n            label = [\"Parameter 1\", \"Parameter 2\", \"Parameter 3\"]\n            trace = np.asarray(trace)\n            corner.corner(trace.T,labels=label, \\\n                quantiles=[0.16, 0.5, 0.84],fig=fig,show_titles=True, label_kwargs=dict(fontsize=16))\n            plt.show()\n\n        return mean, ci \n\n\n    @staticmethod\n    def hpd_interval(trace, fraction):\n\n        r'''\n        A function to calculate the Bayesian credible intervals of a posterior distribution. This function\n        uses the HPD (highest posterior density) method.\n\n        Example:\n            LMM.hpd_interval(trace=emcee_trace, fraction=0.95)\n\n        Parameters:\n            trace (numpy.ndarray): The trace generated by a sampler when sampling a variable to obtain \n                its posterior distribution.\n\n            fraction (float): The percent (in decimal form) requested by the user to set the credibility \n                interval. \n\n        Returns:\n            interval (numpy.ndarray): The credibility interval bounds in a numpy array (format: [min, max]).\n        '''\n\n        sort_list = np.sort(np.copy(trace))\n        total_samples = len(trace)\n\n        int_samples = np.floor(fraction * total_samples).astype(int)\n        int_width = sort_list[int_samples:] - sort_list[:total_samples-int_samples]  # cutting the tails off\n\n        min_int = np.argmin(int_width)  # tells me the location\n\n        interval = np.array([sort_list[min_int], sort_list[min_int+int_samples]])  # gives me the interval\n\n        return interval\n\n\n    @staticmethod\n    def logistic(params, g):\n\n        r'''\n        A basic logistic function often used in machine learning, implemented here with two free\n        parameters to be determined via sampling.\n\n        Example:\n            logistic(params=np.array(), g=0.5)\n\n        Parameters:\n            params (numpy.ndarray): The array of parameters the sampler will determine (here \n                labelled beta0 and beta1, where beta0 controls the location of the function and \n                beta1 controls the slope). \n\n        Returns:\n            mixing (float): The result of the logistic function given the value g.\n        '''\n        beta0, beta1 = params\n\n        mixing = (1.0 + np.exp(-(beta0 + g*beta1)))**(-1.0)\n\n        return mixing\n\n    @staticmethod\n    def cdf(params, g):\n\n        r'''\n        The cumulative distribution function of a standard normal distribution, with two free \n        parameters determined by sampling.\n\n        Example:\n            cdf(params=np.array(), g=0.5)\n\n        Parameters:\n            params (numpy.ndarray): The array of parameters the sampler will determine (here \n                labelled beta0 and beta1, where beta0 controls the location of the function and \n                beta1 controls the slope). \n\n            g : float\n                The value of g the cdf is calculated at.\n\n        Returns:\n            function (float): The result of the cdf function at the value of g. \n        '''\n        beta0, beta1 = params\n\n        function = (1.0 + math.erf((beta0 + g*beta1)/np.sqrt(2.0))) / 2.0\n\n        return function\n\n\n    @staticmethod\n    def switchcos(params, g):\n\n        r'''\n        A piecewise function using two constants at either end, and two cosine functions in the centre,\n        to be used as a mixing function. One free parameter, g3, is found by sampling. \n\n        Example:\n            switchcos(params=np.array(), g=0.5)\n\n        Parameters:\n            params (numpy.ndarray): The array of parameters to be determined by the sampler \n                (here labelled g1, g2, and g3, where g1 is the separation point between the first constant \n                function and the first cosine function, g2 is the separation point between the second \n                cosine function and the second constant function, \n                and g3 is the point between the two cosine functions). \n\n            g (float): The value of g that this cosine function is calculated at.\n\n        Returns:\n            The value of the function at a specific point in g. \n        '''\n\n        #unpack the parameters\n        g1, g2, g3 = params\n\n        if g1 &gt; g2 or g2 &lt; g3 or g1 &gt; g3:\n            return -np.inf\n\n        if g &lt;= g1:\n            return 1.0\n\n        elif g &lt;= g3:\n            return (1.0 + np.cos((np.pi/2.0) * ((g - g1)/(g3 - g1))))/2.0\n\n        elif g &lt; g2:\n            return 0.5 + np.cos((np.pi/2.0) * (1.0 + ((g - g3)/(g2 - g3))))/2.0\n\n        else:\n            return 0.0\n\n    @staticmethod\n    def step(params, g):\n\n        r'''\n        A step mixing function to switch between two models. \n        ***Only useful for two models.***\n\n        Example:\n            step(params, g=0.2)\n\n        Parameters:\n            params (np.ndarray): One single parameter to determine \n                where the step function will break from one model to the other.\n\n            g (float): One value of the input space. \n\n        Returns:\n            The value of the step function at a specific \n            point in g. \n        '''\n\n        #enable step function\n        if g &lt; params:\n            return 1.0 \n        else:\n            return 0.0 \n</code></pre>"},{"location":"mixing/#samba.mixing.LMM.MAP_values","title":"<code>MAP_values(thin, g, g_data, data, sigma, plot=True)</code>","text":"<p>A function to calculate the MAP values of sampled distributions  of parameters. Will calculate for as many parameters as are present  and return results in an array. </p> Example <p>LMM.MAP_values(thin=np.array([]), g_data=np.linspace(),     g=np.linspace(), data=np.array([]), sigma=np.array([]))</p> <p>Parameters:</p> Name Type Description Default <code>thin</code> <code>ndarray</code> <p>The array of thinned samples from the  stats_chain() function.</p> required <code>g</code> <code>linspace</code> <p>The input space over which the mixing  is calculated.</p> required <code>g_data</code> <code>linspace</code> <p>The array of input points in g for  the data set. </p> required <code>data</code> <code>ndarray</code> <p>The data set being used for the mixing  calculation.</p> required <code>sigma</code> <code>ndarray</code> <p>The data error set being used for the  mixing calculation.</p> required <code>plot</code> <code>bool</code> <p>The option to plot the weights over the input space  in g. Default is True. </p> <code>True</code> <p>Returns:</p> Name Type Description <code>map_values</code> <code>ndarray</code> <p>The MAP values of each parameter.</p> Source code in <code>samba/mixing.py</code> <pre><code>def MAP_values(self, thin, g, g_data, data, sigma, plot=True):\n\n    r'''\n    A function to calculate the MAP values of sampled distributions \n    of parameters. Will calculate for as many parameters as are present \n    and return results in an array. \n\n    Example:\n        LMM.MAP_values(thin=np.array([]), g_data=np.linspace(),\n            g=np.linspace(), data=np.array([]), sigma=np.array([]))\n\n    Parameters:\n        thin (numpy.ndarray): The array of thinned samples from the \n            stats_chain() function.\n\n        g (numpy.linspace): The input space over which the mixing \n            is calculated.\n\n        g_data (numpy.linspace): The array of input points in g for \n            the data set. \n\n        data (numpy.ndarray): The data set being used for the mixing \n            calculation.\n\n        sigma (numpy.ndarray): The data error set being used for the \n            mixing calculation.\n\n        plot (bool): The option to plot the weights over the input space \n            in g. Default is True. \n\n    Returns:\n        map_values (numpy.ndarray): The MAP values of each parameter. \n    '''\n\n    #calculate theory error needed \n    siglow = np.sqrt(self.u.variance_low(g_data, self.loworder[0]))\n    sighigh = np.sqrt(self.u.variance_high(g_data, self.highorder[0]))\n\n    #find the posterior using the parameters\n    thetas = np.asarray(thin)\n\n    if self.choice == 'step':\n        posterior = np.zeros(len(thetas))\n        for i in range(len(thetas)):\n            posterior[i] = self.sampler_mix(thetas[i], g_data, data, sigma, siglow, sighigh)\n\n        #MAP value for parameter\n        theta_index = np.argmax(posterior)\n        map_values = np.array([thetas[theta_index]])\n\n    else:\n        posterior = np.zeros([len(thetas[0,:])])\n        for i in range(len(thetas[0,:])):\n            posterior[i] = self.sampler_mix(thetas[:,i], g_data, data, sigma, siglow, sighigh)\n\n        #MAP value calculation\n        theta_index = np.argmax(posterior)\n        map_values = thetas[:, theta_index]\n\n    #plot the weight function and MAP values\n    if plot is True:\n        self.plot_MAP(g, map_values)\n\n    return map_values\n</code></pre>"},{"location":"mixing/#samba.mixing.LMM.__init__","title":"<code>__init__(loworder, highorder, error_model='informative')</code>","text":"<p>This class is designed with all of the necessary functions for creating  a data set, plotting it along with the true model, and calculating expansions  of specific orders of the true model to mix. Dependent on the Models class to  run the expansion functions. </p> Example <p>LMM(loworder=np.array([2]), highorder=np.array([2]), error_model='informative')</p> <p>Parameters:</p> Name Type Description Default <code>loworder</code> <code>(ndarray, int)</code> <p>The truncation order to which we calculate  the small-g expansion. </p> required <code>highorder</code> <code>(ndarray, int)</code> <p>The truncation order to which we calculate the  large-g expansion. </p> required <code>error_model</code> <code>str</code> <p>The error model chosen for this calculation. Can be either  'uninformative' or 'informative'. Default is 'informative'. </p> <code>'informative'</code> <p>Returns:</p> Type Description <p>None.</p> Source code in <code>samba/mixing.py</code> <pre><code>def __init__(self, loworder, highorder, error_model='informative'):\n\n    r'''\n    This class is designed with all of the necessary functions for creating \n    a data set, plotting it along with the true model, and calculating expansions \n    of specific orders of the true model to mix. Dependent on the Models class to \n    run the expansion functions. \n\n    Example:            \n        LMM(loworder=np.array([2]), highorder=np.array([2]), error_model='informative')\n\n    Parameters:\n        loworder (numpy.ndarray, int): The truncation order to which we calculate \n            the small-g expansion. \n\n        highorder (numpy.ndarray, int): The truncation order to which we calculate the \n            large-g expansion. \n\n        error_model (str): The error model chosen for this calculation. Can be either \n            'uninformative' or 'informative'. Default is 'informative'. \n\n    Returns:\n        None.\n    '''    \n\n    #check type and create class variables\n    if isinstance(loworder, float) == True or isinstance(loworder, int) == True:\n        loworder = np.array([loworder])\n\n    if isinstance(highorder, float) == True or isinstance(highorder, int) == True:\n        highorder = np.array([highorder])\n\n    self.loworder = loworder \n    self.highorder = highorder\n\n    #instantiate the Models() and Priors() classes here\n    self.m = Models(self.loworder, self.highorder)\n    self.u = Uncertainties(error_model)\n    self.p = Priors()\n\n    return None\n</code></pre>"},{"location":"mixing/#samba.mixing.LMM.add_data","title":"<code>add_data(g_true, g_data, data=None, sigma=None, error=None, plot=True)</code>","text":"<p>A data generation function that generates data based on the g_data linspace  provided (with the number of points chosen by the user) and the error desired on  each point (also input by the user), or accepts the user's input of an array of data  and standard deviations of the data points. </p> Example <p>LMM.add_data(g_true=np.linspace(0.0, 0.5, 100), g_data=np.linspace(0.0, 0.5, 20), error=0.01, plot=False)</p> <p>Parameters:</p> Name Type Description Default <code>g_true</code> <code>linspace</code> <p>The linspace desired for the true model to be calculated.</p> required <code>g_data</code> <code>linspace</code> <p>The linspace input for the data to be generated within. </p> required <code>data</code> <code>ndarray</code> <p>The data array entered by the user; if user wishes to  generate data, this remains set to None.</p> <code>None</code> <code>sigma</code> <code>ndarray</code> <p>The standard deviation array entered by the user; if  user wishes to generate data, this will remain set to None. </p> <code>None</code> <code>error</code> <code>float</code> <p>The error to put on the data set if the data set is not being  given by the user. Enter in decimal form (0.01 = 1%). Default is None. </p> <code>None</code> <code>plot</code> <code>bool</code> <p>The option to plot the data. Default is True. </p> <code>True</code> <p>Returns:</p> Name Type Description <code>data</code> <code>ndarray</code> <p>The array of data (generated or entered by the user).</p> <code>sigma</code> <code>ndarray</code> <p>The standard deviation at each data point (generated or  entered by the user).</p> Source code in <code>samba/mixing.py</code> <pre><code>def add_data(self, g_true, g_data, data=None, sigma=None, error=None, plot=True):\n\n    r'''\n    A data generation function that generates data based on the g_data linspace \n    provided (with the number of points chosen by the user) and the error desired on \n    each point (also input by the user), or accepts the user's input of an array of data \n    and standard deviations of the data points. \n\n    Example:\n        LMM.add_data(g_true=np.linspace(0.0, 0.5, 100), g_data=np.linspace(0.0, 0.5, 20),\n        error=0.01, plot=False)\n\n    Parameters:\n        g_true (linspace): The linspace desired for the true model to be calculated.\n\n        g_data (linspace): The linspace input for the data to be generated within. \n\n        data (numpy.ndarray): The data array entered by the user; if user wishes to \n            generate data, this remains set to None.\n\n        sigma (numpy.ndarray): The standard deviation array entered by the user; if \n            user wishes to generate data, this will remain set to None. \n\n        error (float): The error to put on the data set if the data set is not being \n            given by the user. Enter in decimal form (0.01 = 1%). Default is None. \n\n        plot (bool): The option to plot the data. Default is True. \n\n    Returns:\n        data (numpy.ndarray): The array of data (generated or entered by the user).\n\n        sigma (numpy.ndarray): The standard deviation at each data point (generated or \n            entered by the user).\n    '''\n\n    #if user has an array of data, skip data generation\n    if data is None:\n\n        if error is None:\n            raise ValueError('Please enter a error in decimal form for the data set generation.')\n        elif error &lt; 0.0 or error &gt; 1.0:\n            raise ValueError('Error must be between 0.0 and 1.0.')\n\n        #generate fake data  \n        data = self.true_model(g_data)\n        rand = np.random.RandomState()\n        var = error*rand.randn(len(g_data))\n        data = data*(1 + var)\n\n        #calculate standard deviation\n        sigma = error*data\n\n    #plot the data and true model\n    if plot is True:\n        self.plot_data(g_true, g_data, data)\n\n    return data, sigma\n</code></pre>"},{"location":"mixing/#samba.mixing.LMM.burnin_trace","title":"<code>burnin_trace(sampler_object, nsteps)</code>","text":"<p>A small function to take the burn-in samples off of the sampler chain from the  LMM.mixed_model function, and to send back the trace of the sampler chain to  LMM.mixed_model.</p> Example <p>LMM.burnin_trace(sampler_object=sampler_mixed, nsteps=3000)</p> <p>Parameters:</p> Name Type Description Default <code>sampler_object</code> <code>object</code> <p>The chain sent back by the emcee sampler after  it finishes running through the samples and walkers.</p> required <code>nsteps</code> <code>int</code> <p>The number of steps per walker.</p> required <p>Returns:</p> Name Type Description <code>emcee_trace_mixed</code> <code>ndarray</code> <p>The trace of the sampler chain with  the user's desired number of burn-in samples removed.</p> Source code in <code>samba/mixing.py</code> <pre><code>def burnin_trace(self, sampler_object, nsteps):\n\n    r'''\n    A small function to take the burn-in samples off of the sampler chain from the \n    LMM.mixed_model function, and to send back the trace of the sampler chain to \n    LMM.mixed_model.\n\n    Example:\n        LMM.burnin_trace(sampler_object=sampler_mixed, nsteps=3000)\n\n    Parameters:\n        sampler_object (object): The chain sent back by the emcee sampler after \n            it finishes running through the samples and walkers.\n\n        nsteps (int): The number of steps per walker.\n\n    Returns:\n        emcee_trace_mixed (numpy.ndarray): The trace of the sampler chain with \n            the user's desired number of burn-in samples removed.\n    '''\n\n    nburnin = int((1/15) * nsteps)\n\n    #throw out the burn-in and reshape again\n    emcee_trace_mixed = sampler_object.chain[:, nburnin:, :].reshape(-1, self.ndim).T\n\n    return emcee_trace_mixed\n</code></pre>"},{"location":"mixing/#samba.mixing.LMM.cdf","title":"<code>cdf(params, g)</code>  <code>staticmethod</code>","text":"<p>The cumulative distribution function of a standard normal distribution, with two free  parameters determined by sampling.</p> Example <p>cdf(params=np.array(), g=0.5)</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>ndarray</code> <p>The array of parameters the sampler will determine (here  labelled beta0 and beta1, where beta0 controls the location of the function and  beta1 controls the slope). </p> required <code>g</code> <p>float The value of g the cdf is calculated at.</p> required <p>Returns:</p> Name Type Description <code>function</code> <code>float</code> <p>The result of the cdf function at the value of g.</p> Source code in <code>samba/mixing.py</code> <pre><code>@staticmethod\ndef cdf(params, g):\n\n    r'''\n    The cumulative distribution function of a standard normal distribution, with two free \n    parameters determined by sampling.\n\n    Example:\n        cdf(params=np.array(), g=0.5)\n\n    Parameters:\n        params (numpy.ndarray): The array of parameters the sampler will determine (here \n            labelled beta0 and beta1, where beta0 controls the location of the function and \n            beta1 controls the slope). \n\n        g : float\n            The value of g the cdf is calculated at.\n\n    Returns:\n        function (float): The result of the cdf function at the value of g. \n    '''\n    beta0, beta1 = params\n\n    function = (1.0 + math.erf((beta0 + g*beta1)/np.sqrt(2.0))) / 2.0\n\n    return function\n</code></pre>"},{"location":"mixing/#samba.mixing.LMM.hpd_interval","title":"<code>hpd_interval(trace, fraction)</code>  <code>staticmethod</code>","text":"<p>A function to calculate the Bayesian credible intervals of a posterior distribution. This function uses the HPD (highest posterior density) method.</p> Example <p>LMM.hpd_interval(trace=emcee_trace, fraction=0.95)</p> <p>Parameters:</p> Name Type Description Default <code>trace</code> <code>ndarray</code> <p>The trace generated by a sampler when sampling a variable to obtain  its posterior distribution.</p> required <code>fraction</code> <code>float</code> <p>The percent (in decimal form) requested by the user to set the credibility  interval. </p> required <p>Returns:</p> Name Type Description <code>interval</code> <code>ndarray</code> <p>The credibility interval bounds in a numpy array (format: [min, max]).</p> Source code in <code>samba/mixing.py</code> <pre><code>@staticmethod\ndef hpd_interval(trace, fraction):\n\n    r'''\n    A function to calculate the Bayesian credible intervals of a posterior distribution. This function\n    uses the HPD (highest posterior density) method.\n\n    Example:\n        LMM.hpd_interval(trace=emcee_trace, fraction=0.95)\n\n    Parameters:\n        trace (numpy.ndarray): The trace generated by a sampler when sampling a variable to obtain \n            its posterior distribution.\n\n        fraction (float): The percent (in decimal form) requested by the user to set the credibility \n            interval. \n\n    Returns:\n        interval (numpy.ndarray): The credibility interval bounds in a numpy array (format: [min, max]).\n    '''\n\n    sort_list = np.sort(np.copy(trace))\n    total_samples = len(trace)\n\n    int_samples = np.floor(fraction * total_samples).astype(int)\n    int_width = sort_list[int_samples:] - sort_list[:total_samples-int_samples]  # cutting the tails off\n\n    min_int = np.argmin(int_width)  # tells me the location\n\n    interval = np.array([sort_list[min_int], sort_list[min_int+int_samples]])  # gives me the interval\n\n    return interval\n</code></pre>"},{"location":"mixing/#samba.mixing.LMM.likelihood_high","title":"<code>likelihood_high(g_data, data, sigma, sighigh)</code>","text":"<p>The likelihood function for the data using the large-g expansion as the model in the  chi-squared.</p> Example <p>LMM.likelihood_high(g_data=np.linspace(0.0, 0.5, 20), data=np.array(),      sigma=np.array(), highorder=23)</p> <p>Parameters:</p> Name Type Description Default <code>g_data</code> <code>linspace</code> <p>A linspace used to generate data points. </p> required <code>data</code> <code>ndarray</code> <p>An array of data points generated or supplied by  the user.</p> required <code>sigma</code> <code>ndarray</code> <p>An array of standard deviations at each point  in 'data'. </p> required <p>Returns:</p> Type Description <p>An array of the likelihood calculated at each data point.</p> Source code in <code>samba/mixing.py</code> <pre><code>def likelihood_high(self, g_data, data, sigma, sighigh):\n\n    r'''\n    The likelihood function for the data using the large-g expansion as the model in the \n    chi-squared.\n\n    Example:\n        LMM.likelihood_high(g_data=np.linspace(0.0, 0.5, 20), data=np.array(), \n            sigma=np.array(), highorder=23)\n\n    Parameters:\n        g_data (linspace): A linspace used to generate data points. \n\n        data (numpy.ndarray): An array of data points generated or supplied by \n            the user.\n\n        sigma (numpy.ndarray): An array of standard deviations at each point \n            in 'data'. \n\n    Returns:\n        An array of the likelihood calculated at each data point. \n    '''\n\n    #set up the uncertainties using experimental &amp; theory errors\n    sigma_t = np.sqrt(sigma**2.0 + sighigh**2.0)\n\n    prehigh = (np.sqrt(2.0 * np.pi) * sigma_t)**(-1.0)\n    insidehigh = -0.5 * ((data - self.m.high_g(g_data))/(sigma_t))**2.0\n\n    return prehigh*np.exp(insidehigh)\n</code></pre>"},{"location":"mixing/#samba.mixing.LMM.likelihood_low","title":"<code>likelihood_low(g_data, data, sigma, siglow)</code>","text":"<p>The likelihood function for the data using the small-g expansion as the model in the  chi-squared.</p> Example <p>LMM.likelihood_low(g_data=np.linspace(0.0, 0.5, 20), data=np.array(),      sigma=np.array(), loworder=5)</p> <p>Parameters:</p> Name Type Description Default <code>g_data</code> <code>linspace</code> <p>A linspace used to generate data points. </p> required <code>data</code> <code>ndarray</code> <p>An array of data points generated or supplied by  the user.</p> required <code>sigma</code> <code>ndarray</code> <p>An array of standard deviations at each point in 'data'. </p> required <p>Returns:</p> Type Description <p>An array of the likelihood calculated at each data point.</p> Source code in <code>samba/mixing.py</code> <pre><code>def likelihood_low(self, g_data, data, sigma, siglow):\n\n    r'''\n    The likelihood function for the data using the small-g expansion as the model in the \n    chi-squared.\n\n    Example:\n        LMM.likelihood_low(g_data=np.linspace(0.0, 0.5, 20), data=np.array(), \n            sigma=np.array(), loworder=5)\n\n    Parameters:\n        g_data (linspace): A linspace used to generate data points. \n\n        data (numpy.ndarray): An array of data points generated or supplied by \n            the user.\n\n        sigma (numpy.ndarray): An array of standard deviations at each point in 'data'. \n\n    Returns:\n        An array of the likelihood calculated at each data point. \n    '''\n\n    #set up the uncertainties using experimental &amp; theory errors\n    sigma_t = np.sqrt(sigma**2.0 + siglow**2.0)\n\n    prelow = (np.sqrt(2.0 * np.pi) * sigma_t)**(-1.0)\n    insidelow = -0.5 * ((data - self.m.low_g(g_data))/(sigma_t))**2.0\n\n    return prelow*np.exp(insidelow)\n</code></pre>"},{"location":"mixing/#samba.mixing.LMM.logistic","title":"<code>logistic(params, g)</code>  <code>staticmethod</code>","text":"<p>A basic logistic function often used in machine learning, implemented here with two free parameters to be determined via sampling.</p> Example <p>logistic(params=np.array(), g=0.5)</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>ndarray</code> <p>The array of parameters the sampler will determine (here  labelled beta0 and beta1, where beta0 controls the location of the function and  beta1 controls the slope). </p> required <p>Returns:</p> Name Type Description <code>mixing</code> <code>float</code> <p>The result of the logistic function given the value g.</p> Source code in <code>samba/mixing.py</code> <pre><code>@staticmethod\ndef logistic(params, g):\n\n    r'''\n    A basic logistic function often used in machine learning, implemented here with two free\n    parameters to be determined via sampling.\n\n    Example:\n        logistic(params=np.array(), g=0.5)\n\n    Parameters:\n        params (numpy.ndarray): The array of parameters the sampler will determine (here \n            labelled beta0 and beta1, where beta0 controls the location of the function and \n            beta1 controls the slope). \n\n    Returns:\n        mixing (float): The result of the logistic function given the value g.\n    '''\n    beta0, beta1 = params\n\n    mixing = (1.0 + np.exp(-(beta0 + g*beta1)))**(-1.0)\n\n    return mixing\n</code></pre>"},{"location":"mixing/#samba.mixing.LMM.mixed_model","title":"<code>mixed_model(g_data, data, sigma, mixing_function='cosine', nsteps=1000)</code>","text":"<p>A function that will run the emcee ensemble sampler for a given mixed model to determine at least one unknown parameter in the mixing function selected. The function asks the user to decide which mixing function to use, and runs the subsequent code to use the correct one. Functions sent to the sampler are static methods defined at the end of this class.</p> Example <p>LMM.mixed_model(g_data=np.linspace(0.0, 0.5, 20), data=np.array(), sigma=np.array(),     mixing_function='cosine', nsteps=3000)</p> <p>Parameters:</p> Name Type Description Default <code>g_data</code> <code>linspace</code> <p>The linspace over which the data was generated.</p> required <code>data</code> <code>ndarray</code> <p>An array of data points, either generated or supplied by  the user.</p> required <code>sigma</code> <code>ndarray</code> <p>An array of standard deviations at each data point.</p> required <code>mixing_function</code> <code>str</code> <p>The name of the mixing function to use for the LMM method.  Default is the piecewise cosine. </p> <code>'cosine'</code> <code>nsteps</code> <code>int</code> <p>The number of steps per walker for the sampler to use. </p> <code>1000</code> <p>Returns:</p> Name Type Description <code>sampler_mixed</code> <code>object</code> <p>The sampler results, contained in a sampler object, from  the determination of the unknown parameter. </p> <code>emcee_trace_mixed</code> <code>ndarray</code> <p>The trace of each parameter, with burnin samples  extracted.</p> Source code in <code>samba/mixing.py</code> <pre><code>def mixed_model(self, g_data, data, sigma, mixing_function='cosine', nsteps=1000):\n\n    r'''\n    A function that will run the emcee ensemble sampler for a given mixed model to determine at least one\n    unknown parameter in the mixing function selected. The function asks the user to decide which mixing\n    function to use, and runs the subsequent code to use the correct one. Functions sent to the sampler are\n    static methods defined at the end of this class.\n\n    Example:\n        LMM.mixed_model(g_data=np.linspace(0.0, 0.5, 20), data=np.array(), sigma=np.array(),\n            mixing_function='cosine', nsteps=3000)\n\n    Parameters:\n        g_data (linspace): The linspace over which the data was generated.\n\n        data (numpy.ndarray): An array of data points, either generated or supplied by \n            the user.\n\n        sigma (numpy.ndarray): An array of standard deviations at each data point.\n\n        mixing_function (str): The name of the mixing function to use for the LMM method. \n            Default is the piecewise cosine. \n\n        nsteps (int): The number of steps per walker for the sampler to use. \n\n    Returns:\n        sampler_mixed (object): The sampler results, contained in a sampler object, from \n            the determination of the unknown parameter. \n\n        emcee_trace_mixed (numpy.ndarray): The trace of each parameter, with burnin samples \n            extracted.\n    '''\n\n    #dictionary of LMM functions\n    self.function_mappings = {\n        'step': self.step,\n        'logistic': self.logistic,\n        'cdf': self.cdf,\n        'cosine': self.switchcos,\n    }\n\n    #ask user which mixing function to use\n    self.choice = mixing_function\n\n    #determine number of hyperparameters \n    if self.choice == 'step':\n        self.ndim = 1\n    elif self.choice == 'logistic' or self.choice == 'cdf':\n        self.ndim = 2 \n    elif self.choice == 'cosine':\n        self.ndim = 3\n    else:\n        raise ValueError('Mixing function requested is not found. Enter one of the valid options.')\n\n    #theory errors via error models\n    siglow = np.sqrt(self.u.variance_low(g_data, self.loworder[0]))\n    sighigh = np.sqrt(self.u.variance_high(g_data, self.highorder[0]))\n\n    #set up sampler\n    nwalkers = 2*int(3*self.ndim + 1)\n\n    #show total samples while running\n    total_samples = nwalkers * nsteps\n    print('Using {} walkers with {} steps each, for a total of {} samples.'.format(nwalkers, nsteps, total_samples))\n\n    #set starting points per parameter\n    starting_points = np.zeros((nwalkers, self.ndim))\n\n    #generalise for ndim=1,2,...!=3 and specify for 3\n    if self.ndim == 3:\n        starting_points[:,0] = np.random.uniform(0.12, 0.18, nwalkers)\n        starting_points[:,2] = np.random.uniform(0.19, 0.24, nwalkers)\n        starting_points[:,1] = np.random.uniform(0.25, 0.30, nwalkers)\n    else:\n        for i in range(self.ndim):\n            starting_points[:,i] = np.random.uniform(0.0, 1.0, nwalkers)\n\n    #set the mixing function\n    self.f = self._select_function(self.choice)\n\n    #call emcee\n    sampler_mixed = emcee.EnsembleSampler(nwalkers, self.ndim, self.sampler_mix, \\\n                                        args=[g_data, data, sigma, siglow, sighigh])\n    now = time.time()\n    sampler_mixed.run_mcmc(starting_points, nsteps)\n    stop = time.time()\n    print('Calculation finished!')\n\n    #time calculation\n    elapsed = int(stop - now)\n    if elapsed / 60 &lt; 1.0:\n        print(f\"Duration = {elapsed} sec.\")\n    elif elapsed / 60 &gt;= 1.0:\n        minutes = int(elapsed / 60)\n        seconds = int(elapsed - 60*minutes)\n        print(f\"Duration = {minutes} min, {seconds} sec.\")\n\n    #find the trace\n    emcee_trace_mixed = self.burnin_trace(sampler_mixed, nsteps)\n\n    return sampler_mixed, emcee_trace_mixed\n</code></pre>"},{"location":"mixing/#samba.mixing.LMM.plot_MAP","title":"<code>plot_MAP(g, map_values)</code>","text":"<p>A simple rough plotter to plot the weight/mixing function for the LMM method using the mixing function calculated at the points in g and the MAP values of its parameters. </p> Example <p>LMM.plot_MAP(g=np.linspace(), map_values=numpy.ndarray([]))</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>linspace</code> <p>The input space over which the mixing  is calculated.</p> required <code>map_values</code> <code>ndarray</code> <p>The results of the MAP_values()  function (MAP values of each parameter in the mixing function  selected).</p> required <p>Returns:</p> Type Description <p>None.</p> Source code in <code>samba/mixing.py</code> <pre><code>def plot_MAP(self, g, map_values):\n\n    r'''\n    A simple rough plotter to plot the weight/mixing function\n    for the LMM method using the mixing function calculated at\n    the points in g and the MAP values of its parameters. \n\n    Example:\n        LMM.plot_MAP(g=np.linspace(), map_values=numpy.ndarray([]))\n\n    Parameters:\n        g (numpy.linspace): The input space over which the mixing \n            is calculated.\n\n        map_values (numpy.ndarray): The results of the MAP_values() \n            function (MAP values of each parameter in the mixing function \n            selected).\n\n    Returns:\n        None.\n    '''\n\n    #set up figure\n    fig = plt.figure(figsize=(8,6), dpi=600)\n    ax = plt.axes()\n    ax.tick_params(axis='x', labelsize=18)\n    ax.tick_params(axis='y', labelsize=18)\n    ax.set_xlim(0.0, 1.0)\n    ax.set_ylim(0.0,1.0)\n    ax.set_xlabel('g', fontsize=22)\n    ax.set_ylabel(r'$\\alpha$(g; $\\theta$)', fontsize=22)\n\n    #solve the mixing function first and plot\n    switch = np.zeros([len(g)])\n    for i in range(len(g)):\n        switch[i] = self.f(map_values, g[i])\n    ax.plot(g, switch, 'k', linewidth=3, label=r'$\\alpha(g; \\theta)$') \n\n    #plot the MAP value lines\n    if len(map_values) == 1:\n        ax.axvline(x=map_values, color='darkorange', linestyle='dashed', label=r'$\\theta_{1}$')\n\n    elif len(map_values) == 2:\n        ax.axvline(x=map_values[0], color='darkorange', linestyle='dashed', label=r'$\\theta_{1}$')\n        ax.axvline(x=map_values[1], color='darkviolet', linestyle='dashdot', label=r'$\\theta_{2}$')\n\n    elif len(map_values) == 3:\n        ax.axvline(x=map_values[0], color='darkorange', linestyle='dashed', label=r'$\\theta_{1}$')\n        ax.axvline(x=map_values[2], color='darkviolet', linestyle='dashdot', label=r'$\\theta_{2}$')\n        ax.axvline(x=map_values[1], color='darkgreen', linestyle='dashed', label=r'$\\theta_{3}$')\n\n    ax.legend(loc='upper right', fontsize=18)\n\n    #fig.savefig('mixing_plot_MAP_5v5.pdf', bbox_inches='tight')\n\n    plt.show()\n\n    return None \n</code></pre>"},{"location":"mixing/#samba.mixing.LMM.plot_data","title":"<code>plot_data(g_true, g_data, data)</code>","text":"<p>The plotting function to display the generated data and true model. </p> Example <p>LMM.plot_data(g_true=np.linspace(0.0, 0.5, 100), g_data=np.linspace(0.0, 0.5, 20),  data=np.array([]))</p> <p>Parameters:</p> Name Type Description Default <code>g_true</code> <code>linspace</code> <p>The linspace desired for the true model to be calculated.</p> required <code>g_data</code> <code>linspace</code> <p>The linspace over which the data was generated. </p> required <code>data</code> <code>ndarray</code> <p>The array of data generated using the LMM.add_data function.</p> required <p>Returns:</p> Type Description <p>None.</p> Source code in <code>samba/mixing.py</code> <pre><code>def plot_data(self, g_true, g_data, data):\n\n    r'''\n    The plotting function to display the generated data and true model. \n\n    Example:\n        LMM.plot_data(g_true=np.linspace(0.0, 0.5, 100), g_data=np.linspace(0.0, 0.5, 20), \n        data=np.array([]))\n\n    Parameters:\n        g_true (linspace): The linspace desired for the true model to be calculated.\n\n        g_data (linspace): The linspace over which the data was generated. \n\n        data (numpy.ndarray): The array of data generated using the LMM.add_data function.\n\n    Returns:\n        None.\n\n    '''\n\n    #set up the plot\n    fig = plt.figure(figsize=(8,6), dpi=600)\n    ax = plt.axes()\n    ax.tick_params(axis='x', labelsize=18)\n    ax.tick_params(axis='y', labelsize=18)\n    ax.locator_params(nbins=6)\n    ax.xaxis.set_minor_locator(AutoMinorLocator())\n    ax.yaxis.set_minor_locator(AutoMinorLocator())\n    ax.set_xlim(min(g_true), max(g_true))\n    ax.set_ylim(1.2,3.2)\n\n    ax.plot(g_data, data, 'k.', label='Data set')\n    ax.plot(g_true, self.m.true_model(g_true), 'k', label='True model')\n\n    ax.legend(fontsize=18)\n    plt.show()\n\n    #save figure option\n    # response = input('Would you like to save this figure? (yes/no)')\n\n    # if response == 'yes':\n    #     name = input('Enter a file name (include .jpg, .png, etc.)')\n    #     fig.savefig(name, bbox_inches='tight')\n\n    return None\n</code></pre>"},{"location":"mixing/#samba.mixing.LMM.plot_ppd","title":"<code>plot_ppd(results, g_data, g_ppd, data, ppd_results, ppd_intervals, percent)</code>","text":"<p>A plotting function that can be used to plot the posterior predictive distribution (PPD) results (mean and  credible interval) obtained from calling the functions above in the main code, as well as data generated,  the true model, and the small- and large-g expansions chosen for the mixed model calculation. </p> Example <p>LMM.plot_ppd(g_data=np.linspace(0.0, 0.5, 20), g_true=np.linspace(0.0, 0.5, 100),      g_ppd=np.linspace(0.0, 0.5, 200), data=np.array(), ppd_results=np.array(),      ppd_intervals=np.array(), percent=68)</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>ndarray</code> <p>The mean or the median of the estimated parameters  from the posterior draws. </p> required <code>g_data</code> <code>linspace</code> <p>The linspace used to generate the data.</p> required <code>g_ppd</code> <code>linspace</code> <p>The linspace chosen to calculate the PPD over. </p> required <code>data</code> <code>ndarray</code> <p>An array of data either generated or supplied by  the user.</p> required <code>ppd_results</code> <code>ndarray</code> <p>An array of the mean of the PPD at each point  in the g_ppd linspace.</p> required <code>ppd_intervals</code> <code>ndarray</code> <p>A 2D array of the credibility interval calculated  for the PPD (containing both bounds).</p> required <code>percent</code> <code>int</code> <p>The percent credibility interval calculated for the variable  ppd_intervals (used in the plot legend). </p> required <p>Returns:</p> Type Description <p>None.</p> Source code in <code>samba/mixing.py</code> <pre><code>def plot_ppd(self, results, g_data, g_ppd, data, ppd_results, ppd_intervals, percent):\n\n    r'''\n    A plotting function that can be used to plot the posterior predictive distribution (PPD) results (mean and \n    credible interval) obtained from calling the functions above in the main code, as well as data generated, \n    the true model, and the small- and large-g expansions chosen for the mixed model calculation. \n\n    Example:\n        LMM.plot_ppd(g_data=np.linspace(0.0, 0.5, 20), g_true=np.linspace(0.0, 0.5, 100), \n            g_ppd=np.linspace(0.0, 0.5, 200), data=np.array(), ppd_results=np.array(), \n            ppd_intervals=np.array(), percent=68)\n\n    Parameters:\n        results (numpy.ndarray): The mean or the median of the estimated parameters \n            from the posterior draws. \n\n        g_data (linspace): The linspace used to generate the data.\n\n        g_ppd (linspace): The linspace chosen to calculate the PPD over. \n\n        data (numpy.ndarray): An array of data either generated or supplied by \n            the user.\n\n        ppd_results (numpy.ndarray): An array of the mean of the PPD at each point \n            in the g_ppd linspace.\n\n        ppd_intervals (numpy.ndarray): A 2D array of the credibility interval calculated \n            for the PPD (containing both bounds).\n\n        percent (int): The percent credibility interval calculated for the variable \n            ppd_intervals (used in the plot legend). \n\n    Returns:\n        None.\n    '''\n\n    fig = plt.figure(figsize=(8,6), dpi=600)\n    ax = plt.axes()\n    ax.tick_params(axis='x', labelsize=18)\n    ax.tick_params(axis='y', labelsize=18)\n    ax.locator_params(nbins=6)\n    ax.xaxis.set_minor_locator(AutoMinorLocator())\n    ax.yaxis.set_minor_locator(AutoMinorLocator())\n    ax.set_xlabel('g', fontsize=22)\n    ax.set_ylabel('F(g)', fontsize=22)\n\n    ax.set_xlim(0.0, 1.0)\n    ax.set_ylim(1.2, 3.2)\n    ax.set_yticks([1.2, 1.6, 2.0, 2.4, 2.8, 3.2])\n\n    #plot the data and true model\n    ax.plot(g_data, data, 'k.', label='Data set')  \n    ax.plot(g_ppd, self.m.true_model(g_ppd), 'k', label='True model')\n\n    #plot the expansions\n    ax.plot(g_ppd, self.m.low_g(g_ppd)[0,:], 'r--', label=r'$f_s$ ($N_s$ = {})'.format(self.loworder[0]))\n    ax.plot(g_ppd, self.m.high_g(g_ppd)[0,:], 'b--', label=r'$f_l$ ($N_l$ = {})'.format(self.highorder[0]))\n\n    #plot the PPD results from mixing\n    ax.plot(g_ppd, ppd_results, 'g', label='Mixed model')\n    ax.plot(g_ppd, ppd_intervals[:,0], 'g', linestyle='dotted', label=r'{}\\% CI (HPD)'.format(percent))\n    ax.plot(g_ppd, ppd_intervals[:,1], 'g', linestyle='dotted')\n    ax.fill_between(g_ppd, ppd_intervals[:,0], ppd_intervals[:,1], color='green', alpha=0.2)\n\n    #parameter results (vertical lines)\n    if self.choice == 'step':\n        ax.axvline(x=results, color='darkviolet', alpha=0.35, label=r'$\\theta_{1}$')\n    else:\n        ax.axvline(x=results[0], color='darkviolet', alpha=0.35, label=r\"$\\theta_{1}$, $\\theta_{2}$, $\\theta_{3}$\")\n        ax.axvline(x=results[1], color='darkviolet', alpha=0.35)\n\n    if len(results) == 3:\n        ax.axvline(x=results[2], color='darkviolet', alpha=0.35)\n\n    ax.legend(fontsize=18, loc='upper right')\n    plt.show()\n\n    # answer = input('Would you like to save the plot to a file (yes/no)?')\n\n    # if answer == 'yes':\n    #     name = input('Enter a file name to use (include file type as .png, .pdf, etc.).')\n    #     fig.savefig(name, bbox_inches='tight')\n\n    return None\n</code></pre>"},{"location":"mixing/#samba.mixing.LMM.ppd","title":"<code>ppd(trace, param_values, g_data, g, data, ci, plot=True)</code>","text":"<p>A function to calculate the posterior predictive distribution (PPD)  for any chosen mixing function defined in this class. </p> Example <p>LMM.ppd(trace, param_values=np.array([]),g_data=np.linspace(1e-6,1.0,10),      g_ppd=np.linspace(0.0, 0.5, 100), ci=68)</p> <p>Parameters:</p> Name Type Description Default <code>trace</code> <code>ndarray</code> <p>The trace of each of the parameters from  the sampler.</p> required <code>param_values</code> <code>ndarray</code> <p>The mean, median, or MAP values of the  parameters. </p> required <code>g_data</code> <code>linspace</code> <p>The linspace in g from which the data set  was calculated.</p> required <code>g</code> <code>linspace</code> <p>The linspace over which the PPD result will be  calculated.</p> required <code>data</code> <code>ndarray</code> <p>The data set used to calculate the mixed model. </p> required <code>ci</code> <code>int</code> <p>The desired credibility interval. Can be either 68 or 95.</p> required <code>plot</code> <code>bool</code> <p>The option to plot the PPD result with the series expansions and true model. Default is True. </p> <code>True</code> <p>Returns:</p> Name Type Description <code>switch_med_results</code> <code>ndarray</code> <p>The array of median values from the  PPD at each point in g.</p> <code>switch_g_intervals</code> <code>ndarray</code> <p>The array of credibility interval values  for the median results of the PPD.</p> Source code in <code>samba/mixing.py</code> <pre><code>def ppd(self, trace, param_values, g_data, g, data, ci, plot=True):\n\n    r'''\n    A function to calculate the posterior predictive distribution (PPD) \n    for any chosen mixing function defined in this class. \n\n    Example:\n        LMM.ppd(trace, param_values=np.array([]),g_data=np.linspace(1e-6,1.0,10), \n            g_ppd=np.linspace(0.0, 0.5, 100), ci=68)\n\n    Parameters:\n        trace (numpy.ndarray): The trace of each of the parameters from \n            the sampler.\n\n        param_values (numpy.ndarray): The mean, median, or MAP values of the \n            parameters. \n\n        g_data (numpy.linspace): The linspace in g from which the data set \n            was calculated.\n\n        g (numpy.linspace): The linspace over which the PPD result will be \n            calculated.\n\n        data (numpy.ndarray): The data set used to calculate the mixed model. \n\n        ci (int): The desired credibility interval. Can be either 68 or 95.\n\n        plot (bool): The option to plot the PPD result with the series expansions\n            and true model. Default is True. \n\n    Returns:\n        switch_med_results (numpy.ndarray): The array of median values from the \n            PPD at each point in g.\n\n        switch_g_intervals (numpy.ndarray): The array of credibility interval values \n            for the median results of the PPD.\n    '''\n\n    #convert list to array\n    trace = np.asarray(trace)\n\n    #check interval\n    if ci == 68:\n        ci = 0.68\n    elif ci == 95:\n        ci = 0.95\n\n    if self.choice != 'step':\n        result_array = np.empty([len(g), len(trace[0].T)])\n    elif self.choice == 'step':\n        result_array = np.empty([len(g), len(trace)])\n    gmax = max(g)\n\n    #determine which mixing function was used\n    if self.choice == 'step':\n        for i in range(len(g)):\n            for j in range(len(trace)):\n                result_array[i,j] = self.step(trace[j], g[i]) * self.m.low_g(g[i]) \\\n                    + (1.0 - self.step(trace[j], g[i])) \\\n                        * self.m.high_g(g[i])\n\n    elif self.choice == 'logistic' or self.choice == 'cdf':\n\n        for i in range(len(g)):\n            for j in range(len(trace[0].T)):\n\n                if (self.m.low_g(g[i]) - self.m.high_g(g[i]))\\\n                &gt; 0.1 and g[i] &gt; (0.25*gmax):\n                    result_array[i,j] = self.m.high_g(g[i])\n\n                elif (self.m.low_g(g[i]) - self.m.high_g(g[i])) &gt; 0.1:\n                    result_array[i,j] = self.m.low_g(g[i])\n\n                else:\n                    params = np.array([trace[0, j], trace[1, j]])\n\n                    result_array[i,j] = self.f(params, g[i])*self.m.low_g(g[i]) \\\n                                    + (1.0 - self.f(params, g[i])) \\\n                                    *self.m.high_g(g[i])\n\n    elif self.choice == 'cosine':\n\n        params = np.array([np.mean(trace[0,:]), np.mean(trace[1,:]), np.mean(trace[2,:])])\n\n        for i in range(len(g)):\n            for j in range(len(trace[0].T)):\n\n                params = np.array([trace[0, j], trace[1, j], trace[2, j]])\n\n                result_array[i,j] = self.switchcos(params, g[i]) * self.m.low_g(g[i]) \\\n                                + (1.0 - self.switchcos(params, g[i])) \\\n                                * self.m.high_g(g[i])\n\n    #define the credibility intervals\n    switch_med_results = np.empty([len(g)])\n    switch_g_intervals = np.empty([len(g), 2])\n\n    for i in range(len(g)):\n        switch_med_results[i] = statistics.median(result_array[i,:])\n        switch_g_intervals[i, :] = self.hpd_interval(result_array[i,:], ci)  # this is what I need (put in M-R curves)\n\n    #plot the PPD results\n    if plot is True:\n        self.plot_ppd(param_values, g_data, g, data, switch_med_results, switch_g_intervals, percent=68)\n\n    return switch_med_results, switch_g_intervals\n</code></pre>"},{"location":"mixing/#samba.mixing.LMM.sampler_mix","title":"<code>sampler_mix(params, g_data, data, sigma, siglow, sighigh)</code>","text":"<p>The model mixing function sent to the sampler to find the values of the parameters in the  selected mixing function. </p> Example <p>emcee.EnsembleSampler(nwalkers, self.sampler_mix,     args=[g_data, data, sigma])</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>ndarray</code> <p>The parameters that are being determined by the  sampler, in an array (not input).</p> required <code>g_data</code> <code>linspace</code> <p>The linspace used to generate the data.</p> required <code>data</code> <code>ndarray</code> <p>An array of data either generated or supplied by  the user. </p> required <code>sigma</code> <code>ndarray</code> <p>An array of standard deviations for each data point.</p> required <p>Returns:</p> Name Type Description <code>mixed_results</code> <code>ndarray</code> <p>The results of the mixing function for the  entire linspace in g, in an array format.</p> Source code in <code>samba/mixing.py</code> <pre><code>def sampler_mix(self, params, g_data, data, sigma, siglow, sighigh):\n\n    r'''\n    The model mixing function sent to the sampler to find the values of the parameters in the \n    selected mixing function. \n\n    Example:\n        emcee.EnsembleSampler(nwalkers, self.sampler_mix,\n            args=[g_data, data, sigma])\n\n    Parameters:\n        params (numpy.ndarray): The parameters that are being determined by the \n            sampler, in an array (not input).\n\n        g_data (linspace): The linspace used to generate the data.\n\n        data (numpy.ndarray): An array of data either generated or supplied by \n            the user. \n\n        sigma (numpy.ndarray): An array of standard deviations for each data point.\n\n    Returns:\n        mixed_results (numpy.ndarray): The results of the mixing function for the \n            entire linspace in g, in an array format.\n    '''\n\n    #set up arrays\n    mixed_likelihood = np.empty([len(g_data)])\n    log_ml = np.empty([len(g_data)])\n\n    #test prior first\n    logprior = self.p.lpdf(params)\n\n    if math.isnan(logprior) == True or np.isinf(-logprior) == True:\n        return -np.inf\n\n    else:\n\n        #likelihood mixing\n        for i in range(len(g_data)):\n            mixed_likelihood[i] = self.f(params, g_data[i]) * \\\n                                LMM.likelihood_low(self, g_data[i], data[i], sigma[i], siglow[i]) \\\n                                + (1.0- self.f(params, g_data[i])) * \\\n                                LMM.likelihood_high(self, g_data[i], data[i], sigma[i], sighigh[i])\n\n            if mixed_likelihood[i] &lt;= 0.0:\n                return -np.inf\n\n            log_ml[i] = np.log(mixed_likelihood[i])\n\n        total_lml = np.sum(log_ml)\n\n        #add the priors\n        mixed_results = total_lml + self.p.lpdf(params)\n\n        return mixed_results\n</code></pre>"},{"location":"mixing/#samba.mixing.LMM.stats_chain","title":"<code>stats_chain(chain, plot=True)</code>","text":"<p>Calculates the autocorrelation time and thins the samples accordingly for a better estimate of the mean, median, and MAP values. </p> Example <p>LMM.stats_chain(chain=emcee.object, plot=False)</p> <p>Parameters:</p> Name Type Description Default <code>chain</code> <code>object</code> <p>The object resulting from sampling the parameters using emcee. The chain of samples must be extracted from it. </p> required <code>plot</code> <code>bool</code> <p>The option to plot the traces of the sample chains and the corner plot of the parameter distributions. Default is True.  </p> <code>True</code> <p>Returns:</p> Name Type Description <code>thin</code> <code>ndarray</code> <p>The array of thinned samples per parameter.  Used externally to calculate the MAP values.</p> <code>median_results</code> <code>ndarray</code> <p>Each of the median parameter  values found from the sampling.</p> <code>mean_results</code> <code>ndarray</code> <p>Each of the mean parameter values found  from the sampling.</p> Source code in <code>samba/mixing.py</code> <pre><code>def stats_chain(self, chain, plot=True):\n\n    r'''\n    Calculates the autocorrelation time and thins the samples\n    accordingly for a better estimate of the mean, median, and MAP values. \n\n    Example: \n        LMM.stats_chain(chain=emcee.object, plot=False)\n\n    Parameters:\n        chain (object): The object resulting from sampling the parameters\n            using emcee. The chain of samples must be extracted\n            from it. \n\n        plot (bool): The option to plot the traces of the sample\n            chains and the corner plot of the parameter\n            distributions. Default is True.  \n\n    Returns:\n        thin (numpy.ndarray): The array of thinned samples per parameter. \n            Used externally to calculate the MAP values.\n\n        median_results (numpy.ndarray): Each of the median parameter \n            values found from the sampling.\n\n        mean_results (numpy.ndarray): Each of the mean parameter values found \n            from the sampling. \n    '''\n\n    #retrieve the chain\n    chain_result = chain.chain[:,:,:]\n\n    #\"quick and dirty\" method; will finish later more generally\n    if self.ndim == 1:\n\n        #set up arrays\n        chain1 = chain_result[:,:,0]\n\n        #flatten each individual array\n        flat1 = chain1.flatten()\n\n        #call autocorrelation to find the lengths\n        post_acors1 = self._autocorrelation(flat1, max_lag=200)\n\n        #determine the autocorrelation time\n        post_rho1 = post_acors1[25:35]\n\n        post_y = np.arange(10)\n        post_x1 = -np.log(post_rho1)\n\n        #linear fits\n        p1, _ = np.polyfit(post_x1, post_y, 1, cov=True)\n\n        #thin the samples given the determined autocorrelation time\n        thin1 = []\n        time = p1[0]\n\n        time = int(time)\n\n        for i in range(len(flat1)):\n            if i % time == 0:\n                thin1.append(flat1[i])\n\n        #array thinned samples\n        thin = np.array(thin1)\n\n        #call stats_trace for plots\n        if plot is True:\n            _, _ = LMM.stats_trace(self, thin)\n\n        #median calculation\n        median_1 = statistics.median(thin)\n\n        #mean calculation\n        mean_1 = np.mean(thin)\n\n        #arrays\n        mean_results = np.array([mean_1])\n        median_results = np.array([median_1])\n\n        return thin, mean_results, median_results\n\n    else:\n\n        #set up lists and arrays\n        chains = []\n        post_acors = []\n        post_rho = []\n        post_x = []\n        p = []\n\n        post_y = np.arange(10)\n\n        #create list using parameter arrays and flatten\n        for i in range(self.ndim):\n            chains.append(chain_result[:,:,i].flatten())\n\n        #max lag determination\n        max_lag = 200\n        if max_lag &gt; int(0.2*len(chains[0])):\n            max_lag = int(0.15*len(chains[0]))\n\n        #determine autocorrelation\n        for i in range(len(chains)):\n            post_acors.append(self._autocorrelation(chains[i], max_lag=max_lag))\n            post_rho.append(post_acors[i][25:35])\n            post_x.append(-np.log(post_rho[i]))\n            p_temp, _ = np.polyfit(post_x[i], post_y, 1, cov=True)\n            p.append(p_temp[0])\n\n    #thin the samples based on the results above\n    thin = []\n\n    #get the autocorrelation time for all parameters\n    for i in range(len(chains)-1):\n        if p[i] &gt; p[i+1]:\n            time = int(p[i])\n        else:\n            time = int(p[i+1])\n\n    #thin the samples for each parameter  ---&gt; double counting, need to separate params correctly\n    for i in range(self.ndim):\n        thin_temp = []\n        for j in range(len(chains[0])):\n            if j % time == 0:\n                thin_temp.append(chains[i][j])\n\n        thin.append(thin_temp)\n\n    #plot the traces\n    if plot is True:\n            _, _ = LMM.stats_trace(self, thin)\n\n    #median, mean calculations\n    median = []\n    mean = []\n\n    for i in range(len(thin)):\n        median.append(statistics.median(thin[i]))\n        mean.append(np.mean(thin[i]))\n\n    return thin, mean, median\n</code></pre>"},{"location":"mixing/#samba.mixing.LMM.stats_trace","title":"<code>stats_trace(trace)</code>","text":"<p>A function to calculate the mean and credible intervals corresponding to each parameter. The trace plots for each parameter are plotted. </p> Example <p>LMM.stats_trace(trace=np.array([]))</p> <p>Parameters:</p> Name Type Description Default <code>trace</code> <code>ndarray</code> <p>The trace from the sampler object that was generated  when estimating the parameters of the mixing function.</p> required <p>Returns:</p> Name Type Description <code>mean</code> <code>ndarray</code> <p>The array of mean values for each parameter.</p> <code>ci</code> <code>ndarray</code> <p>The array of sets of credible interval bounds for  each parameter.</p> Source code in <code>samba/mixing.py</code> <pre><code>def stats_trace(self, trace):\n\n    r'''\n    A function to calculate the mean and credible intervals corresponding to\n    each parameter. The trace plots for each parameter are plotted. \n\n    Example:\n        LMM.stats_trace(trace=np.array([]))\n\n    Parameters:\n        trace (numpy.ndarray): The trace from the sampler object that was generated \n            when estimating the parameters of the mixing function.\n\n    Returns:\n        mean (numpy.ndarray): The array of mean values for each parameter.\n\n        ci (numpy.ndarray): The array of sets of credible interval bounds for \n            each parameter. \n    '''\n\n    #calculate mean and credible intervals\n    mean = []\n    ci = []\n\n    if self.ndim == 1:\n        mean.append(np.mean(trace))\n        ci.append(self.hpd_interval(trace, 0.95))\n    else:\n        for i in range(self.ndim):\n            mean.append(np.mean(trace[i]))\n            ci.append(self.hpd_interval(trace[i], 0.95))\n\n    mean = np.asarray(mean)\n    ci = np.asarray(ci)\n\n    #plot traces with mean and credible intervals\n    fig, ax = plt.subplots(self.ndim,1,figsize=(7,4*self.ndim), dpi=600)\n\n    if self.ndim == 1:\n        ax.plot(trace, 'k')\n        ax.set_ylabel('Parameter 1', fontsize=22)\n        ax.set_title('Trace plot: Parameter 1', fontsize=22)\n\n        ax.axhline(y=mean, color='b', linestyle='solid', label='Mean')\n        ax.axhline(y=ci[0, 0], color='b', linestyle='dashed')\n        ax.axhline(y=ci[0, 1], color='b', linestyle='dashed')\n\n        #plot the median over the mean\n        med = []\n        med.append(np.median(trace[0]))\n        med = np.asarray(med)\n\n        ax.axhline(y=med, color='r', linestyle='solid', label='Median')\n        ax.legend(loc='upper right')\n\n        plt.show()\n\n\n    if self.ndim &gt; 1:\n        for irow in range(self.ndim):\n            ax[irow].plot(trace[irow], 'k')\n            ax[irow].set_ylabel('Parameter {0}'.format(irow+1), fontsize=22)\n            ax[irow].set_title('Trace plot: Parameter {0}'.format(irow+1), fontsize=22)\n\n            ax[irow].axhline(y=mean[irow], color='b', linestyle='solid', label='Mean')\n            ax[irow].axhline(y=ci[irow, 0], color='b', linestyle='dashed')\n            ax[irow].axhline(y=ci[irow, 1], color='b', linestyle='dashed')\n\n        #plot the median over the mean\n        med = []\n\n        for i in range(self.ndim):\n            med.append(np.median(trace[i]))\n\n        med = np.asarray(med)\n\n        for irow in range(self.ndim):\n            ax[irow].axhline(y=med[irow], color='r', linestyle='solid', label='Median')\n\n            ax[irow].legend(loc='upper right')\n\n        plt.show()\n\n        #corner plots for hyperparameter posteriors\n        fig, axs = plt.subplots(self.ndim,self.ndim, figsize=(8,8), dpi=600)\n        label = [\"Parameter 1\", \"Parameter 2\", \"Parameter 3\"]\n        trace = np.asarray(trace)\n        corner.corner(trace.T,labels=label, \\\n            quantiles=[0.16, 0.5, 0.84],fig=fig,show_titles=True, label_kwargs=dict(fontsize=16))\n        plt.show()\n\n    return mean, ci \n</code></pre>"},{"location":"mixing/#samba.mixing.LMM.step","title":"<code>step(params, g)</code>  <code>staticmethod</code>","text":"<p>A step mixing function to switch between two models.  Only useful for two models.</p> Example <p>step(params, g=0.2)</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>ndarray</code> <p>One single parameter to determine  where the step function will break from one model to the other.</p> required <code>g</code> <code>float</code> <p>One value of the input space. </p> required <p>Returns:</p> Type Description <p>The value of the step function at a specific </p> <p>point in g.</p> Source code in <code>samba/mixing.py</code> <pre><code>@staticmethod\ndef step(params, g):\n\n    r'''\n    A step mixing function to switch between two models. \n    ***Only useful for two models.***\n\n    Example:\n        step(params, g=0.2)\n\n    Parameters:\n        params (np.ndarray): One single parameter to determine \n            where the step function will break from one model to the other.\n\n        g (float): One value of the input space. \n\n    Returns:\n        The value of the step function at a specific \n        point in g. \n    '''\n\n    #enable step function\n    if g &lt; params:\n        return 1.0 \n    else:\n        return 0.0 \n</code></pre>"},{"location":"mixing/#samba.mixing.LMM.switchcos","title":"<code>switchcos(params, g)</code>  <code>staticmethod</code>","text":"<p>A piecewise function using two constants at either end, and two cosine functions in the centre, to be used as a mixing function. One free parameter, g3, is found by sampling. </p> Example <p>switchcos(params=np.array(), g=0.5)</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>ndarray</code> <p>The array of parameters to be determined by the sampler  (here labelled g1, g2, and g3, where g1 is the separation point between the first constant  function and the first cosine function, g2 is the separation point between the second  cosine function and the second constant function,  and g3 is the point between the two cosine functions). </p> required <code>g</code> <code>float</code> <p>The value of g that this cosine function is calculated at.</p> required <p>Returns:</p> Type Description <p>The value of the function at a specific point in g.</p> Source code in <code>samba/mixing.py</code> <pre><code>@staticmethod\ndef switchcos(params, g):\n\n    r'''\n    A piecewise function using two constants at either end, and two cosine functions in the centre,\n    to be used as a mixing function. One free parameter, g3, is found by sampling. \n\n    Example:\n        switchcos(params=np.array(), g=0.5)\n\n    Parameters:\n        params (numpy.ndarray): The array of parameters to be determined by the sampler \n            (here labelled g1, g2, and g3, where g1 is the separation point between the first constant \n            function and the first cosine function, g2 is the separation point between the second \n            cosine function and the second constant function, \n            and g3 is the point between the two cosine functions). \n\n        g (float): The value of g that this cosine function is calculated at.\n\n    Returns:\n        The value of the function at a specific point in g. \n    '''\n\n    #unpack the parameters\n    g1, g2, g3 = params\n\n    if g1 &gt; g2 or g2 &lt; g3 or g1 &gt; g3:\n        return -np.inf\n\n    if g &lt;= g1:\n        return 1.0\n\n    elif g &lt;= g3:\n        return (1.0 + np.cos((np.pi/2.0) * ((g - g1)/(g3 - g1))))/2.0\n\n    elif g &lt; g2:\n        return 0.5 + np.cos((np.pi/2.0) * (1.0 + ((g - g3)/(g2 - g3))))/2.0\n\n    else:\n        return 0.0\n</code></pre>"},{"location":"models/","title":"Models","text":"<p>The models in SAMBA are two expansions of a toy model, where the full toy model is given by</p> <p>$$  F(g) = \\int_{-\\infty}^{\\infty} dx~ e^{-\\frac{x^{2}}{2} - g^{2} x^{4}} = \\frac{e^{\\frac{1}{32 g^{2}}}}{2 \\sqrt{2}g} K_{\\frac{1}{4}}\\left(\\frac{1}{32 g^{2}} \\right). $$</p> <p>The two expansions are limits taken at $g = 0$ and $g = \\infty$:</p> <p>$$ F_{s}^{N_s}(g) = \\sum_{k=0}^{N_{s}} s_{k} g^{k}, $$</p> <p>and </p> <p>$$ F_{l}^{N_{l}}(g) = \\frac{1}{\\sqrt{g}} \\sum_{k=0}^{N_{l}} l_{k} g^{-k}, $$</p> <p>with coefficients given as:</p> <p>$$ s_{2k} = \\frac{\\sqrt{2} \\Gamma{(2k + 1/2)}}{k!} (-4)^{k},~~~~~s_{2k + 1} = 0 $$</p> <p>and</p> <p>$$ l_{k} = \\frac{\\Gamma{\\left(\\frac{k}{2} + \\frac{1}{4}\\right)}}{2k!} \\left(-\\frac{1}{2}\\right)^{k}. $$</p> <p>We also include models for the uncertainties of each expansion, given in the uninformative limit, for the small-$g$ expansion, as</p> <p>$$ \\sigma_{N_s}(g)= \\Gamma(N_s+3) g^{N_s + 2} \\bar{c}, $$</p> <p>if $N_s$ is even, and</p> <p>$$ \\sigma_{N_s}(g)= \\Gamma(N_s+2) g^{N_s+1} \\bar{c}, $$</p> <p>if $N_s$ is odd. For the large-$g$ limit,</p> <p>$$ \\sigma_{N_l}(g)=\\frac{1}{\\Gamma(N_l+2)} \\frac{1}{g^{N_l+3/2}} \\bar{d}. $$</p> <p>We also devise expressions for the informative limit, for the small-$g$ expansion, as</p> <p>$$ \\sigma_{N_s}(g)= \\Gamma(N_s/2+1) (4g)^{N_s + 2} \\bar{c},  $$</p> <p>if $N_s$ is even, and</p> <p>$$ \\sigma_{N_s}(g)= \\Gamma(N_s/2+1/2) (4g)^{N_s+1} \\bar{c}, $$</p> <p>if $N_s$ is odd. For the large-$g$ limit,</p> <p>$$ \\sigma_{N_l}(g)=\\left(\\frac{1}{4g}\\right)^{N_l + 3/2} \\frac{1}{\\Gamma(N_l/2+3/2)} \\bar{d}. $$</p>"},{"location":"models/#samba.models.Models","title":"<code>Models</code>","text":"Source code in <code>samba/models.py</code> <pre><code>class Models():\n\n    def __init__(self, loworder, highorder):\n\n        r'''\n        The class containing the expansion models from Honda's paper\n        and the means to plot them. \n\n        Example:\n            Models(loworder=np.array([2]), highorder=np.array([5]))\n\n        Parameters:\n            loworder (numpy.ndarray, int, float): The value of N_s to be used \n                to truncate the small-g expansion. Can be an array of multiple \n                values or one. \n\n            highorder (numpy.ndarray, int, float): The value of N_l to be used \n                to truncate the large-g expansion. Can be an array of multiple \n                values or one. \n\n        Returns:\n            None.\n        '''\n\n        #check type and assign to class variable\n        if isinstance(loworder, float) == True or isinstance(loworder, int) == True:\n            loworder = np.array([loworder])\n\n        #check type and assign to class variable\n        if isinstance(highorder, float) == True or isinstance(highorder, int) == True:\n            highorder = np.array([highorder])\n\n        self.loworder = loworder\n        self.highorder = highorder \n\n        return None \n\n\n    def low_g(self, g):\n\n        '''\n        A function to calculate the small-g divergent asymptotic expansion for a \n        given range in the coupling constant, g.\n\n        Example:\n            Models.low_g(g=np.linspace(0.0, 0.5, 20))\n\n        Parameters:\n            g (linspace): The linspace of the coupling constant for \n                this calculation. \n\n        Returns:\n            output (numpy.ndarray): The array of values of the expansion in \n                small-g at each point in g_true space, for each value of loworder \n                (highest power the expansion reaches).\n        '''\n\n        output = []\n\n        for order in self.loworder:\n            low_c = np.empty([int(order)+1])\n            low_terms = np.empty([int(order) + 1])\n\n            #if g is an array, execute here\n            try:\n                value = np.empty([len(g)])\n\n                #loop over array in g\n                for i in range(len(g)):      \n\n                    #loop over orders\n                    for k in range(int(order)+1):\n\n                        if k % 2 == 0:\n                            low_c[k] = np.sqrt(2.0) * special.gamma(k + 0.5) * (-4.0)**(k//2) / (math.factorial(k//2))\n                        else:\n                            low_c[k] = 0\n\n                        low_terms[k] = low_c[k] * g[i]**(k) \n\n                    value[i] = np.sum(low_terms)\n\n                output.append(value)\n                data = np.array(output, dtype = np.float64)\n\n            #if g is a single value, execute here\n            except:\n                value = 0.0\n                for k in range(int(order)+1):\n\n                    if k % 2 == 0:\n                        low_c[k] = np.sqrt(2.0) * special.gamma(k + 0.5) * (-4.0)**(k//2) / (math.factorial(k//2))\n                    else:\n                        low_c[k] = 0\n\n                    low_terms[k] = low_c[k] * g**(k) \n\n                value = np.sum(low_terms)\n                data = value\n\n        return data\n\n\n    def high_g(self, g):\n\n        r'''\n        A function to calculate the large-g convergent Taylor expansion for a given range in the coupling \n        constant, g.\n\n        Example:\n            Models.high_g(g=np.linspace(1e-6,1.0,100))\n\n        Parameters:\n            g (linspace): The linspace of the coupling constant for this calculation.\n\n        Returns:\n            output (numpy.ndarray): The array of values of the expansion at large-g at \n                each point in g_true space, for each value of highorder (highest power \n                the expansion reaches).\n        '''\n\n        output = []\n\n        for order in self.highorder:\n            high_c = np.empty([int(order) + 1])\n            high_terms = np.empty([int(order) + 1])\n\n            #if g is an array, execute here\n            try:\n                value = np.empty([len(g)])\n\n                #loop over array in g\n                for i in range(len(g)):\n\n                    #loop over orders\n                    for k in range(int(order)+1):\n\n                        high_c[k] = special.gamma(k/2.0 + 0.25) * (-0.5)**k / (2.0 * math.factorial(k))\n\n                        high_terms[k] = (high_c[k] * g[i]**(-k)) / np.sqrt(g[i])\n\n                    #sum the terms for each value of g\n                    value[i] = np.sum(high_terms)\n\n                output.append(value)\n\n                data = np.array(output, dtype = np.float64)\n\n            #if g is a single value, execute here           \n            except:\n                value = 0.0\n\n                #loop over orders\n                for k in range(int(order)+1):\n\n                    high_c[k] = special.gamma(k/2.0 + 0.25) * (-0.5)**k / (2.0 * math.factorial(k))\n\n                    high_terms[k] = (high_c[k] * g**(-k)) / np.sqrt(g) \n\n                #sum the terms for each value of g\n                value = np.sum(high_terms)\n                data = value\n\n        return data \n\n\n    def true_model(self, g):\n\n        r'''\n        The true model of the zero-dimensional phi^4 theory partition function using an input \n        linspace.\n\n        Example:\n            Models.true_model(g=np.linspace(0.0, 0.5, 100))\n\n        Parameters:\n            g (linspace): The linspace for g desired to calculate the true model. This can be \n                the g_true linspace, g_data linspace, or another linspace of the user's choosing. \n\n        Returns:\n            model (numpy.ndarray): The model calculated at each point in g space. \n        '''\n\n        #define a function for the integrand\n        def function(x,g):\n            return np.exp(-(x**2.0)/2.0 - (g**2.0 * x**4.0)) \n\n        #initialization\n        self.model = np.zeros([len(g)])\n\n        #perform the integral for each g\n        for i in range(len(g)):\n\n            self.model[i], self.err = integrate.quad(function, -np.inf, np.inf, args=(g[i],))\n\n        return self.model \n\n\n    def plot_models(self, g, only_true=False):\n\n        r'''\n        A plotting function to produce a figure of the model expansions calculated in Models.low_g and Models.high_g, \n        and including the true model calculated using Mixing.true_model.\n\n        Example:\n            Mixing.plot_models(g=np.linspace(0.0, 0.5, 100))\n\n        Parameters:\n            g (linspace): The linspace in on which the models will be plotted here. \n\n        Returns:\n            None.\n        '''\n\n        #uncertainties\n        unc = Uncertainties()\n\n        #set up the plot\n        fig = plt.figure(figsize=(8,6), dpi=600)\n        ax = plt.axes()\n        ax.tick_params(axis='x', labelsize=18)\n        ax.tick_params(axis='y', labelsize=18)\n        ax.locator_params(nbins=8)\n        ax.xaxis.set_minor_locator(AutoMinorLocator())\n        ax.yaxis.set_minor_locator(AutoMinorLocator())\n        ax.set_xlabel('g', fontsize=22)\n        ax.set_ylabel('F(g)', fontsize=22)\n        ax.set_facecolor(\"white\")\n\n        #x and y limits\n        if min(g) == 1e-6:\n            ax.set_xlim(0.0, max(g))\n        else:\n            ax.set_xlim(min(g), max(g))\n        ax.set_ylim(1.0,3.0)\n      #  ax.set_ylim(1.2,3.2)\n      #  ax.set_yticks([1.2, 1.6, 2.0, 2.4, 2.8, 3.2])\n\n        #title option\n        ax.set_title('F(g): expansions and true model', fontsize=22)\n\n        #plot the true model \n        ax.plot(g, self.true_model(g), 'k', label='True model')\n\n        if only_true is False:\n\n            #add linestyle cycler\n            linestyle_cycler = cycler(linestyle=['dashed'])\n            ax.set_prop_cycle(linestyle_cycler)\n\n            #for each large-g order, calculate and plot\n            for i,j in zip(range(len(self.loworder)), self.loworder):\n                ax.plot(g, self.low_g(g)[i,:], color='r', label=r'$f_s$ ($N_s$ = {})'.format(j))\n\n            #for each large-g order, calculate and plot\n            for i,j in zip(range(len(self.highorder)), self.highorder):\n                ax.plot(g, self.high_g(g)[i,:], color='b', label=r'$f_l$ ($N_l$ = {})'.format(j))\n\n            #calculate uncertainty bands\n            std_low = []\n            std_high = []\n\n            for i in (self.loworder):\n                std_low.append(np.sqrt(unc.variance_low(g,i)))\n\n            for i in (self.highorder):\n                std_high.append(np.sqrt(unc.variance_high(g,i)))\n\n            #plot uncertainties as fill-between\n            for i,j in zip(range(len(self.loworder)), self.loworder):\n                ax.plot(g, self.low_g(g)[i,:]-std_low[i], color='red', linestyle='dotted')\n                ax.plot(g, self.low_g(g)[i,:]+std_low[i], color='red', linestyle='dotted')\n#                 ax.fill_between(g, self.low_g(g)[i,:]-std_low[i], self.low_g(g)[i,:]+std_low[i],\n#                             zorder=i-5, facecolor='red', alpha=0.1, lw=0.6, label=r'$f_s$ ($N_s$ = {}) 68\\% CI'.format(j))\n\n            for i,j in zip(range(len(self.highorder)), self.highorder):\n                ax.plot(g, self.high_g(g)[i,:]-std_high[i], color='blue', linestyle='dotted')\n                ax.plot(g, self.high_g(g)[i,:]+std_high[i], color='blue', linestyle='dotted')\n#                 ax.fill_between(g, self.high_g(g)[i,:]-std_high[i], self.high_g(g)[i,:]+std_high[i],\n#                                 zorder=i-5, facecolor='blue', alpha=0.1, lw=0.6, label=r'$f_l$ ($N_l$ = {}) 68\\% CI'.format(j))\n\n        ax.legend(fontsize=16, loc='upper right')\n        plt.show()\n\n        return fig\n\n\n    def residuals(self):\n\n        r'''\n        A calculation and plot of the residuals of the model expansions vs the true model values at each point in g.\n        g is set internally for this plot, as the plot must be shown in loglog format to see the power law of the\n        residuals. \n\n        Example:\n            Mixing.residuals()\n\n        Parameters:\n            None.\n\n        Returns:\n            None. \n        '''\n\n        #set up the plot\n        fig = plt.figure(figsize=(8,6), dpi=600)\n        ax = plt.axes()\n        ax.tick_params(axis='x', labelsize=18)\n        ax.tick_params(axis='y', labelsize=18)\n        ax.xaxis.set_minor_locator(AutoMinorLocator())\n        ax.yaxis.set_minor_locator(AutoMinorLocator())\n        ax.set_xlabel('g', fontsize=22)\n        ax.set_ylabel('Residual', fontsize=22)\n        ax.set_title('F(g): residuals', fontsize=22)\n        ax.set_xlim(1e-2, 10.)\n        ax.set_ylim(1e-6, 1e17)\n\n        #set range for g\n        g_ext = np.logspace(-6., 6., 800)\n\n        #set up marker cycler\n        marker_cycler = cycler(marker=['.', '*', '+', '.', '*', '+'])\n        ax.set_prop_cycle(marker_cycler)\n\n        #calculate true model\n        value_true = self.true_model(g_ext)\n\n        #for each small-g order, plot\n        valuelow = np.zeros([len(self.loworder), len(g_ext)])\n        residlow = np.zeros([len(self.loworder), len(g_ext)])\n\n        for i,j in zip(range(len(self.loworder)), self.loworder):\n            valuelow[i,:] = self.low_g(g_ext)[i]\n            residlow[i,:] = (valuelow[i,:] - value_true)/value_true\n            ax.loglog(g_ext, abs(residlow[i,:]), 'r', linestyle=\"None\", label=r\"$F_s({})$\".format(j))\n\n        #for each large-g order, plot\n        valuehi = np.zeros([len(self.highorder), len(g_ext)])\n        residhi = np.zeros([len(self.highorder), len(g_ext)])\n\n        for i,j in zip(range(len(self.highorder)), self.highorder):\n            valuehi[i,:] = self.high_g(g_ext)[i]\n            residhi[i,:] = (valuehi[i,:] - value_true)/value_true\n            ax.loglog(g_ext, abs(residhi[i,:]), 'b', linestyle=\"None\", label=r\"$F_l({})$\".format(j))\n\n        ax.legend(fontsize=18)\n        plt.show()\n</code></pre>"},{"location":"models/#samba.models.Models.__init__","title":"<code>__init__(loworder, highorder)</code>","text":"<p>The class containing the expansion models from Honda's paper and the means to plot them. </p> Example <p>Models(loworder=np.array([2]), highorder=np.array([5]))</p> <p>Parameters:</p> Name Type Description Default <code>loworder</code> <code>(ndarray, int, float)</code> <p>The value of N_s to be used  to truncate the small-g expansion. Can be an array of multiple  values or one. </p> required <code>highorder</code> <code>(ndarray, int, float)</code> <p>The value of N_l to be used  to truncate the large-g expansion. Can be an array of multiple  values or one. </p> required <p>Returns:</p> Type Description <p>None.</p> Source code in <code>samba/models.py</code> <pre><code>def __init__(self, loworder, highorder):\n\n    r'''\n    The class containing the expansion models from Honda's paper\n    and the means to plot them. \n\n    Example:\n        Models(loworder=np.array([2]), highorder=np.array([5]))\n\n    Parameters:\n        loworder (numpy.ndarray, int, float): The value of N_s to be used \n            to truncate the small-g expansion. Can be an array of multiple \n            values or one. \n\n        highorder (numpy.ndarray, int, float): The value of N_l to be used \n            to truncate the large-g expansion. Can be an array of multiple \n            values or one. \n\n    Returns:\n        None.\n    '''\n\n    #check type and assign to class variable\n    if isinstance(loworder, float) == True or isinstance(loworder, int) == True:\n        loworder = np.array([loworder])\n\n    #check type and assign to class variable\n    if isinstance(highorder, float) == True or isinstance(highorder, int) == True:\n        highorder = np.array([highorder])\n\n    self.loworder = loworder\n    self.highorder = highorder \n\n    return None \n</code></pre>"},{"location":"models/#samba.models.Models.high_g","title":"<code>high_g(g)</code>","text":"<p>A function to calculate the large-g convergent Taylor expansion for a given range in the coupling  constant, g.</p> Example <p>Models.high_g(g=np.linspace(1e-6,1.0,100))</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>linspace</code> <p>The linspace of the coupling constant for this calculation.</p> required <p>Returns:</p> Name Type Description <code>output</code> <code>ndarray</code> <p>The array of values of the expansion at large-g at  each point in g_true space, for each value of highorder (highest power  the expansion reaches).</p> Source code in <code>samba/models.py</code> <pre><code>def high_g(self, g):\n\n    r'''\n    A function to calculate the large-g convergent Taylor expansion for a given range in the coupling \n    constant, g.\n\n    Example:\n        Models.high_g(g=np.linspace(1e-6,1.0,100))\n\n    Parameters:\n        g (linspace): The linspace of the coupling constant for this calculation.\n\n    Returns:\n        output (numpy.ndarray): The array of values of the expansion at large-g at \n            each point in g_true space, for each value of highorder (highest power \n            the expansion reaches).\n    '''\n\n    output = []\n\n    for order in self.highorder:\n        high_c = np.empty([int(order) + 1])\n        high_terms = np.empty([int(order) + 1])\n\n        #if g is an array, execute here\n        try:\n            value = np.empty([len(g)])\n\n            #loop over array in g\n            for i in range(len(g)):\n\n                #loop over orders\n                for k in range(int(order)+1):\n\n                    high_c[k] = special.gamma(k/2.0 + 0.25) * (-0.5)**k / (2.0 * math.factorial(k))\n\n                    high_terms[k] = (high_c[k] * g[i]**(-k)) / np.sqrt(g[i])\n\n                #sum the terms for each value of g\n                value[i] = np.sum(high_terms)\n\n            output.append(value)\n\n            data = np.array(output, dtype = np.float64)\n\n        #if g is a single value, execute here           \n        except:\n            value = 0.0\n\n            #loop over orders\n            for k in range(int(order)+1):\n\n                high_c[k] = special.gamma(k/2.0 + 0.25) * (-0.5)**k / (2.0 * math.factorial(k))\n\n                high_terms[k] = (high_c[k] * g**(-k)) / np.sqrt(g) \n\n            #sum the terms for each value of g\n            value = np.sum(high_terms)\n            data = value\n\n    return data \n</code></pre>"},{"location":"models/#samba.models.Models.low_g","title":"<code>low_g(g)</code>","text":"<p>A function to calculate the small-g divergent asymptotic expansion for a  given range in the coupling constant, g.</p> Example <p>Models.low_g(g=np.linspace(0.0, 0.5, 20))</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>linspace</code> <p>The linspace of the coupling constant for  this calculation. </p> required <p>Returns:</p> Name Type Description <code>output</code> <code>ndarray</code> <p>The array of values of the expansion in  small-g at each point in g_true space, for each value of loworder  (highest power the expansion reaches).</p> Source code in <code>samba/models.py</code> <pre><code>def low_g(self, g):\n\n    '''\n    A function to calculate the small-g divergent asymptotic expansion for a \n    given range in the coupling constant, g.\n\n    Example:\n        Models.low_g(g=np.linspace(0.0, 0.5, 20))\n\n    Parameters:\n        g (linspace): The linspace of the coupling constant for \n            this calculation. \n\n    Returns:\n        output (numpy.ndarray): The array of values of the expansion in \n            small-g at each point in g_true space, for each value of loworder \n            (highest power the expansion reaches).\n    '''\n\n    output = []\n\n    for order in self.loworder:\n        low_c = np.empty([int(order)+1])\n        low_terms = np.empty([int(order) + 1])\n\n        #if g is an array, execute here\n        try:\n            value = np.empty([len(g)])\n\n            #loop over array in g\n            for i in range(len(g)):      \n\n                #loop over orders\n                for k in range(int(order)+1):\n\n                    if k % 2 == 0:\n                        low_c[k] = np.sqrt(2.0) * special.gamma(k + 0.5) * (-4.0)**(k//2) / (math.factorial(k//2))\n                    else:\n                        low_c[k] = 0\n\n                    low_terms[k] = low_c[k] * g[i]**(k) \n\n                value[i] = np.sum(low_terms)\n\n            output.append(value)\n            data = np.array(output, dtype = np.float64)\n\n        #if g is a single value, execute here\n        except:\n            value = 0.0\n            for k in range(int(order)+1):\n\n                if k % 2 == 0:\n                    low_c[k] = np.sqrt(2.0) * special.gamma(k + 0.5) * (-4.0)**(k//2) / (math.factorial(k//2))\n                else:\n                    low_c[k] = 0\n\n                low_terms[k] = low_c[k] * g**(k) \n\n            value = np.sum(low_terms)\n            data = value\n\n    return data\n</code></pre>"},{"location":"models/#samba.models.Models.plot_models","title":"<code>plot_models(g, only_true=False)</code>","text":"<p>A plotting function to produce a figure of the model expansions calculated in Models.low_g and Models.high_g,  and including the true model calculated using Mixing.true_model.</p> Example <p>Mixing.plot_models(g=np.linspace(0.0, 0.5, 100))</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>linspace</code> <p>The linspace in on which the models will be plotted here. </p> required <p>Returns:</p> Type Description <p>None.</p> Source code in <code>samba/models.py</code> <pre><code>    def plot_models(self, g, only_true=False):\n\n        r'''\n        A plotting function to produce a figure of the model expansions calculated in Models.low_g and Models.high_g, \n        and including the true model calculated using Mixing.true_model.\n\n        Example:\n            Mixing.plot_models(g=np.linspace(0.0, 0.5, 100))\n\n        Parameters:\n            g (linspace): The linspace in on which the models will be plotted here. \n\n        Returns:\n            None.\n        '''\n\n        #uncertainties\n        unc = Uncertainties()\n\n        #set up the plot\n        fig = plt.figure(figsize=(8,6), dpi=600)\n        ax = plt.axes()\n        ax.tick_params(axis='x', labelsize=18)\n        ax.tick_params(axis='y', labelsize=18)\n        ax.locator_params(nbins=8)\n        ax.xaxis.set_minor_locator(AutoMinorLocator())\n        ax.yaxis.set_minor_locator(AutoMinorLocator())\n        ax.set_xlabel('g', fontsize=22)\n        ax.set_ylabel('F(g)', fontsize=22)\n        ax.set_facecolor(\"white\")\n\n        #x and y limits\n        if min(g) == 1e-6:\n            ax.set_xlim(0.0, max(g))\n        else:\n            ax.set_xlim(min(g), max(g))\n        ax.set_ylim(1.0,3.0)\n      #  ax.set_ylim(1.2,3.2)\n      #  ax.set_yticks([1.2, 1.6, 2.0, 2.4, 2.8, 3.2])\n\n        #title option\n        ax.set_title('F(g): expansions and true model', fontsize=22)\n\n        #plot the true model \n        ax.plot(g, self.true_model(g), 'k', label='True model')\n\n        if only_true is False:\n\n            #add linestyle cycler\n            linestyle_cycler = cycler(linestyle=['dashed'])\n            ax.set_prop_cycle(linestyle_cycler)\n\n            #for each large-g order, calculate and plot\n            for i,j in zip(range(len(self.loworder)), self.loworder):\n                ax.plot(g, self.low_g(g)[i,:], color='r', label=r'$f_s$ ($N_s$ = {})'.format(j))\n\n            #for each large-g order, calculate and plot\n            for i,j in zip(range(len(self.highorder)), self.highorder):\n                ax.plot(g, self.high_g(g)[i,:], color='b', label=r'$f_l$ ($N_l$ = {})'.format(j))\n\n            #calculate uncertainty bands\n            std_low = []\n            std_high = []\n\n            for i in (self.loworder):\n                std_low.append(np.sqrt(unc.variance_low(g,i)))\n\n            for i in (self.highorder):\n                std_high.append(np.sqrt(unc.variance_high(g,i)))\n\n            #plot uncertainties as fill-between\n            for i,j in zip(range(len(self.loworder)), self.loworder):\n                ax.plot(g, self.low_g(g)[i,:]-std_low[i], color='red', linestyle='dotted')\n                ax.plot(g, self.low_g(g)[i,:]+std_low[i], color='red', linestyle='dotted')\n#                 ax.fill_between(g, self.low_g(g)[i,:]-std_low[i], self.low_g(g)[i,:]+std_low[i],\n#                             zorder=i-5, facecolor='red', alpha=0.1, lw=0.6, label=r'$f_s$ ($N_s$ = {}) 68\\% CI'.format(j))\n\n            for i,j in zip(range(len(self.highorder)), self.highorder):\n                ax.plot(g, self.high_g(g)[i,:]-std_high[i], color='blue', linestyle='dotted')\n                ax.plot(g, self.high_g(g)[i,:]+std_high[i], color='blue', linestyle='dotted')\n#                 ax.fill_between(g, self.high_g(g)[i,:]-std_high[i], self.high_g(g)[i,:]+std_high[i],\n#                                 zorder=i-5, facecolor='blue', alpha=0.1, lw=0.6, label=r'$f_l$ ($N_l$ = {}) 68\\% CI'.format(j))\n\n        ax.legend(fontsize=16, loc='upper right')\n        plt.show()\n\n        return fig\n</code></pre>"},{"location":"models/#samba.models.Models.residuals","title":"<code>residuals()</code>","text":"<p>A calculation and plot of the residuals of the model expansions vs the true model values at each point in g. g is set internally for this plot, as the plot must be shown in loglog format to see the power law of the residuals. </p> Example <p>Mixing.residuals()</p> <p>Returns:</p> Type Description <p>None.</p> Source code in <code>samba/models.py</code> <pre><code>def residuals(self):\n\n    r'''\n    A calculation and plot of the residuals of the model expansions vs the true model values at each point in g.\n    g is set internally for this plot, as the plot must be shown in loglog format to see the power law of the\n    residuals. \n\n    Example:\n        Mixing.residuals()\n\n    Parameters:\n        None.\n\n    Returns:\n        None. \n    '''\n\n    #set up the plot\n    fig = plt.figure(figsize=(8,6), dpi=600)\n    ax = plt.axes()\n    ax.tick_params(axis='x', labelsize=18)\n    ax.tick_params(axis='y', labelsize=18)\n    ax.xaxis.set_minor_locator(AutoMinorLocator())\n    ax.yaxis.set_minor_locator(AutoMinorLocator())\n    ax.set_xlabel('g', fontsize=22)\n    ax.set_ylabel('Residual', fontsize=22)\n    ax.set_title('F(g): residuals', fontsize=22)\n    ax.set_xlim(1e-2, 10.)\n    ax.set_ylim(1e-6, 1e17)\n\n    #set range for g\n    g_ext = np.logspace(-6., 6., 800)\n\n    #set up marker cycler\n    marker_cycler = cycler(marker=['.', '*', '+', '.', '*', '+'])\n    ax.set_prop_cycle(marker_cycler)\n\n    #calculate true model\n    value_true = self.true_model(g_ext)\n\n    #for each small-g order, plot\n    valuelow = np.zeros([len(self.loworder), len(g_ext)])\n    residlow = np.zeros([len(self.loworder), len(g_ext)])\n\n    for i,j in zip(range(len(self.loworder)), self.loworder):\n        valuelow[i,:] = self.low_g(g_ext)[i]\n        residlow[i,:] = (valuelow[i,:] - value_true)/value_true\n        ax.loglog(g_ext, abs(residlow[i,:]), 'r', linestyle=\"None\", label=r\"$F_s({})$\".format(j))\n\n    #for each large-g order, plot\n    valuehi = np.zeros([len(self.highorder), len(g_ext)])\n    residhi = np.zeros([len(self.highorder), len(g_ext)])\n\n    for i,j in zip(range(len(self.highorder)), self.highorder):\n        valuehi[i,:] = self.high_g(g_ext)[i]\n        residhi[i,:] = (valuehi[i,:] - value_true)/value_true\n        ax.loglog(g_ext, abs(residhi[i,:]), 'b', linestyle=\"None\", label=r\"$F_l({})$\".format(j))\n\n    ax.legend(fontsize=18)\n    plt.show()\n</code></pre>"},{"location":"models/#samba.models.Models.true_model","title":"<code>true_model(g)</code>","text":"<p>The true model of the zero-dimensional phi^4 theory partition function using an input  linspace.</p> Example <p>Models.true_model(g=np.linspace(0.0, 0.5, 100))</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>linspace</code> <p>The linspace for g desired to calculate the true model. This can be  the g_true linspace, g_data linspace, or another linspace of the user's choosing. </p> required <p>Returns:</p> Name Type Description <code>model</code> <code>ndarray</code> <p>The model calculated at each point in g space.</p> Source code in <code>samba/models.py</code> <pre><code>def true_model(self, g):\n\n    r'''\n    The true model of the zero-dimensional phi^4 theory partition function using an input \n    linspace.\n\n    Example:\n        Models.true_model(g=np.linspace(0.0, 0.5, 100))\n\n    Parameters:\n        g (linspace): The linspace for g desired to calculate the true model. This can be \n            the g_true linspace, g_data linspace, or another linspace of the user's choosing. \n\n    Returns:\n        model (numpy.ndarray): The model calculated at each point in g space. \n    '''\n\n    #define a function for the integrand\n    def function(x,g):\n        return np.exp(-(x**2.0)/2.0 - (g**2.0 * x**4.0)) \n\n    #initialization\n    self.model = np.zeros([len(g)])\n\n    #perform the integral for each g\n    for i in range(len(g)):\n\n        self.model[i], self.err = integrate.quad(function, -np.inf, np.inf, args=(g[i],))\n\n    return self.model \n</code></pre>"},{"location":"models/#samba.models.Uncertainties","title":"<code>Uncertainties</code>","text":"Source code in <code>samba/models.py</code> <pre><code>class Uncertainties:\n\n\n    def __init__(self, error_model='informative'):\n\n        r'''\n        An accompanying class to Models() that possesses the truncation error models\n        that are included as variances with the small-g and large-g expansions. \n\n        Example:\n            Uncertainties()\n\n        Parameters:\n            error_model (str): The name of the error model to use in the calculation. \n                Options are 'uninformative' and 'informative'. Default is 'informative'.\n\n        Returns:\n            None.\n        '''\n\n        #assign error model \n        if error_model == 'uninformative':\n           self.error_model = 1\n\n        elif error_model == 'informative':\n            self.error_model = 2\n\n        else:\n            raise ValueError(\"Please choose 'uninformative' or 'informative'.\")\n\n        return None\n\n\n    def variance_low(self, g, loworder):\n\n\n        r'''\n        A function to calculate the variance corresponding to the small-g expansion model.\n\n        Example:\n            Bivariate.variance_low(g=np.linspace(1e-6, 0.5, 100), loworder=5)\n\n        Parameters:\n            g (numpy.linspace): The linspace over which this calculation is performed.\n\n            loworder (int): The order to which we know our expansion model. Must be \n                passed one at a time if more than one model is to be calculated.\n\n        Returns:\n            var1 (numpy.ndarray): The array of variance values corresponding to each \n                value in the linspace of g. \n        '''\n\n        #even order \n        if loworder % 2 == 0:\n\n            #find coefficients\n            c = np.empty([int(loworder + 2)])\n\n            #model 1 for even orders\n            if self.error_model == 1:\n\n                for k in range(int(loworder + 2)):\n\n                    if k % 2 == 0:\n                        c[k] = np.sqrt(2.0) * special.gamma(k + 0.5) * (-4.0)**(k//2) / (math.factorial(k) * math.factorial(k//2))\n                    else:\n                        c[k] = 0.0\n\n                #rms value\n                cbar = np.sqrt(np.sum((np.asarray(c))**2.0) / (loworder//2 + 1))\n\n                #variance \n                var1 = (cbar)**2.0 * (math.factorial(loworder + 2))**2.0 * g**(2.0*(loworder + 2))\n\n            #model 2 for even orders\n            elif self.error_model == 2:\n\n                for k in range(int(loworder + 2)):\n\n                    if k % 2 == 0:\n\n                        #skip first coefficient\n                        if k == 0:\n                            c[k] = 0.0\n                        else:\n                            c[k] = np.sqrt(2.0) * special.gamma(k + 0.5) * (-4.0)**(k//2) / (math.factorial(k//2) \\\n                                   * math.factorial(k//2 - 1) * 4.0**(k))\n                    else:\n                        c[k] = 0.0\n\n                #rms value\n                cbar = np.sqrt(np.sum((np.asarray(c))**2.0) / (loworder//2 + 1))\n\n                #variance\n                var1 = (cbar)**2.0 * (math.factorial(loworder//2))**2.0 * (4.0 * g)**(2.0*(loworder + 2))\n\n        #odd order\n        else:\n\n            #find coefficients\n            c = np.empty([int(loworder + 1)])\n\n            #model 1 for odd orders\n            if self.error_model == 1:\n\n                for k in range(int(loworder + 1)):\n\n                    if k % 2 == 0:\n                        c[k] = np.sqrt(2.0) * special.gamma(k + 0.5) * (-4.0)**(k//2) / (math.factorial(k) * math.factorial(k//2))\n                    else:\n                        c[k] = 0.0\n\n                #rms value\n                cbar = np.sqrt(np.sum((np.asarray(c))**2.0) / (loworder//2 + 1))\n\n                #variance\n                var1 = (cbar)**2.0 * (math.factorial(loworder + 1))**2.0 * g**(2.0*(loworder + 1))\n\n            #model 2 for odd orders\n            elif self.error_model == 2:\n\n                for k in range(int(loworder + 1)):\n\n                    if k % 2 == 0:\n\n                        #skip first coefficient\n                        if k == 0:\n                            c[k] = 0.0\n                        else:\n                            c[k] = np.sqrt(2.0) * special.gamma(k + 0.5) * (-4.0)**(k//2) / (math.factorial(k//2) \\\n                                    * math.factorial(k//2 - 1) * 4.0**(k))\n                    else:\n                        c[k] = 0.0\n\n                #rms value\n                cbar = np.sqrt(np.sum((np.asarray(c))**2.0) / (loworder//2 + 1))\n\n                #variance\n                var1 = (cbar)**2.0 * (math.factorial((loworder-1)//2))**2.0 * (4.0 * g)**(2.0*(loworder + 1))\n\n        return var1\n\n\n    def variance_high(self, g, highorder):\n\n        r'''\n        A function to calculate the variance corresponding to the large-g expansion model.\n\n        Example:\n            Bivariate.variance_low(g=np.linspace(1e-6, 0.5, 100), highorder=23)\n\n        Parameters:\n            g (numpy.linspace): The linspace over which this calculation is performed.\n\n            highorder (int): The order to which we know our expansion model. This must be \n                a single value.\n\n        Returns:\n            var2 (numpy.ndarray): The array of variance values corresponding to each value \n                in the linspace of g. \n        '''\n\n        #find coefficients\n        d = np.zeros([int(highorder) + 1])\n\n        #model 1\n        if self.error_model == 1:\n\n            for k in range(int(highorder) + 1):\n\n                d[k] = special.gamma(k/2.0 + 0.25) * (-0.5)**k * (math.factorial(k)) / (2.0 * math.factorial(k))\n\n            #rms value (ignore first two coefficients in this model)\n            dbar = np.sqrt(np.sum((np.asarray(d)[2:])**2.0) / (highorder-1))\n\n            #variance\n            var2 = (dbar)**2.0 * (g)**(-1.0) * (math.factorial(highorder + 1))**(-2.0) \\\n                    * g**(-2.0*highorder - 2)\n\n        #model 2\n        elif self.error_model == 2:\n\n            for k in range(int(highorder) + 1):\n\n                d[k] = special.gamma(k/2.0 + 0.25) * special.gamma(k/2.0 + 1.0) * 4.0**(k) \\\n                       * (-0.5)**k / (2.0 * math.factorial(k))\n\n            #rms value\n            dbar = np.sqrt(np.sum((np.asarray(d))**2.0) / (highorder + 1))\n\n            #variance\n            var2 = (dbar)**2.0 * g**(-1.0) * (special.gamma((highorder + 3)/2.0))**(-2.0) \\\n                    * (4.0 * g)**(-2.0*highorder - 2.0)\n\n        return var2\n</code></pre>"},{"location":"models/#samba.models.Uncertainties.__init__","title":"<code>__init__(error_model='informative')</code>","text":"<p>An accompanying class to Models() that possesses the truncation error models that are included as variances with the small-g and large-g expansions. </p> Example <p>Uncertainties()</p> <p>Parameters:</p> Name Type Description Default <code>error_model</code> <code>str</code> <p>The name of the error model to use in the calculation.  Options are 'uninformative' and 'informative'. Default is 'informative'.</p> <code>'informative'</code> <p>Returns:</p> Type Description <p>None.</p> Source code in <code>samba/models.py</code> <pre><code>def __init__(self, error_model='informative'):\n\n    r'''\n    An accompanying class to Models() that possesses the truncation error models\n    that are included as variances with the small-g and large-g expansions. \n\n    Example:\n        Uncertainties()\n\n    Parameters:\n        error_model (str): The name of the error model to use in the calculation. \n            Options are 'uninformative' and 'informative'. Default is 'informative'.\n\n    Returns:\n        None.\n    '''\n\n    #assign error model \n    if error_model == 'uninformative':\n       self.error_model = 1\n\n    elif error_model == 'informative':\n        self.error_model = 2\n\n    else:\n        raise ValueError(\"Please choose 'uninformative' or 'informative'.\")\n\n    return None\n</code></pre>"},{"location":"models/#samba.models.Uncertainties.variance_high","title":"<code>variance_high(g, highorder)</code>","text":"<p>A function to calculate the variance corresponding to the large-g expansion model.</p> Example <p>Bivariate.variance_low(g=np.linspace(1e-6, 0.5, 100), highorder=23)</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>linspace</code> <p>The linspace over which this calculation is performed.</p> required <code>highorder</code> <code>int</code> <p>The order to which we know our expansion model. This must be  a single value.</p> required <p>Returns:</p> Name Type Description <code>var2</code> <code>ndarray</code> <p>The array of variance values corresponding to each value  in the linspace of g.</p> Source code in <code>samba/models.py</code> <pre><code>def variance_high(self, g, highorder):\n\n    r'''\n    A function to calculate the variance corresponding to the large-g expansion model.\n\n    Example:\n        Bivariate.variance_low(g=np.linspace(1e-6, 0.5, 100), highorder=23)\n\n    Parameters:\n        g (numpy.linspace): The linspace over which this calculation is performed.\n\n        highorder (int): The order to which we know our expansion model. This must be \n            a single value.\n\n    Returns:\n        var2 (numpy.ndarray): The array of variance values corresponding to each value \n            in the linspace of g. \n    '''\n\n    #find coefficients\n    d = np.zeros([int(highorder) + 1])\n\n    #model 1\n    if self.error_model == 1:\n\n        for k in range(int(highorder) + 1):\n\n            d[k] = special.gamma(k/2.0 + 0.25) * (-0.5)**k * (math.factorial(k)) / (2.0 * math.factorial(k))\n\n        #rms value (ignore first two coefficients in this model)\n        dbar = np.sqrt(np.sum((np.asarray(d)[2:])**2.0) / (highorder-1))\n\n        #variance\n        var2 = (dbar)**2.0 * (g)**(-1.0) * (math.factorial(highorder + 1))**(-2.0) \\\n                * g**(-2.0*highorder - 2)\n\n    #model 2\n    elif self.error_model == 2:\n\n        for k in range(int(highorder) + 1):\n\n            d[k] = special.gamma(k/2.0 + 0.25) * special.gamma(k/2.0 + 1.0) * 4.0**(k) \\\n                   * (-0.5)**k / (2.0 * math.factorial(k))\n\n        #rms value\n        dbar = np.sqrt(np.sum((np.asarray(d))**2.0) / (highorder + 1))\n\n        #variance\n        var2 = (dbar)**2.0 * g**(-1.0) * (special.gamma((highorder + 3)/2.0))**(-2.0) \\\n                * (4.0 * g)**(-2.0*highorder - 2.0)\n\n    return var2\n</code></pre>"},{"location":"models/#samba.models.Uncertainties.variance_low","title":"<code>variance_low(g, loworder)</code>","text":"<p>A function to calculate the variance corresponding to the small-g expansion model.</p> Example <p>Bivariate.variance_low(g=np.linspace(1e-6, 0.5, 100), loworder=5)</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>linspace</code> <p>The linspace over which this calculation is performed.</p> required <code>loworder</code> <code>int</code> <p>The order to which we know our expansion model. Must be  passed one at a time if more than one model is to be calculated.</p> required <p>Returns:</p> Name Type Description <code>var1</code> <code>ndarray</code> <p>The array of variance values corresponding to each  value in the linspace of g.</p> Source code in <code>samba/models.py</code> <pre><code>def variance_low(self, g, loworder):\n\n\n    r'''\n    A function to calculate the variance corresponding to the small-g expansion model.\n\n    Example:\n        Bivariate.variance_low(g=np.linspace(1e-6, 0.5, 100), loworder=5)\n\n    Parameters:\n        g (numpy.linspace): The linspace over which this calculation is performed.\n\n        loworder (int): The order to which we know our expansion model. Must be \n            passed one at a time if more than one model is to be calculated.\n\n    Returns:\n        var1 (numpy.ndarray): The array of variance values corresponding to each \n            value in the linspace of g. \n    '''\n\n    #even order \n    if loworder % 2 == 0:\n\n        #find coefficients\n        c = np.empty([int(loworder + 2)])\n\n        #model 1 for even orders\n        if self.error_model == 1:\n\n            for k in range(int(loworder + 2)):\n\n                if k % 2 == 0:\n                    c[k] = np.sqrt(2.0) * special.gamma(k + 0.5) * (-4.0)**(k//2) / (math.factorial(k) * math.factorial(k//2))\n                else:\n                    c[k] = 0.0\n\n            #rms value\n            cbar = np.sqrt(np.sum((np.asarray(c))**2.0) / (loworder//2 + 1))\n\n            #variance \n            var1 = (cbar)**2.0 * (math.factorial(loworder + 2))**2.0 * g**(2.0*(loworder + 2))\n\n        #model 2 for even orders\n        elif self.error_model == 2:\n\n            for k in range(int(loworder + 2)):\n\n                if k % 2 == 0:\n\n                    #skip first coefficient\n                    if k == 0:\n                        c[k] = 0.0\n                    else:\n                        c[k] = np.sqrt(2.0) * special.gamma(k + 0.5) * (-4.0)**(k//2) / (math.factorial(k//2) \\\n                               * math.factorial(k//2 - 1) * 4.0**(k))\n                else:\n                    c[k] = 0.0\n\n            #rms value\n            cbar = np.sqrt(np.sum((np.asarray(c))**2.0) / (loworder//2 + 1))\n\n            #variance\n            var1 = (cbar)**2.0 * (math.factorial(loworder//2))**2.0 * (4.0 * g)**(2.0*(loworder + 2))\n\n    #odd order\n    else:\n\n        #find coefficients\n        c = np.empty([int(loworder + 1)])\n\n        #model 1 for odd orders\n        if self.error_model == 1:\n\n            for k in range(int(loworder + 1)):\n\n                if k % 2 == 0:\n                    c[k] = np.sqrt(2.0) * special.gamma(k + 0.5) * (-4.0)**(k//2) / (math.factorial(k) * math.factorial(k//2))\n                else:\n                    c[k] = 0.0\n\n            #rms value\n            cbar = np.sqrt(np.sum((np.asarray(c))**2.0) / (loworder//2 + 1))\n\n            #variance\n            var1 = (cbar)**2.0 * (math.factorial(loworder + 1))**2.0 * g**(2.0*(loworder + 1))\n\n        #model 2 for odd orders\n        elif self.error_model == 2:\n\n            for k in range(int(loworder + 1)):\n\n                if k % 2 == 0:\n\n                    #skip first coefficient\n                    if k == 0:\n                        c[k] = 0.0\n                    else:\n                        c[k] = np.sqrt(2.0) * special.gamma(k + 0.5) * (-4.0)**(k//2) / (math.factorial(k//2) \\\n                                * math.factorial(k//2 - 1) * 4.0**(k))\n                else:\n                    c[k] = 0.0\n\n            #rms value\n            cbar = np.sqrt(np.sum((np.asarray(c))**2.0) / (loworder//2 + 1))\n\n            #variance\n            var1 = (cbar)**2.0 * (math.factorial((loworder-1)//2))**2.0 * (4.0 * g)**(2.0*(loworder + 1))\n\n    return var1\n</code></pre>"},{"location":"priors/","title":"Priors for the linear model mixing hyperparameter estimation","text":"<p>This class contains the priors developed for the hyperparameters of the mixing function for the first method, linear model mixing.  This class can be expanded by the user to employ any other desired priors.</p>"},{"location":"priors/#samba.priors.Priors","title":"<code>Priors</code>","text":"Source code in <code>samba/priors.py</code> <pre><code>class Priors:\n\n    def __init__(self):\n\n        r'''\n        Prior class for the three mixing functions currently\n        in the package -&gt; logistic, cdf, and cosine. These are \n        found in the LMM() class. In future, users can input \n        these functions themselves for their choice of mixing \n        function and data set. \n\n        Example:\n            Priors()\n\n        Parameters:\n            None.\n\n        Returns:\n            None.\n        '''\n        return None \n\n\n    def luniform(self, theta, a, b):\n\n        r'''\n        General uniform prior to be used to truncate the normal\n        distributions used in the parameter priors. \n\n        Parameters:\n            theta (float): The parameter in question.\n\n            a (float): The lower cutoff of the uniform prior.\n\n            b (float): The upper cutoff of the uniform prior.\n\n        Returns:\n            The value of the log uniform prior given the \n                value of the hyperparameter theta.\n        '''\n\n        if theta &gt; a and theta &lt; b:\n            return 0.0\n        else:\n            return -np.inf\n\n\n    def lpdf(self, params):\n\n        r'''\n        Log pdf of the priors for the parameters. Must be truncated\n        for the sampler to walk in valid regions. \n\n        Parameters:\n            params (numpy.ndarray): The hyperparameters of the \n                mixing function that are being estimated.\n\n        Returns:\n            The value of the total log pdf of the priors on the \n                hyperparameters.\n        '''\n\n        if isinstance(params, float) == True:\n            params = np.array([params])\n\n        if len(params) == 1:\n            param_1 = self.luniform(params[0], 0.0, 1.0)\n\n            return param_1\n\n        if len(params) == 2:\n            # param_1 = self.luniform(params[0], -20, 20) #0, 10\n            # param_2 = self.luniform(params[1], -20, 20) #-50, 0\n            param_1 = stats.norm.logpdf(params[0], 10.0, 2.0)\n            param_2 = stats.norm.logpdf(params[1], -20.0, 10.0)\n\n            return (param_1 + param_2)\n\n        elif len(params) == 3:\n            #g1 truncated between (0, 0.35) \n            g1 = self.luniform(params[0], 0.01, 0.3) + stats.norm.logpdf(params[0], 0.1, 0.05)   #0.1 for 2 v 2, #0.1 for 5 v 5\n\n            #g3 truncated between (g1, 0.35)\n            g3 = self.luniform(params[2], params[0], 0.55) + stats.norm.logpdf(params[2], 0.4, 0.05)   #0.4 for 2 v 2, #0.25 for 5 v 5\n\n            #g2 truncated between (g3, 0.35)\n            g2 = self.luniform(params[1], params[2], 0.8) + stats.norm.logpdf(params[1], 0.6, 0.05)  #0.6 for 2 v 2, #0.4 for 5 v 5\n\n            return (g1 + g2 + g3)\n\n        else:\n            raise ValueError('The number of parameters does not match any available switching function.')\n</code></pre>"},{"location":"priors/#samba.priors.Priors.__init__","title":"<code>__init__()</code>","text":"<p>Prior class for the three mixing functions currently in the package -&gt; logistic, cdf, and cosine. These are  found in the LMM() class. In future, users can input  these functions themselves for their choice of mixing  function and data set. </p> Example <p>Priors()</p> <p>Returns:</p> Type Description <p>None.</p> Source code in <code>samba/priors.py</code> <pre><code>def __init__(self):\n\n    r'''\n    Prior class for the three mixing functions currently\n    in the package -&gt; logistic, cdf, and cosine. These are \n    found in the LMM() class. In future, users can input \n    these functions themselves for their choice of mixing \n    function and data set. \n\n    Example:\n        Priors()\n\n    Parameters:\n        None.\n\n    Returns:\n        None.\n    '''\n    return None \n</code></pre>"},{"location":"priors/#samba.priors.Priors.lpdf","title":"<code>lpdf(params)</code>","text":"<p>Log pdf of the priors for the parameters. Must be truncated for the sampler to walk in valid regions. </p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>ndarray</code> <p>The hyperparameters of the  mixing function that are being estimated.</p> required <p>Returns:</p> Type Description <p>The value of the total log pdf of the priors on the  hyperparameters.</p> Source code in <code>samba/priors.py</code> <pre><code>def lpdf(self, params):\n\n    r'''\n    Log pdf of the priors for the parameters. Must be truncated\n    for the sampler to walk in valid regions. \n\n    Parameters:\n        params (numpy.ndarray): The hyperparameters of the \n            mixing function that are being estimated.\n\n    Returns:\n        The value of the total log pdf of the priors on the \n            hyperparameters.\n    '''\n\n    if isinstance(params, float) == True:\n        params = np.array([params])\n\n    if len(params) == 1:\n        param_1 = self.luniform(params[0], 0.0, 1.0)\n\n        return param_1\n\n    if len(params) == 2:\n        # param_1 = self.luniform(params[0], -20, 20) #0, 10\n        # param_2 = self.luniform(params[1], -20, 20) #-50, 0\n        param_1 = stats.norm.logpdf(params[0], 10.0, 2.0)\n        param_2 = stats.norm.logpdf(params[1], -20.0, 10.0)\n\n        return (param_1 + param_2)\n\n    elif len(params) == 3:\n        #g1 truncated between (0, 0.35) \n        g1 = self.luniform(params[0], 0.01, 0.3) + stats.norm.logpdf(params[0], 0.1, 0.05)   #0.1 for 2 v 2, #0.1 for 5 v 5\n\n        #g3 truncated between (g1, 0.35)\n        g3 = self.luniform(params[2], params[0], 0.55) + stats.norm.logpdf(params[2], 0.4, 0.05)   #0.4 for 2 v 2, #0.25 for 5 v 5\n\n        #g2 truncated between (g3, 0.35)\n        g2 = self.luniform(params[1], params[2], 0.8) + stats.norm.logpdf(params[1], 0.6, 0.05)  #0.6 for 2 v 2, #0.4 for 5 v 5\n\n        return (g1 + g2 + g3)\n\n    else:\n        raise ValueError('The number of parameters does not match any available switching function.')\n</code></pre>"},{"location":"priors/#samba.priors.Priors.luniform","title":"<code>luniform(theta, a, b)</code>","text":"<p>General uniform prior to be used to truncate the normal distributions used in the parameter priors. </p> <p>Parameters:</p> Name Type Description Default <code>theta</code> <code>float</code> <p>The parameter in question.</p> required <code>a</code> <code>float</code> <p>The lower cutoff of the uniform prior.</p> required <code>b</code> <code>float</code> <p>The upper cutoff of the uniform prior.</p> required <p>Returns:</p> Type Description <p>The value of the log uniform prior given the  value of the hyperparameter theta.</p> Source code in <code>samba/priors.py</code> <pre><code>def luniform(self, theta, a, b):\n\n    r'''\n    General uniform prior to be used to truncate the normal\n    distributions used in the parameter priors. \n\n    Parameters:\n        theta (float): The parameter in question.\n\n        a (float): The lower cutoff of the uniform prior.\n\n        b (float): The upper cutoff of the uniform prior.\n\n    Returns:\n        The value of the log uniform prior given the \n            value of the hyperparameter theta.\n    '''\n\n    if theta &gt; a and theta &lt; b:\n        return 0.0\n    else:\n        return -np.inf\n</code></pre>"},{"location":"Tutorials/","title":"SAMBA Tutorials","text":"<p>Welcome to the tutorials designed to help you navigate SAMBA! There are three tutorial Jupyter notebooks contained in this folder to guide you through the three different BMM methods in SAMBA: the Linear Mixture Model (LMM.ipynb), Bivariate BMM (Bivariate_BMM.ipynb), and Trivariate BMM with a GP (GP_BMM.ipynb). Each tutorial has benchmark values you can check against to make sure SAMBA is operating properly on your machine. Open any of the notebooks to get started and see how SAMBA employs BMM with the current toy model setup! If you would like to see a walkthrough of the toy model problem before getting to the code, open <code>LMM.ipynb</code> first, where it has been laid out in full. </p> <p>If you want to explore even more BMM methods, check out the other BMM BAND Collaboraton package, Taweret!</p>"},{"location":"Tutorials/Bivariate_BMM/","title":"An introduction to SAMBA: Bivariate Bayesian Model Mixing","text":"<p>From the last notebook, you saw the linear mixture model in action. Now we are going to look at the second method in the paper: Bivariate Bayesian Model Mixing. This is the easiest method to implement in code. We are simply going to take our two series expansions, $F_{s}^{N_{s}}(g)$ and $F_{l}^{N_{l}}(g)$, and mix them according to $$ f^{\\dagger} \\sim \\bigl(Z_P^{-1}\\sum_k \\frac{1}{v_k}f_k, Z_P^{-1}\\bigr), $$</p> <p>where</p> <p>$$ Z_P \\equiv \\sum_{k=1}^{K}\\frac{1}{v_k}. $$</p> <p>$Z_P$ is the precision, or the inverse of the variance, of the $K$ models, $v_{k}$ the individual variances of each model (which we previously denoted the theory error), and $f^{\\dagger}$ the mixed model.</p> <p>We start by loading all of our necessary packages and classes.</p> In\u00a0[2]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\n#import packages\nimport numpy as np\nimport math\nimport statistics\nfrom scipy import stats, special, integrate\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import AutoMinorLocator\n%matplotlib inline\n\n#matplotlib settings for Latex plots\nimport matplotlib\n</pre> %load_ext autoreload %autoreload 2  #import packages import numpy as np import math import statistics from scipy import stats, special, integrate  import matplotlib.pyplot as plt from matplotlib.ticker import AutoMinorLocator %matplotlib inline  #matplotlib settings for Latex plots import matplotlib <pre>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</pre> <p>Next we load the pertinent classes from SAMBA.</p> In\u00a0[4]: Copied! <pre>#load SAMBA classes\nimport sys\nsys.path.append('../../')\n\nfrom samba.models import Models, Uncertainties\nfrom samba.discrepancy import Bivariate\n</pre> #load SAMBA classes import sys sys.path.append('../../')  from samba.models import Models, Uncertainties from samba.discrepancy import Bivariate <p>Let's choose $N_{s} = N_{l} = 2$ again for the first calculation, to compare to the results of the LMM method if desired.</p> In\u00a0[11]: Copied! <pre>#set up linspace and expansions\ng = np.linspace(1e-6,1.0,200)\nns = 2\nnl = 2\n</pre> #set up linspace and expansions g = np.linspace(1e-6,1.0,200) ns = 2 nl = 2 <p>Now we can instantiate an object for the Bivariate method.</p> In\u00a0[12]: Copied! <pre>#instantiate first object for N_s = N_l = 2\nmodel1 = Bivariate(ns, nl, error_model='informative')\n</pre> #instantiate first object for N_s = N_l = 2 model1 = Bivariate(ns, nl, error_model='informative') <p>Let's plot our functions again before we mix them.</p> In\u00a0[13]: Copied! <pre>#call plot_models() from Models() class\nplot_model = model1.plot_models(g)\n</pre> #call plot_models() from Models() class plot_model = model1.plot_models(g) <p>Now we want to mix the two expansions. All we will need is to call the plot_mix() function from the Bivariate() class, as it already calculates the necessary variances internally via the Uncertainties() class, and uses the fdagger() function internally as well.</p> <p>Expected values for mean1 and intervals1 below for $N_{s}=N_{l}=2$:</p> <p>mean1 = [2.50662827 2.50643831 2.50586856 2.50491903 2.50358972 ... 1.57507465 1.57205367 1.56905117 1.56606692 1.56310071]</p> <p>intervals1 = [[2.50662827 2.50662827] [2.50643825 2.50643836] [2.50586769 2.50586943]... [1.55288561 1.58521673] [1.55018531 1.58194852] [1.54749668 1.57870474]]</p> In\u00a0[14]: Copied! <pre>#call plot_mix() function to mix\nmean1, intervals1 = model1.plot_mix(g, plot_fdagger=True)\n</pre> #call plot_mix() function to mix mean1, intervals1 = model1.plot_mix(g, plot_fdagger=True) In\u00a0[15]: Copied! <pre>#check the mean and variance for N_s = N_l = 2 with the original values\nif ns == nl == 2:\n    mean_check = np.array([2.50662827, 2.50643831, 2.50586856, 2.50491903, 2.50358972,\n        2.50188062, 2.49979175, 2.49732309, 2.49447465, 2.49124643,\n        2.48763843, 2.48365064, 2.47928308, 2.47453573, 2.4694086 ,\n        2.4639017 , 2.45801502, 2.45174857, 2.44510236, 2.43807642,\n        2.43067076, 2.42288546, 2.41472057, 2.40617623, 2.39725262,\n        2.38795001, 2.3782688 , 2.36820957, 2.35777312, 2.3469606 ,\n        2.33577358, 2.32421423, 2.3122855 , 2.29999134, 2.28733708,\n        2.27432978, 2.26097875, 2.24729626, 2.23329829, 2.21900568,\n        2.20444538, 2.18965213, 2.17467054, 2.15955764, 2.14438598,\n        2.12924736, 2.11425728, 2.0995599 , 2.08533359, 2.07179644,\n        2.0592113 , 2.04788893, 2.03818779, 2.03050792, 2.02527619,\n        2.02291984, 2.02382653, 2.02829115, 2.03645436, 2.04824287,\n        2.06332654, 2.08110857, 2.10075991, 2.12129795, 2.1416953 ,\n        2.16099444, 2.17840257, 2.19334916, 2.20550125, 2.21474359,\n        2.22113648, 2.22486541, 2.22619265, 2.22541717, 2.22284479,\n        2.21876832, 2.21345579, 2.20714485, 2.20004125, 2.19232   ,\n        2.1841278 , 2.17558633, 2.16679557, 2.15783699, 2.14877658,\n        2.13966736, 2.13055169, 2.12146316, 2.11242816, 2.10346724,\n        2.09459618, 2.08582692, 2.07716827, 2.06862652, 2.06020597,\n        2.05190928, 2.04373783, 2.03569196, 2.02777122, 2.01997453,\n        2.0123003 , 2.00474659, 1.99731117, 1.98999159, 1.98278528,\n        1.97568957, 1.96870172, 1.96181898, 1.95503859, 1.94835782,\n        1.94177397, 1.93528439, 1.92888648, 1.9225777 , 1.9163556 ,\n        1.91021777, 1.90416189, 1.89818571, 1.89228705, 1.88646381,\n        1.88071395, 1.87503552, 1.86942662, 1.86388544, 1.8584102 ,\n        1.85299922, 1.84765086, 1.84236355, 1.83713578, 1.83196608,\n        1.82685304, 1.82179531, 1.81679158, 1.81184059, 1.80694111,\n        1.80209199, 1.79729208, 1.79254029, 1.78783557, 1.78317691,\n        1.77856331, 1.77399384, 1.76946756, 1.7649836 , 1.7605411 ,\n        1.75613922, 1.75177717, 1.74745417, 1.74316946, 1.73892232,\n        1.73471204, 1.73053794, 1.72639935, 1.72229564, 1.71822617,\n        1.71419035, 1.71018758, 1.70621731, 1.70227896, 1.69837201,\n        1.69449593, 1.69065022, 1.68683438, 1.68304792, 1.67929039,\n        1.67556132, 1.67186027, 1.66818682, 1.66454054, 1.66092102,\n        1.65732786, 1.65376068, 1.65021909, 1.64670272, 1.64321121,\n        1.63974422, 1.63630139, 1.63288239, 1.6294869 , 1.62611458,\n        1.62276514, 1.61943825, 1.61613363, 1.61285098, 1.60959002,\n        1.60635046, 1.60313203, 1.59993446, 1.5967575 , 1.59360088,\n        1.59046435, 1.58734767, 1.58425059, 1.58117288, 1.57811431,\n        1.57507465, 1.57205367, 1.56905117, 1.56606692, 1.56310071])\n    var_check = np.array([[2.50662827, 2.50662827],\n        [2.50643825, 2.50643836],\n        [2.50586769, 2.50586943],\n        [2.50491463, 2.50492343],\n        [2.50357583, 2.50360361],\n        [2.50184671, 2.50191454],\n        [2.49972143, 2.49986207],\n        [2.49719282, 2.49745336],\n        [2.49425242, 2.49469688],\n        [2.49089046, 2.4916024 ],\n        [2.48709588, 2.48818097],\n        [2.48285631, 2.48444498],\n        [2.47815807, 2.48040809],\n        [2.47298619, 2.47608527],\n        [2.46732441, 2.4714928 ],\n        [2.46115513, 2.46664827],\n        [2.45445949, 2.46157054],\n        [2.44721732, 2.45627982],\n        [2.43940713, 2.45079759],\n        [2.43100617, 2.44514667],\n        [2.42199037, 2.43935116],\n        [2.4123344 , 2.43343651],\n        [2.40201165, 2.42742949],\n        [2.39099425, 2.42135821],\n        [2.37925308, 2.41525216],\n        [2.36675782, 2.4091422 ],\n        [2.35347698, 2.40306063],\n        [2.33937793, 2.39704121],\n        [2.32442699, 2.39111926],\n        [2.30858951, 2.3853317 ],\n        [2.29182998, 2.37971718],\n        [2.27411223, 2.37431624],\n        [2.25539957, 2.36917142],\n        [2.23565511, 2.36432758],\n        [2.21484208, 2.35983208],\n        [2.19292434, 2.35573521],\n        [2.16986692, 2.35209059],\n        [2.14563685, 2.34895566],\n        [2.12020419, 2.3463924 ],\n        [2.09354332, 2.34446805],\n        [2.06563473, 2.34325603],\n        [2.03646723, 2.34283702],\n        [2.00604089, 2.34330019],\n        [1.97437074, 2.34474454],\n        [1.94149154, 2.34728041],\n        [1.90746376, 2.35103096],\n        [1.87238106, 2.35613349],\n        [1.83637937, 2.36274043],\n        [1.79964771, 2.37101947],\n        [1.76244058, 2.3811523 ],\n        [1.72509147, 2.39333113],\n        [1.68802624, 2.40775163],\n        [1.65177453, 2.42460104],\n        [1.61697608, 2.44403976],\n        [1.58437755, 2.46617483],\n        [1.55481471, 2.49102497],\n        [1.52917477, 2.51847829],\n        [1.50833527, 2.54824703],\n        [1.49308084, 2.57982789],\n        [1.48400579, 2.61247995],\n        [1.48141897, 2.64523411],\n        [1.48527312, 2.67694401],\n        [1.49514019, 2.70637962],\n        [1.51024509, 2.73235081],\n        [1.52955395, 2.75383666],\n        [1.55189735, 2.77009153],\n        [1.57609977, 2.78070538],\n        [1.60108853, 2.78560979],\n        [1.62596591, 2.7850366 ],\n        [1.65004101, 2.77944617],\n        [1.6728279 , 2.76944507],\n        [1.69402175, 2.75570906],\n        [1.71346423, 2.73892106],\n        [1.73110693, 2.7197274 ],\n        [1.74697836, 2.69871123],\n        [1.7611568 , 2.67637985],\n        [1.77374959, 2.65316199],\n        [1.78487826, 2.62941143],\n        [1.79466834, 2.60541417],\n        [1.80324285, 2.58139714],\n        [1.81071841, 2.55753719],\n        [1.81720314, 2.53396953],\n        [1.82279579, 2.51079534],\n        [1.82758564, 2.48808834],\n        [1.83165285, 2.46590031],\n        [1.83506906, 2.44426566],\n        [1.83789814, 2.42320524],\n        [1.84019698, 2.40272934],\n        [1.84201618, 2.38284014],\n        [1.8434008 , 2.36353368],\n        [1.84439101, 2.34480135],\n        [1.84502266, 2.32663118],\n        [1.84532777, 2.30900876],\n        [1.84533507, 2.29191798],\n        [1.8450703 , 2.27534165],\n        [1.84455664, 2.25926193],\n        [1.84381498, 2.24366067],\n        [1.84286419, 2.22851972],\n        [1.84172137, 2.21382107],\n        [1.84040202, 2.19954703],\n        [1.83892024, 2.18568036],\n        [1.83728889, 2.17220429],\n        [1.8355197 , 2.15910263],\n        [1.83362341, 2.14635977],\n        [1.83160985, 2.13396072],\n        [1.82948805, 2.12189108],\n        [1.82726634, 2.1101371 ],\n        [1.82495235, 2.0986856 ],\n        [1.82255315, 2.08752403],\n        [1.82007526, 2.07664038],\n        [1.81752472, 2.06602322],\n        [1.81490711, 2.05566167],\n        [1.81222762, 2.04554534],\n        [1.80949105, 2.03566435],\n        [1.80670189, 2.02600931],\n        [1.80386429, 2.01657125],\n        [1.80098212, 2.00734165],\n        [1.79805901, 1.99831241],\n        [1.79509831, 1.98947579],\n        [1.79210318, 1.98082444],\n        [1.78907655, 1.97235136],\n        [1.78602117, 1.96404988],\n        [1.7829396 , 1.95591364],\n        [1.77983426, 1.94793661],\n        [1.7767074 , 1.94011299],\n        [1.77356113, 1.93243731],\n        [1.77039742, 1.9249043 ],\n        [1.76721814, 1.91750897],\n        [1.76402501, 1.91024655],\n        [1.76081969, 1.90311247],\n        [1.7576037 , 1.89610239],\n        [1.75437848, 1.88921215],\n        [1.75114538, 1.88243778],\n        [1.74790569, 1.87577548],\n        [1.74466059, 1.86922164],\n        [1.7414112 , 1.86277277],\n        [1.73815859, 1.85642556],\n        [1.73490375, 1.85017683],\n        [1.73164761, 1.84402354],\n        [1.72839105, 1.83796277],\n        [1.72513489, 1.83199174],\n        [1.72187992, 1.82610776],\n        [1.71862686, 1.82030827],\n        [1.71537639, 1.81459081],\n        [1.71212918, 1.80895302],\n        [1.70888581, 1.80339263],\n        [1.70564687, 1.79790747],\n        [1.70241289, 1.79249544],\n        [1.69918438, 1.78715454],\n        [1.6959618 , 1.78188284],\n        [1.6927456 , 1.77667848],\n        [1.68953619, 1.77153968],\n        [1.68633398, 1.76646471],\n        [1.68313933, 1.76145194],\n        [1.67995258, 1.75649976],\n        [1.67677405, 1.75160665],\n        [1.67360404, 1.74677113],\n        [1.67044285, 1.74199177],\n        [1.66729072, 1.7372672 ],\n        [1.66414791, 1.73259611],\n        [1.66101465, 1.72797721],\n        [1.65789116, 1.72340928],\n        [1.65477763, 1.71889113],\n        [1.65167424, 1.7144216 ],\n        [1.64858118, 1.70999959],\n        [1.6454986 , 1.70562404],\n        [1.64242664, 1.7012939 ],\n        [1.63936546, 1.69700818],\n        [1.63631517, 1.69276591],\n        [1.63327588, 1.68856616],\n        [1.63024772, 1.68440801],\n        [1.62723076, 1.68029059],\n        [1.62422511, 1.67621306],\n        [1.62123085, 1.67217459],\n        [1.61824803, 1.66817439],\n        [1.61527675, 1.66421169],\n        [1.61231704, 1.66028574],\n        [1.60936897, 1.65639582],\n        [1.60643258, 1.65254122],\n        [1.60350791, 1.64872126],\n        [1.60059499, 1.64493528],\n        [1.59769386, 1.64118265],\n        [1.59480453, 1.63746273],\n        [1.59192704, 1.63377493],\n        [1.58906139, 1.63011865],\n        [1.58620759, 1.62649333],\n        [1.58336565, 1.62289841],\n        [1.58053558, 1.61933335],\n        [1.57771737, 1.61579763],\n        [1.57491101, 1.61229074],\n        [1.57211651, 1.60881218],\n        [1.56933385, 1.60536148],\n        [1.56656302, 1.60193816],\n        [1.563804  , 1.59854176],\n        [1.56105677, 1.59517185],\n        [1.55832131, 1.59182799],\n        [1.5555976 , 1.58850975],\n        [1.55288561, 1.58521673],\n        [1.55018531, 1.58194852],\n        [1.54749668, 1.57870474]])\n    \n    mean_diff = np.abs(mean1 - mean_check)\n    var_diff = np.abs(intervals1 - var_check)\n    \n    #set tolerance\n    abs_tol = 1e-8\n\n    #check all elements of the arrays\n    for i in range(len(mean_diff)):\n\n        #check mean\n        if mean_diff[i] &lt; abs_tol:\n            mean_ans = 'True'\n        else:\n            mean_ans = 'False'\n\n        #check intervals\n        if var_diff[i,0] &lt; abs_tol:\n            var_ans_1 = 'True'\n        else: \n            var_ans_1 = 'False'\n        if var_diff[i, 1] &lt; abs_tol:\n            var_ans_2 = 'True'\n        else: \n            var_ans_2 = 'False'\n        if var_ans_1 == 'True' and var_ans_2 == 'True':\n            var_ans = 'True'\n        elif var_ans_1 != 'True' or var_ans_2 != 'True':\n            var_ans = 'False'\n        \n    #print test results\n    print('Mean passing?', mean_ans)\n    print('Intervals passing?', var_ans)\n    \nelse:\n    pass\n</pre> #check the mean and variance for N_s = N_l = 2 with the original values if ns == nl == 2:     mean_check = np.array([2.50662827, 2.50643831, 2.50586856, 2.50491903, 2.50358972,         2.50188062, 2.49979175, 2.49732309, 2.49447465, 2.49124643,         2.48763843, 2.48365064, 2.47928308, 2.47453573, 2.4694086 ,         2.4639017 , 2.45801502, 2.45174857, 2.44510236, 2.43807642,         2.43067076, 2.42288546, 2.41472057, 2.40617623, 2.39725262,         2.38795001, 2.3782688 , 2.36820957, 2.35777312, 2.3469606 ,         2.33577358, 2.32421423, 2.3122855 , 2.29999134, 2.28733708,         2.27432978, 2.26097875, 2.24729626, 2.23329829, 2.21900568,         2.20444538, 2.18965213, 2.17467054, 2.15955764, 2.14438598,         2.12924736, 2.11425728, 2.0995599 , 2.08533359, 2.07179644,         2.0592113 , 2.04788893, 2.03818779, 2.03050792, 2.02527619,         2.02291984, 2.02382653, 2.02829115, 2.03645436, 2.04824287,         2.06332654, 2.08110857, 2.10075991, 2.12129795, 2.1416953 ,         2.16099444, 2.17840257, 2.19334916, 2.20550125, 2.21474359,         2.22113648, 2.22486541, 2.22619265, 2.22541717, 2.22284479,         2.21876832, 2.21345579, 2.20714485, 2.20004125, 2.19232   ,         2.1841278 , 2.17558633, 2.16679557, 2.15783699, 2.14877658,         2.13966736, 2.13055169, 2.12146316, 2.11242816, 2.10346724,         2.09459618, 2.08582692, 2.07716827, 2.06862652, 2.06020597,         2.05190928, 2.04373783, 2.03569196, 2.02777122, 2.01997453,         2.0123003 , 2.00474659, 1.99731117, 1.98999159, 1.98278528,         1.97568957, 1.96870172, 1.96181898, 1.95503859, 1.94835782,         1.94177397, 1.93528439, 1.92888648, 1.9225777 , 1.9163556 ,         1.91021777, 1.90416189, 1.89818571, 1.89228705, 1.88646381,         1.88071395, 1.87503552, 1.86942662, 1.86388544, 1.8584102 ,         1.85299922, 1.84765086, 1.84236355, 1.83713578, 1.83196608,         1.82685304, 1.82179531, 1.81679158, 1.81184059, 1.80694111,         1.80209199, 1.79729208, 1.79254029, 1.78783557, 1.78317691,         1.77856331, 1.77399384, 1.76946756, 1.7649836 , 1.7605411 ,         1.75613922, 1.75177717, 1.74745417, 1.74316946, 1.73892232,         1.73471204, 1.73053794, 1.72639935, 1.72229564, 1.71822617,         1.71419035, 1.71018758, 1.70621731, 1.70227896, 1.69837201,         1.69449593, 1.69065022, 1.68683438, 1.68304792, 1.67929039,         1.67556132, 1.67186027, 1.66818682, 1.66454054, 1.66092102,         1.65732786, 1.65376068, 1.65021909, 1.64670272, 1.64321121,         1.63974422, 1.63630139, 1.63288239, 1.6294869 , 1.62611458,         1.62276514, 1.61943825, 1.61613363, 1.61285098, 1.60959002,         1.60635046, 1.60313203, 1.59993446, 1.5967575 , 1.59360088,         1.59046435, 1.58734767, 1.58425059, 1.58117288, 1.57811431,         1.57507465, 1.57205367, 1.56905117, 1.56606692, 1.56310071])     var_check = np.array([[2.50662827, 2.50662827],         [2.50643825, 2.50643836],         [2.50586769, 2.50586943],         [2.50491463, 2.50492343],         [2.50357583, 2.50360361],         [2.50184671, 2.50191454],         [2.49972143, 2.49986207],         [2.49719282, 2.49745336],         [2.49425242, 2.49469688],         [2.49089046, 2.4916024 ],         [2.48709588, 2.48818097],         [2.48285631, 2.48444498],         [2.47815807, 2.48040809],         [2.47298619, 2.47608527],         [2.46732441, 2.4714928 ],         [2.46115513, 2.46664827],         [2.45445949, 2.46157054],         [2.44721732, 2.45627982],         [2.43940713, 2.45079759],         [2.43100617, 2.44514667],         [2.42199037, 2.43935116],         [2.4123344 , 2.43343651],         [2.40201165, 2.42742949],         [2.39099425, 2.42135821],         [2.37925308, 2.41525216],         [2.36675782, 2.4091422 ],         [2.35347698, 2.40306063],         [2.33937793, 2.39704121],         [2.32442699, 2.39111926],         [2.30858951, 2.3853317 ],         [2.29182998, 2.37971718],         [2.27411223, 2.37431624],         [2.25539957, 2.36917142],         [2.23565511, 2.36432758],         [2.21484208, 2.35983208],         [2.19292434, 2.35573521],         [2.16986692, 2.35209059],         [2.14563685, 2.34895566],         [2.12020419, 2.3463924 ],         [2.09354332, 2.34446805],         [2.06563473, 2.34325603],         [2.03646723, 2.34283702],         [2.00604089, 2.34330019],         [1.97437074, 2.34474454],         [1.94149154, 2.34728041],         [1.90746376, 2.35103096],         [1.87238106, 2.35613349],         [1.83637937, 2.36274043],         [1.79964771, 2.37101947],         [1.76244058, 2.3811523 ],         [1.72509147, 2.39333113],         [1.68802624, 2.40775163],         [1.65177453, 2.42460104],         [1.61697608, 2.44403976],         [1.58437755, 2.46617483],         [1.55481471, 2.49102497],         [1.52917477, 2.51847829],         [1.50833527, 2.54824703],         [1.49308084, 2.57982789],         [1.48400579, 2.61247995],         [1.48141897, 2.64523411],         [1.48527312, 2.67694401],         [1.49514019, 2.70637962],         [1.51024509, 2.73235081],         [1.52955395, 2.75383666],         [1.55189735, 2.77009153],         [1.57609977, 2.78070538],         [1.60108853, 2.78560979],         [1.62596591, 2.7850366 ],         [1.65004101, 2.77944617],         [1.6728279 , 2.76944507],         [1.69402175, 2.75570906],         [1.71346423, 2.73892106],         [1.73110693, 2.7197274 ],         [1.74697836, 2.69871123],         [1.7611568 , 2.67637985],         [1.77374959, 2.65316199],         [1.78487826, 2.62941143],         [1.79466834, 2.60541417],         [1.80324285, 2.58139714],         [1.81071841, 2.55753719],         [1.81720314, 2.53396953],         [1.82279579, 2.51079534],         [1.82758564, 2.48808834],         [1.83165285, 2.46590031],         [1.83506906, 2.44426566],         [1.83789814, 2.42320524],         [1.84019698, 2.40272934],         [1.84201618, 2.38284014],         [1.8434008 , 2.36353368],         [1.84439101, 2.34480135],         [1.84502266, 2.32663118],         [1.84532777, 2.30900876],         [1.84533507, 2.29191798],         [1.8450703 , 2.27534165],         [1.84455664, 2.25926193],         [1.84381498, 2.24366067],         [1.84286419, 2.22851972],         [1.84172137, 2.21382107],         [1.84040202, 2.19954703],         [1.83892024, 2.18568036],         [1.83728889, 2.17220429],         [1.8355197 , 2.15910263],         [1.83362341, 2.14635977],         [1.83160985, 2.13396072],         [1.82948805, 2.12189108],         [1.82726634, 2.1101371 ],         [1.82495235, 2.0986856 ],         [1.82255315, 2.08752403],         [1.82007526, 2.07664038],         [1.81752472, 2.06602322],         [1.81490711, 2.05566167],         [1.81222762, 2.04554534],         [1.80949105, 2.03566435],         [1.80670189, 2.02600931],         [1.80386429, 2.01657125],         [1.80098212, 2.00734165],         [1.79805901, 1.99831241],         [1.79509831, 1.98947579],         [1.79210318, 1.98082444],         [1.78907655, 1.97235136],         [1.78602117, 1.96404988],         [1.7829396 , 1.95591364],         [1.77983426, 1.94793661],         [1.7767074 , 1.94011299],         [1.77356113, 1.93243731],         [1.77039742, 1.9249043 ],         [1.76721814, 1.91750897],         [1.76402501, 1.91024655],         [1.76081969, 1.90311247],         [1.7576037 , 1.89610239],         [1.75437848, 1.88921215],         [1.75114538, 1.88243778],         [1.74790569, 1.87577548],         [1.74466059, 1.86922164],         [1.7414112 , 1.86277277],         [1.73815859, 1.85642556],         [1.73490375, 1.85017683],         [1.73164761, 1.84402354],         [1.72839105, 1.83796277],         [1.72513489, 1.83199174],         [1.72187992, 1.82610776],         [1.71862686, 1.82030827],         [1.71537639, 1.81459081],         [1.71212918, 1.80895302],         [1.70888581, 1.80339263],         [1.70564687, 1.79790747],         [1.70241289, 1.79249544],         [1.69918438, 1.78715454],         [1.6959618 , 1.78188284],         [1.6927456 , 1.77667848],         [1.68953619, 1.77153968],         [1.68633398, 1.76646471],         [1.68313933, 1.76145194],         [1.67995258, 1.75649976],         [1.67677405, 1.75160665],         [1.67360404, 1.74677113],         [1.67044285, 1.74199177],         [1.66729072, 1.7372672 ],         [1.66414791, 1.73259611],         [1.66101465, 1.72797721],         [1.65789116, 1.72340928],         [1.65477763, 1.71889113],         [1.65167424, 1.7144216 ],         [1.64858118, 1.70999959],         [1.6454986 , 1.70562404],         [1.64242664, 1.7012939 ],         [1.63936546, 1.69700818],         [1.63631517, 1.69276591],         [1.63327588, 1.68856616],         [1.63024772, 1.68440801],         [1.62723076, 1.68029059],         [1.62422511, 1.67621306],         [1.62123085, 1.67217459],         [1.61824803, 1.66817439],         [1.61527675, 1.66421169],         [1.61231704, 1.66028574],         [1.60936897, 1.65639582],         [1.60643258, 1.65254122],         [1.60350791, 1.64872126],         [1.60059499, 1.64493528],         [1.59769386, 1.64118265],         [1.59480453, 1.63746273],         [1.59192704, 1.63377493],         [1.58906139, 1.63011865],         [1.58620759, 1.62649333],         [1.58336565, 1.62289841],         [1.58053558, 1.61933335],         [1.57771737, 1.61579763],         [1.57491101, 1.61229074],         [1.57211651, 1.60881218],         [1.56933385, 1.60536148],         [1.56656302, 1.60193816],         [1.563804  , 1.59854176],         [1.56105677, 1.59517185],         [1.55832131, 1.59182799],         [1.5555976 , 1.58850975],         [1.55288561, 1.58521673],         [1.55018531, 1.58194852],         [1.54749668, 1.57870474]])          mean_diff = np.abs(mean1 - mean_check)     var_diff = np.abs(intervals1 - var_check)          #set tolerance     abs_tol = 1e-8      #check all elements of the arrays     for i in range(len(mean_diff)):          #check mean         if mean_diff[i] &lt; abs_tol:             mean_ans = 'True'         else:             mean_ans = 'False'          #check intervals         if var_diff[i,0] &lt; abs_tol:             var_ans_1 = 'True'         else:              var_ans_1 = 'False'         if var_diff[i, 1] &lt; abs_tol:             var_ans_2 = 'True'         else:              var_ans_2 = 'False'         if var_ans_1 == 'True' and var_ans_2 == 'True':             var_ans = 'True'         elif var_ans_1 != 'True' or var_ans_2 != 'True':             var_ans = 'False'              #print test results     print('Mean passing?', mean_ans)     print('Intervals passing?', var_ans)      else:     pass <pre>Mean passing? True\nIntervals passing? True\n</pre> <p>And, as simple as that, there is the mixed model! The green curve is the PPD from the $f^{\\dagger}$ equation at the top of this notebook. The credibility interval, in shaded green, is quite large, as was discussed at length in the paper in Section IV.</p> <p>In order to fix this (in our toy case) overly conservative band is to implement a GP in the center to take care of some of the interpolation there, where we do not have any prior information other than the variances of the series expansions (shown in dotted red and blue above). That will be the subject of the next notebook, but first let's also look at another case: $N_{s} = N_{l} = 5$.</p> <p>We begin by instantiating another object for this new choice of models. We again select 'i' for the informative error model when the question is asked.</p> In\u00a0[16]: Copied! <pre>#instantiate a new object for N_s = N_l = 5\nns = 5\nnl = 5\nmodel2 = Bivariate(ns, nl, error_model='informative')\n</pre> #instantiate a new object for N_s = N_l = 5 ns = 5 nl = 5 model2 = Bivariate(ns, nl, error_model='informative') <p>Now we can simply call plot_mix() again to plot the results of mixing these two expansions.</p> <p>Expected values for mean2 and intervals2 below for $N_{s}=N_{l}=5$:</p> <p>mean2 = [2.50662827 2.50643839 2.5058699  2.50492583 2.5036112...1.56617848 1.56330958 1.56045582 1.55761707 1.55479321]</p> <p>intervals2 = [[2.50662827 2.50662827] [2.50643839 2.50643839] [2.5058699  2.50586991] ... [1.56040879 1.56050285] [1.55757156 1.55766258] [1.55474917 1.55483725]]</p> In\u00a0[17]: Copied! <pre>#call plot_mix()\nmean2, intervals2 = model2.plot_mix(g, plot_fdagger=True)\n</pre> #call plot_mix() mean2, intervals2 = model2.plot_mix(g, plot_fdagger=True) <p>This mixed model result is quite similar to the one for $N_{s} = N_{l} = 2$, but with a smaller gap, so a smaller uncertainty band overall. Let's look next at a comparison between the two theory error models developed in the paper.</p> <p>In the paper, we discuss (in Section IIB) that there are multiple different ways of quantifying uncertainty on these series expansions, two of which we have coded and at our disposal now. The one we used above is dependent on which letter you typed for the error model to be chosen: u (uninformative) or i (informative). These error models have slightly different shapes when it comes to their uncertainty bands, as will be demonstrated below.</p> <p>First we instantiate another object from the Bivariate method for $N_{s} = N_{l} = 3$, as this case makes the differences quite clear between the uninformative and informative error models. We type 'u' or 'i' for this first instance, as it does not matter for our comparison plot, and is only setting the error model for the rest of the object (which we will not be using anyway).</p> In\u00a0[18]: Copied! <pre>#instantiate a third object for N_s = N_l = 3\nns = 3\nnl = 3\nmodel3 = Bivariate(ns, nl, error_model='informative')\n</pre> #instantiate a third object for N_s = N_l = 3 ns = 3 nl = 3 model3 = Bivariate(ns, nl, error_model='informative') <p>We begin (and end, as everything is done in one function) by calling the plot_error_models() function from the Bivariate() class, and type 'u' for the first panel, and 'i' for the second panel.</p> In\u00a0[19]: Copied! <pre>#plot the uninformative and informative error models\nmodel3.plot_error_models(g)\n</pre> #plot the uninformative and informative error models model3.plot_error_models(g) <p>It is not too obvious upon first glance, but it can be seen that panel (a) possesses a larger uncertainty band than panel (b) does. This is the subtle difference in the error models coming into play---panel (b) is using an error model that knows more about the next term in the series than panel (a)'s model does, so the uncertainty in the gap in panel (b) is a little smaller. However, they're both still pretty large for our toy case. Now we should try the GP and see what kind of help it provides---see the next notebook (GP_BMM) for details on how to include a Gaussian Process in this mixed model.</p> <p>Written by: Alexandra Semposki (06 June 2022)</p>"},{"location":"Tutorials/Bivariate_BMM/#an-introduction-to-samba-bivariate-bayesian-model-mixing","title":"An introduction to SAMBA: Bivariate Bayesian Model Mixing\u00b6","text":"<p>Alexandra Semposki</p> <p>Date: 06 June 2022</p>"},{"location":"Tutorials/Bivariate_BMM/#introduction","title":"Introduction\u00b6","text":""},{"location":"Tutorials/Bivariate_BMM/#bivariate-model-mixing-ppd","title":"Bivariate model mixing: PPD\u00b6","text":""},{"location":"Tutorials/Bivariate_BMM/#error-model-comparison","title":"Error Model Comparison\u00b6","text":""},{"location":"Tutorials/GP_BMM/","title":"An introduction to SAMBA: Trivariate Bayesian Model Mixing w/GPs","text":"<p>From the last notebook, you saw the bivariate model mixing produce a well-mixed model with an overly conservative uncertainty band in the gap (for our toy model). We now want to try mixing in an interpolant in the form of a Gaussian Process (GP). This will enter $f^{\\dagger}$ as a third individual model. Recall the function for $f^\\dagger$:</p> <p>$$ f^{\\dagger} \\sim \\bigl(Z_P^{-1}\\sum_k \\frac{1}{v_k}f_k, Z_P^{-1}\\bigr), $$</p> <p>where</p> <p>$$ Z_P \\equiv \\sum_{k=1}^{K}\\frac{1}{v_k}. $$</p> <p>$Z_P$ is the precision, or the inverse of the variance, of the $K$ models, $v_{k}$ the individual variances of each model (which we previously denoted the theory error), and $f^{\\dagger}$ the mixed model. Now $K = 3$ for this trivariate case.</p> <p>We start by loading all of our necessary packages and SAMBA classes.</p> In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\n#import packages\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom matplotlib.ticker import AutoMinorLocator\n%matplotlib inline\n\n#matplotlib settings for Latex plots\nimport matplotlib\n</pre> %load_ext autoreload %autoreload 2  #import packages import numpy as np import matplotlib.pyplot as plt from scipy import stats from matplotlib.ticker import AutoMinorLocator %matplotlib inline  #matplotlib settings for Latex plots import matplotlib In\u00a0[2]: Copied! <pre>#import the SAMBA classes needed\nimport sys\nsys.path.append('../../')\n\nfrom samba.models import Models, Uncertainties\nfrom samba.discrepancy import Bivariate\nfrom samba.gaussprocess import GP\nfrom samba.fprdat import FPR\n</pre> #import the SAMBA classes needed import sys sys.path.append('../../')  from samba.models import Models, Uncertainties from samba.discrepancy import Bivariate from samba.gaussprocess import GP from samba.fprdat import FPR <p>As usual, we start by defining two series expansions to mix. We will begin with $N_{s} = N_{l} = 3$. (Note that we could mix more than two series expansions, but we're sticking with the bare minimum in this tutorial.)</p> In\u00a0[16]: Copied! <pre>#define g and series expansions\ng = np.linspace(1e-6,1.0,100)\nns = 3\nnl = 3\n</pre> #define g and series expansions g = np.linspace(1e-6,1.0,100) ns = 3 nl = 3 In\u00a0[17]: Copied! <pre># run something simple for array to check and save in a txt file\nobj = Models(ns,nl)\nobjvar = Uncertainties(error_model='informative')\nlow_val = obj.low_g(g)[0]\nhigh_val = obj.high_g(g)[0]\nlow_std = objvar.variance_low(g, ns)\nhigh_std = objvar.variance_high(g, nl)\n\nmodel1 = Bivariate(ns, nl, error_model='informative')\nmean1, intervals1 = model1.plot_mix(g, plot_fdagger=True)\nstd_dev = intervals1[:,1] - mean1\n\n# obtain the weights and then store them\nweights = model1.var_weights\nweights_low = weights[0,:]\nweights_high = weights[1,:]\n\n# concatenate them / list format arrays\nsamba_values = np.array([low_val, high_val, low_std, high_std, mean1, intervals1[:,0], intervals1[:,1], std_dev, weights_low, weights_high])\nsamba_arrays = np.savetxt('samba_results.txt', samba_values, delimiter=',')\n</pre> # run something simple for array to check and save in a txt file obj = Models(ns,nl) objvar = Uncertainties(error_model='informative') low_val = obj.low_g(g)[0] high_val = obj.high_g(g)[0] low_std = objvar.variance_low(g, ns) high_std = objvar.variance_high(g, nl)  model1 = Bivariate(ns, nl, error_model='informative') mean1, intervals1 = model1.plot_mix(g, plot_fdagger=True) std_dev = intervals1[:,1] - mean1  # obtain the weights and then store them weights = model1.var_weights weights_low = weights[0,:] weights_high = weights[1,:]  # concatenate them / list format arrays samba_values = np.array([low_val, high_val, low_std, high_std, mean1, intervals1[:,0], intervals1[:,1], std_dev, weights_low, weights_high]) samba_arrays = np.savetxt('samba_results.txt', samba_values, delimiter=',') <p>Now we instantiate the GP class, which acts as a wrapper for the Python package scikit learn. We immediately pick the kernel we wish to use for the GP here. The options are RBF, Matern, and Rational Quadratic. If we want to pick Matern, we also supply the value of $\\nu$ when asked. This is shown below.</p> In\u00a0[18]: Copied! <pre>#instantiate GP object for N_s = N_l = 3\nobj1 = GP(g, ns, nl, kernel=\"Matern\", nu=1.5, ci=68, error_model='informative')\n</pre> #instantiate GP object for N_s = N_l = 3 obj1 = GP(g, ns, nl, kernel=\"Matern\", nu=1.5, ci=68, error_model='informative') <p>Now we have a GP object that will use the Matern 3/2 kernel for its analysis. When using a GP, we need to train on a set of data. In a typical situation where the user has data to give the GP, these would be the training points used. However, in our case, with a toy model, we must generate these training points. In SAMBA, the training points are chosen by offsetting the original linspace in $g$ by a small amount, so a completely new linspace in $g$ is created that does not overlap exactly with any of the points first given to the GP class. This is because those points will be used in both the series expansion calculation and the GP prediction---which means, if we want the series expansions and GP to mix correctly in $f^\\dagger$, we need to have the points in $g$ for all three models to be the same, or we will not be mixing the right points. Hence, we create a training set after a prediction set for the toy model, unlike in real situations.</p> <p>SAMBA takes care of generating this new training array for you, but it does request that you choose a method to determine the fourth and final training point, located within the region of the large-$g$ expansion. This is because there are many different ways we could assign that training point. The three methods that we tried are</p> <ul> <li><p>Fixing the point at $g = 0.6$ (independent of the location of the gap)</p> </li> <li><p>Choosing the point in the array computed for the large-$g$ expansion where the value of $F_s(g)$ cuts off (where we choose to cut off the calculation of the small-$g$ expansion at the edge of the pertinent domain in $F(g)$)</p> </li> <li><p>Putting the training point at the location where the theory error on the current point in the large-$g$ expansion array is larger than 5% of the next value of $F_l(g)$ in the array</p> </li> </ul> <p>SAMBA places the other 3 points at fixed points: two in the small-$g$ expansion array, and one at $g = 1$ in the large-$g$ expansion.</p> <p>In our tutorial, we pick method 2, which tends to yield the best results when comparing the true curve to the mixed model. However, none of the 3 methods are spectacularly better than the others, which shows how the GP is not overly dependent on the placement of the training points (great news for generalizing this!). We also select 'error=True' in the arguments of the training() function, because we wish to use the theory errors in our GP.</p> <p>Expected Gaussian parameters for method 2 with Matern 3/2 kernel and $N_{s}=N_{l}=3$ below:</p> <p>length_scale = 2.08</p> <p>variance = 2.27</p> In\u00a0[19]: Copied! <pre>#call the training() function from the GP() class\nobj_tr1 = obj1.training(error=True, method=2, plot=True)\n</pre> #call the training() function from the GP() class obj_tr1 = obj1.training(error=True, method=2, plot=True) <pre>Gaussian process parameters: 2.3**2 * Matern(length_scale=2.14, nu=1.5)\n</pre> <p>The plot above shows the theory errors on each calculated training point, with the red and blue curves being the small-$g$ and large-$g$ expansions, as usual. The black points are the four chosen training points for our GP that it just used to train itself. We will now use the validate() function in the GP class to predict at each original point in $g$.</p> <p>Expected mean1, sig1, cov1 for method 2, Matern 3/2 kernel and $N_{s}=N_{l}=3$ below:</p> <p>mean1 = [2.55678727 2.550882   2.54491822 2.53889552 2.53281353 ...1.56303294 1.56056141 1.55811837 1.55570382 1.55331773]</p> <p>sig1 = [0.0193262  0.01764195 0.01599236 0.01437896 0.01280341...0.00835179 0.00663118 0.00485759 0.00305163 0.00135394]</p> <p>cov1 = [[3.73502044e-04 3.40524975e-04 3.07502834e-04 ... 4.08375077e-07 2.44743834e-07 8.20266537e-08] [3.40524975e-04 3.11238518e-04 2.81740028e-04 ... 3.80396282e-07 2.27975826e-07 7.64068068e-08] ... [2.44743834e-07 2.27975826e-07 2.11062770e-07 ... 1.46638192e-05 9.31247488e-06 3.69959419e-06] [8.20266537e-08 7.64068068e-08 7.07383467e-08 ... 5.46592671e-06 3.69959419e-06 1.83315966e-06]]</p> In\u00a0[20]: Copied! <pre>#call the validate() function\nmean1, sig1, cov1 = obj1.validate(plot=True)\n</pre> #call the validate() function mean1, sig1, cov1 = obj1.validate(plot=True) <p>We can now see the shaded green region and green curve in the plot above, which correspond to the GP variance band and GP mean curve, respectively.</p> <p>Expected values of mixed_mean, mixed_intervals for Matern 3/2 kernel, method 2, with $N_{s}=N_{l}=3$ below:</p> <p>mixed_mean = [2.50662827 2.50643831 2.50586856 2.50491903 2.50358975 ... 1.56463932 1.56177765 1.55892173 1.55608208 1.55336891]</p> <p>mixed_intervals = [[2.50662827 2.50662827] [2.50643825 2.50643836] [2.50586769 2.50586943] ... [1.55670377 1.56113969] [1.55417787 1.55798629] [1.5521918  1.55454602]]</p> In\u00a0[21]: Copied! <pre>#call plot_mix() to mix in the GP\nmixed_mean, mixed_intervals = obj1.plot_mix(g, plot_fdagger=True, plot_true=True, GP_mean=mean1, GP_var=np.square(sig1))\n</pre> #call plot_mix() to mix in the GP mixed_mean, mixed_intervals = obj1.plot_mix(g, plot_fdagger=True, plot_true=True, GP_mean=mean1, GP_var=np.square(sig1)) <p>This result looks WAY better in the gap than the bivariate BMM did alone! Now let's directly compare to that case using our comparison function subplot_mix(), also found in the Bivariate() class.</p> In\u00a0[22]: Copied! <pre>#call subplot_mix() and plot no GP results next to GP results\nobj1.subplot_mix(g, GP_mean=mean1, GP_var=np.square(sig1))\n</pre> #call subplot_mix() and plot no GP results next to GP results obj1.subplot_mix(g, GP_mean=mean1, GP_var=np.square(sig1)) <p>The GP shows an obvious improvement in the gap in panel (b), as it follows the true curve quite well. The uncertainty band is also more believable, as it encompasses most of the true curve and does not allow for the possibility of any surprising result (like the dip seen in panel (a), which might be possible for a physical situation but not in a toy case where we know exactly what we are expecting to get).</p> <p>Let's check our results and see if we can truly trust them. One good way to do this is our diagnostic, the Mahalanobis distance. This is given by</p> <p>$$ \\mathrm{D}^{2}_{\\mathrm{MD}} = (y - m)^{T}\\textit{K}^{-1}(y - m), $$</p> <p>where $y$ is the vector of true solutions at each point in $g$ that we are comparing to (our true curve), $m$ is the GP solution at each point in $g$, and $K^{-1}$ is the inverse of the GP covariance matrix, returned by our validate() function above.</p> <p>Let's try this calculation to see what we get for the Mahalanobis distance. We will use 3 points to calculate it, as our lengthscale is quite long, and we can only put a few points in it without $K$ becoming a singular matrix. We will also choose 1000 draws from our GP to compare our Mahalanobis distance to a reference distribution.</p> <p>Expected values for md_g, md_mean, md_sig, and md_cov for pts=3, $N_{s}=N_{l}=3$, Matern 3/2 kernel, method 2 below:</p> <p>md_g = [0.29648312 0.52763866 0.76381933]</p> <p>md_mean = [2.16914526 1.90627322 1.69666482]</p> <p>md_sig = [0.04170691 0.04040816 0.02373173]</p> <p>md_cov = [[ 0.00173947  0.00111714 -0.00027924] [ 0.00111714  0.00163282 -0.00048348] [-0.00027924 -0.00048348  0.00056319]]</p> <p>Expected values for md_gp, md_ref below:</p> <p>md_gp = 0.2452667195670961</p> <p>md_ref = [ 2.18957992  2.51578661  0.71175949  1.7594288   2.12702599 ... 2.92643679 1.28506737  0.50875747  2.12148018  4.50742567]</p> In\u00a0[23]: Copied! <pre>#calculate the Mahalanobis points\nmd_g, md_mean, md_sig, md_cov = obj1.MD_set(pts=3, plot=True)\n\n#use the points to calculate the Mahalanobis distance for our GP\nmd_gp, md_ref = obj1.md_squared(md_g, md_mean, md_cov, n_curves=1000)\n</pre> #calculate the Mahalanobis points md_g, md_mean, md_sig, md_cov = obj1.MD_set(pts=3, plot=True)  #use the points to calculate the Mahalanobis distance for our GP md_gp, md_ref = obj1.md_squared(md_g, md_mean, md_cov, n_curves=1000) <p>Now that we have all of this information, let's plot the histogram of our GP curves to double check that we are indeed getting the $\\chi^{2}$ distribution shape we expect from our GP draws.</p> In\u00a0[24]: Copied! <pre>#call \nhelp(obj1.md_plotter)\nobj1.md_plotter(md_gp, md_ref, md_mean, md_cov, hist=True, box=False)\n</pre> #call  help(obj1.md_plotter) obj1.md_plotter(md_gp, md_ref, md_mean, md_cov, hist=True, box=False) <pre>Help on method md_plotter in module samba.gaussprocess:\n\nmd_plotter(md_gp, md_ref, md_mean=None, md_cov=None, hist=True, box=False) method of samba.gaussprocess.GP instance\n    A plotting function that allows the Mahalanobis distance\n    to be plotted using either a histogram or a box and whisker\n    plot, or both. \n    \n    Box and whisker plot code heavily drawn from J. Melendez' gsum\n    code (https://github.com/buqeye/gsum).\n    \n    :Example:\n        GP.md_plotter(md_gp=np.array([]), md_ref=np.array([]),\n        hist=False, box=True)\n    \n    Parameters:\n    -----------\n    md_gp : float\n        The MD^2 value for the GP curve. \n    \n    md_ref : numpy.ndarray\n        The array of MD^2 values for the reference\n        distribution.\n    \n    md_mean : numpy.ndarray\n        The values of the GP mean at the md_g points. Only used\n        for box and whisker option; default is None. \n    \n    md_cov : numpy.ndarray\n        The values of the GP covariance matrix at the md_g points. \n        Only used for box and whisker option; default is None.\n    \n    hist : bool\n        Toggle for plotting a histogram. Default is True. \n    \n    box : bool\n        Toggle for plotting a box plot. Default is False. \n    \n    Returns:\n    --------\n    None.\n\n</pre> <p>This looks good; the red dot at the low end is the squared MD result from our GP. It is close to 0, indicating that the result we get is not too far from the true curve, and it is within the expectation from the $\\chi^{2}$, so we have a good prediction for our mixed model.</p> In\u00a0[25]: Copied! <pre>#pull weights out of fdagger()\nvargp1 = obj1.var_weights[0]\nvargp2 = obj1.var_weights[1]\nvargp3 = obj1.var_weights[2]\n\n#set up figure\nfig = plt.figure(figsize=(8,6), dpi=600)\nax = plt.axes()\nax.tick_params(axis='x', labelsize=18)\nax.tick_params(axis='y', labelsize=18)\nax.locator_params(nbins=5)\nax.xaxis.set_minor_locator(AutoMinorLocator())\nax.yaxis.set_minor_locator(AutoMinorLocator())\nax.set_xlim(0.0,1.0)\nax.set_ylim(0.0,1.0)\nax.set_yticks([0.1, 0.3, 0.5, 0.7, 0.9])\n\n#labels and true model\nax.set_xlabel('g', fontsize=22)\nax.set_ylabel(r'$\\hat{w}_{k}$', fontsize=22)\n\nax.plot(g, vargp1, 'r', linewidth=3, label=r'$v_{s}^{-1}$')\nax.plot(g, vargp2, 'b', linewidth=3, label=r'$v_{l}^{-1}$')\nax.plot(g, vargp3, 'g', linewidth=3, label=r'$v_{GP}^{-1}$')\n\nax.legend(fontsize=18, loc='upper right')\nplt.show()\n</pre> #pull weights out of fdagger() vargp1 = obj1.var_weights[0] vargp2 = obj1.var_weights[1] vargp3 = obj1.var_weights[2]  #set up figure fig = plt.figure(figsize=(8,6), dpi=600) ax = plt.axes() ax.tick_params(axis='x', labelsize=18) ax.tick_params(axis='y', labelsize=18) ax.locator_params(nbins=5) ax.xaxis.set_minor_locator(AutoMinorLocator()) ax.yaxis.set_minor_locator(AutoMinorLocator()) ax.set_xlim(0.0,1.0) ax.set_ylim(0.0,1.0) ax.set_yticks([0.1, 0.3, 0.5, 0.7, 0.9])  #labels and true model ax.set_xlabel('g', fontsize=22) ax.set_ylabel(r'$\\hat{w}_{k}$', fontsize=22)  ax.plot(g, vargp1, 'r', linewidth=3, label=r'$v_{s}^{-1}$') ax.plot(g, vargp2, 'b', linewidth=3, label=r'$v_{l}^{-1}$') ax.plot(g, vargp3, 'g', linewidth=3, label=r'$v_{GP}^{-1}$')  ax.legend(fontsize=18, loc='upper right') plt.show() <p>This is quite an interesting shape; you should see that the GP (the green curve) takes over in the gap in the mixing process, but then drops in favour of the large-$g$ expansion (blue curve) at the edge of the gap, but then comes back as the dominant model after this for a little bit before mostly surrendering to the large-$g$ expansion. It appears that the jumps in the weights are at the values of $g$ that are close to the training points.</p> <p>In case you wish to compare these results to those of Honda in the paper, where he used the Fractional Power of Rational function (FPR) method (see Sec. 2.3 of this paper) to approximate the true model, you can pull some of his results from the FPR() class and overlay them with the mixed model result we just obtained above.</p> In\u00a0[26]: Copied! <pre>#call the fpr_plot function from FPR() class\nfpr_obj1 = FPR(g, ns, nl)\nfpr_obj1.fpr_plot(mixed_mean, mixed_intervals, fpr_keys=['(3,3)^(1/6)'], ci=68)\n</pre> #call the fpr_plot function from FPR() class fpr_obj1 = FPR(g, ns, nl) fpr_obj1.fpr_plot(mixed_mean, mixed_intervals, fpr_keys=['(3,3)^(1/6)'], ci=68) <p>You can see, in the inset plot, the purple dashed line there, which represents the FPR result. You can change the value of $\\alpha$ to any of the other available values (for $N_{s}, N_{l}=3$, these are 1/2, 1/6, and 1/14) in the fprset() function in the FPR() class, and see how the FPR result changes in comparison to both the true curve and our mixed model result. You see that the FPR result has no uncertainty quantification despite often being rather good at predicting the true curve, making the need for our mixed model clear.</p> <p>Now let's repeat the process of mixing our models for $N_{s} = N_{l} = 2$ and $N_{s} = N_{l} = 5$, as in Fig. 7 in the paper.</p> <p>*Expected Gaussian parameters for for $N_{s}=N_{l}=2$, Matern 3/2 kernel, method 2 below:</p> <p>length_scale = 2.19</p> <p>variance = 2.44</p> <p>Expected values for mean2, sig2, cov2 below:</p> <p>mean2 = [2.55722619 2.55126655 2.54525169 2.53918126 2.53305492...1.57512836 1.57209261 1.56908678 1.56611116 1.56316604]</p> <p>sig2 = [0.01876248 0.01712592 0.01552338 0.01395631 0.01242632 ... 0.0099753  0.00865197 0.00746104 0.00652541 0.00602179]</p> <p>cov2 = [[3.52030736e-04 3.20925503e-04 2.89789816e-04 ... 3.72260454e-07 2.40822250e-07 1.10005451e-07] [3.20925503e-04 2.93297249e-04 2.65481431e-04 ... 3.46610444e-07 2.24230144e-07 1.02428391e-07] ... [2.40822250e-07 2.24230144e-07 2.07502354e-07 ... 4.73465254e-05 4.25809124e-05 3.75668099e-05] [1.10005451e-07 1.02428391e-07 9.47893683e-08 ... 3.87738358e-05 3.75668099e-05 3.62619107e-05]]</p> <p>Expected values for mixed_mean2, mixed_intervals2 below:</p> <p>mixed_mean2 = [2.50662827 2.50643831 2.50586856 2.50491903 2.50358975 ... 1.5751143  1.57208418 1.56908052 1.56610477 1.56315758]</p> <p>mixed_intervals2 = [[2.50662827 2.50662827] [2.50643825 2.50643836] [2.50586769 2.50586943]...[1.56230621 1.57585484] [1.56006899 1.57214055] [1.55753961 1.56877554]]</p> In\u00a0[27]: Copied! <pre>#new object for N_s = N_l = 2 and same steps as before\nns = 2\nnl = 2\nobj2 = GP(g, ns, nl, ci=68, kernel=\"Matern\", nu=1.5, error_model='informative')\nobj_tr2 = obj2.training(plot=False)\nmean2, sig2, cov2 = obj2.validate(plot=True)\nmixed_mean2, mixed_intervals2 = obj2.plot_mix(g, plot_fdagger=True, plot_true=True, GP_mean=mean2, GP_var=np.square(sig2))\n</pre> #new object for N_s = N_l = 2 and same steps as before ns = 2 nl = 2 obj2 = GP(g, ns, nl, ci=68, kernel=\"Matern\", nu=1.5, error_model='informative') obj_tr2 = obj2.training(plot=False) mean2, sig2, cov2 = obj2.validate(plot=True) mixed_mean2, mixed_intervals2 = obj2.plot_mix(g, plot_fdagger=True, plot_true=True, GP_mean=mean2, GP_var=np.square(sig2)) <pre>Gaussian process parameters: 2.53**2 * Matern(length_scale=2.34, nu=1.5)\n</pre> <p>Expected Gaussian parameters for $N_{s}=N_{l}=5$, Matern 3/2 kernel, method 2 below:</p> <p>length_scale = 0.889</p> <p>variance = 1.43</p> <p>Expected values for mean3, sig3, cov3 below:</p> <p>mean3 = [2.51973149 2.51700401 2.51418648 2.51127741 2.50827525...1.55654875 1.55573191 1.55497964 1.55429276 1.55367207]</p> <p>sig3 = [0.03393844 0.03096305 0.02803949 0.02517217 0.02236587... 0.0198764  0.01557175 0.01120256 0.00676925 0.00227229]</p> <p>cov3 = [[ 1.15181746e-03  1.04912112e-03  9.45330624e-04 ... -1.83742307e-06 -1.09250665e-06 -3.60753668e-07] [ 1.04912112e-03  9.58710327e-04  8.66592363e-04 ... -1.71804751e-06 -1.02152758e-06 -3.37315859e-07] ... [-1.09250665e-06 -1.02152758e-06 -9.49045906e-07 ...  7.55800038e-05 4.58227379e-05  1.52962535e-05] [-3.60753668e-07 -3.37315859e-07 -3.13381883e-07 ...  2.51179533e-05 1.52962535e-05  5.16330684e-06]]</p> <p>Expected values for mixed_mean3, mixed_intervals3 below:</p> <p>mixed_mean3 = [2.50662827 2.50643839 2.5058699  2.50492583 2.5036112 ... 1.56617842 1.56330951 1.56045572 1.55761692 1.55479279]</p> <p>mixed_intervals3 = [[2.50662827 2.50662827] [2.50643839 2.50643839] [2.5058699  2.50586991]...[1.56040869 1.56050275] [1.55757141 1.55766243] [1.55474875 1.55483682]]</p> In\u00a0[28]: Copied! <pre>#new object for N_s = N_l = 5 and same steps as before\nns = 5\nnl = 5\nobj3 = GP(g, ns, nl, ci=68, kernel=\"Matern\", nu=1.5, error_model='informative')\nobj_tr3 = obj3.training(plot=False)\nmean3, sig3, cov3 = obj3.validate(plot=True)\nmixed_mean3, mixed_intervals3 = obj3.plot_mix(g, plot_fdagger=True, plot_true=True, GP_mean=mean3, GP_var=np.square(sig3))\n</pre> #new object for N_s = N_l = 5 and same steps as before ns = 5 nl = 5 obj3 = GP(g, ns, nl, ci=68, kernel=\"Matern\", nu=1.5, error_model='informative') obj_tr3 = obj3.training(plot=False) mean3, sig3, cov3 = obj3.validate(plot=True) mixed_mean3, mixed_intervals3 = obj3.plot_mix(g, plot_fdagger=True, plot_true=True, GP_mean=mean3, GP_var=np.square(sig3)) <pre>Gaussian process parameters: 1.42**2 * Matern(length_scale=0.865, nu=1.5)\n</pre> <p>These results also look pretty good! We can say that this method is by far the best for our toy model, and we cannot wait to apply this to real nuclear physics applications! Stay tuned for the next release, where you will be able to input your own functions and test them in this sandbox!</p> <p>Written by: Alexandra Semposki (07 June 2022)</p>"},{"location":"Tutorials/GP_BMM/#an-introduction-to-samba-trivariate-bayesian-model-mixing-wgps","title":"An introduction to SAMBA: Trivariate Bayesian Model Mixing w/GPs\u00b6","text":"<p>Alexandra Semposki</p> <p>Date: 07 June 2022</p>"},{"location":"Tutorials/GP_BMM/#introduction","title":"Introduction\u00b6","text":""},{"location":"Tutorials/GP_BMM/#setting-up-a-gaussian-process-model","title":"Setting up a Gaussian Process model\u00b6","text":""},{"location":"Tutorials/GP_BMM/#multivariate-model-mixing-ppd","title":"Multivariate model mixing: PPD\u00b6","text":"<p>Now comes the time to mix this model in with the two expansions and see what we get. To do this, we call plot_mix() from the Bivariate() class again, but this time we send it the GP mean and variance results.</p>"},{"location":"Tutorials/GP_BMM/#diagnostics-mahalanobis-distance","title":"Diagnostics: Mahalanobis distance\u00b6","text":""},{"location":"Tutorials/GP_BMM/#weights","title":"Weights\u00b6","text":"<p>Now let's look at what the weights of each function look like (the normalized precisions of each model) across the input space, $g$. The actual weight values are contained within a class variable in the fdagger() function, under the name var_weights.</p>"},{"location":"Tutorials/GP_BMM/#fpr-comparison","title":"FPR Comparison\u00b6","text":""},{"location":"Tutorials/LMM/","title":"An introduction to SAMBA: Linear Mixture Model","text":"<p>Welcome to SAMBA, our toddler computational package that allows you to explore the world of Bayesian model mixing! SAMBA (SAndbox for Mixing using Bayesian Analysis) currently supports three methods of model mixing developed in our paper (see this arXiv link). In the future, we will release a version of this package where you, the reader, can input your own functions to mix. At the time of this release, however, we've got our toy problem set up for you to play with.</p> <p>Let's quickly define the toy problem from the paper linked above. We want to mix the expansions of the zero-dimensional $\\phi^4$-theory partition function, below:</p> <p>$$  F(g) = \\int_{-\\infty}^{\\infty} dx~ e^{-\\frac{x^{2}}{2} - g^{2} x^{4}} = \\frac{e^{\\frac{1}{32 g^{2}}}}{2 \\sqrt{2}g} K_{\\frac{1}{4}}\\left(\\frac{1}{32 g^{2}} \\right). $$</p> <p>The two expansions are limits taken at $g = 0$ and $g = \\infty$:</p> <p>$$ F_{s}^{N_s}(g) = \\sum_{k=0}^{N_{s}} s_{k} g^{k}, $$</p> <p>and</p> <p>$$ F_{l}^{N_{l}}(g) = \\frac{1}{\\sqrt{g}} \\sum_{k=0}^{N_{l}} l_{k} g^{-k}, $$</p> <p>with coefficients given as:</p> <p>$$ s_{2k} = \\frac{\\sqrt{2} \\Gamma{(2k + 1/2)}}{k!} (-4)^{k},~~~~~s_{2k + 1} = 0 $$</p> <p>and</p> <p>$$ l_{k} = \\frac{\\Gamma{\\left(\\frac{k}{2} + \\frac{1}{4}\\right)}}{2k!} \\left(-\\frac{1}{2}\\right)^{k}. $$</p> <p>We begin by importing all of the Python packages we will need in this Jupyter notebook.</p> In\u00a0[2]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport numpy as np\nimport math\nimport statistics\nimport emcee\nimport corner\nfrom cycler import cycler\nfrom scipy import stats, special, integrate\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import AutoMinorLocator\n%matplotlib inline\n\n#matplotlib settings for Latex plots\nimport matplotlib\n</pre> %load_ext autoreload %autoreload 2  import numpy as np import math import statistics import emcee import corner from cycler import cycler from scipy import stats, special, integrate  import matplotlib.pyplot as plt from matplotlib.ticker import AutoMinorLocator %matplotlib inline  #matplotlib settings for Latex plots import matplotlib <pre>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</pre> <p>Now we import the classes from SAMBA that we'll need for the Linear Mixture Model (LMM from here on).</p> In\u00a0[3]: Copied! <pre>import sys\nsys.path.append('../../')\n\nfrom samba.models import Models, Uncertainties\nfrom samba.mixing import LMM\nfrom samba.priors import Priors\n</pre> import sys sys.path.append('../../')  from samba.models import Models, Uncertainties from samba.mixing import LMM from samba.priors import Priors <p>The first two classes, Models and Uncertainties, give our toy model and its theory errors. LMM is the class that performs the linear mixture method, and Priors are the priors needed to sample any parameters in this model. This will be clear later on.</p> <p>Now we plot the functions we want to mix to see what they look like. We need to first pick a value for $N_{s}$ and $N_{l}$. Let's pick $N_{s}$ = 2 and $N_{l}$ = 2, as in the paper.</p> In\u00a0[5]: Copied! <pre>#set up the linspace for input variable g and N_s and N_l\ng = np.linspace(1e-6,1.0,100)\nns = np.array([2])\nnl = np.array([2])\n\n#call the plot_models() function from Models()\nm = LMM(ns, nl, error_model='informative')\nplot = m.plot_models(g)\n</pre> #set up the linspace for input variable g and N_s and N_l g = np.linspace(1e-6,1.0,100) ns = np.array([2]) nl = np.array([2])  #call the plot_models() function from Models() m = LMM(ns, nl, error_model='informative') plot = m.plot_models(g) <p>Now let's try to mix these two functions using the LMM() class. We'll need data in the gap, as this method relies on some data there. To do this, we estimate the length of the gap by eye and make a linspace over it with a few data points.</p> In\u00a0[6]: Copied! <pre>#make linspace for data\ng_data = np.linspace(0.1, 0.6, 10)\n\n#call the add_data() function to create the data\ndata, sigma = m.add_data(g, g_data, error=0.01)\n</pre> #make linspace for data g_data = np.linspace(0.1, 0.6, 10)  #call the add_data() function to create the data data, sigma = m.add_data(g, g_data, error=0.01) <p>Now we call the mixing function, mixed_model(), and give it this data. It will ask us which mixing function we wish to use, out of three options: logistic, cdf, and piecewise cosine. As discussed in the paper, the best option is the piecewise cosine. This mixing function will require emcee to sample 3 parameters, as we'll see later on in a plot we can generate with the results of the sampling.</p> <p>The equation the sampler will solve is the posterior written as $$ p(\\theta|\\mathbf{D}) = p(\\theta)\\prod_{i=1}^{n} \\left\\{ \\alpha(g_{i}; \\theta) ~\\mathcal{N}(F^{N_s}_{s}(g_{i}), \\sigma_{d_{i}}^{2} + \\sigma_{N_s}^2) + (1 - \\alpha(g_{i}; \\theta))~ \\mathcal{N}(F^{N_l}_{l}(g_{i}), \\sigma_{d_{i}}^{2} + \\sigma_{N_l}^2) \\right\\}, $$</p> <p>where $\\alpha(g;\\theta)$ is this piecewise cosine mixing function (see Eq. (15) in the paper), the two individual models are written as Gaussian distributions, and their variances, $\\sigma_{d_{i}}^{2} + \\sigma_{N_s}^2$ and $\\sigma_{d_{i}}^{2} + \\sigma_{N_l}^2$, are combinations of the error on the data points ($\\sigma_{d_{i}}^{2}$) and the theory error at each data point ($\\sigma_{N}^2$). The prior $p(\\theta)$ is given as three truncated Gaussians in the form</p> <p>$$ p(\\theta) = \\mathcal{U}(\\theta_{1} \\in [0,b])~\\mathcal{N}(\\theta_{1}; \\mu_{1}, 0.05)         \\times\\mathcal{U}(\\theta_{2} \\in [\\theta_{1},b])~\\mathcal{N}(\\theta_{2}; \\mu_{2}, 0.05)         \\times\\mathcal{U}(\\theta_{3} \\in [\\theta_{2},b])~\\mathcal{N}(\\theta_{3}; \\mu_{3}, 0.05). $$</p> <p>The means $\\mu$ and ranges $b$, as well as the variance (given there as 0.05 for each prior) are dependent upon the series expansion chosen and therefore can be altered as need arises.</p> <p>NOTE: If you merely choose some data points and run the code, you will notice that, when all is said and done, the parameters may not be located in very accurate spots in the gap. This could be because the Priors() function lpdf() needs some adjusting, by you! Your freedom in the toy model so far is to adjust the information in the Priors() class so that the best starting points for the sampler are given to it. As mentioned above, the mean and variance of the Gaussian prior can be changed to better match where one would expect that parameter to be located, and the uniform prior helps the sampler keep the parameters from crossing when they are sampled. They must not cross, else the sampler will not be happy and the mixing will not work properly. Hence, adjusting the uniform prior so that the parameters remain in their own regions, but allowing them some wiggle room via the Gaussian variance, is the best method when dealing with the piecewise cosine mixing function.</p> <p>So far, the best way change these parameters is by directly altering the class file priors.py yourself, but in the future when more flexibility for users is added, lpdf() will become a function you can directly input into the class without changing anything internally in the package.</p> <p>Note that the values here will not be absolute---the emcee sampler will allow for some differences between runs, as it is stochastic, and the data generated in this notebook is randomized for each run. However, expected values for the trace and onward in this notebook should be within a few percent of the results you obtain yourself using the prior values below:</p> <p>Prior information for benchmark (set within priors.py file):</p> <p>g1 = self.luniform(params[0], 0.01, 0.3) + stats.norm.logpdf(params[0], 0.1, 0.05)</p> <p>g3 = self.luniform(params[2], params[0], 0.55) + stats.norm.logpdf(params[2], 0.4, 0.05)</p> <p>g2 = self.luniform(params[1], params[2], 0.8) + stats.norm.logpdf(params[1], 0.6, 0.05)</p> <p>*Expected values of trace for $N_{s}=N_{l}=2$, cosine mixing function, nsteps=3000:</p> <p>trace = [[0.07418137 0.07418137 0.03490732 ... 0.12225827 0.14157856 0.16379292] [0.59066046 0.59066046 0.54589913 ... 0.51536977 0.50214119 0.48472922] [0.39022897 0.39022897 0.37559441 ... 0.41987635 0.4359556  0.42130717]]</p> In\u00a0[7]: Copied! <pre>#call mixed_model()\nchain, trace = m.mixed_model(g_data, data, sigma, mixing_function='cosine', nsteps=3000)\n</pre> #call mixed_model() chain, trace = m.mixed_model(g_data, data, sigma, mixing_function='cosine', nsteps=3000) <pre>Using 20 walkers with 3000 steps each, for a total of 60000 samples.\nCalculation finished!\nDuration = 1 min, 33 sec.\n</pre> <p>We have the samples in hand now. However, we would like to thin them so that we cut out any correlations in the samples. This allows us to capture the most important features of the samples. Let's do this using the autocorrelation length in the function stats_chain(). The results for the mean and median of the mixing function parameter distributions will also be returned from this function, along with the thinned array for any further work we wish to do with the sample results.</p> <p>Expected values for thin_array, mean, median for $N_{s}=N_{l}=2$, cosine mixing function:</p> <p>thin_array = [[0.13377046 0.15029928 0.2248071  ... 0.17242319 0.19149209 0.11299201] [0.31068    0.30495626 0.49641581 ... 0.51533557 0.63964489 0.57477458] [0.21107303 0.25329124 0.37645051 ... 0.36013452 0.36440693 0.42444696]]</p> <p>mean = [0.11096199 0.5882248  0.38049597]</p> <p>median = [0.10839164 0.59186747 0.38149871]</p> In\u00a0[8]: Copied! <pre>thin_array, mean, median = m.stats_chain(chain)\n</pre> thin_array, mean, median = m.stats_chain(chain) <p>Now that we know these values, we can take a look at the mixing function we used (before looking at the final PPD result of the mixed model). To do this properly, we employ the MAP value results from the function MAP_values(), instead of using the mean or the median, as these can be misleading when one's distributions are not purely Gaussian. We then overlay the MAP values with the mixing curve itself, to show where the parameter values have landed in $g$.</p> <p>Expected MAP values for $N_{2}=N_{l}=2$, cosine mixing function:</p> <p>map_values = [0.10497989 0.59295989 0.38282007]</p> In\u00a0[9]: Copied! <pre>map_values = m.MAP_values(thin_array, g, g_data, data, sigma)\n\nprint(map_values)\n</pre> map_values = m.MAP_values(thin_array, g, g_data, data, sigma)  print(map_values) <pre>[0.10164784 0.59502968 0.3833919 ]\n</pre> In\u00a0[10]: Copied! <pre>#define a weight plot function for simplicity\ndef weight_plot(g, map_values):\n    \n    #set up figure\n    fig = plt.figure(figsize=(8,6), dpi=600)\n    ax = plt.axes()\n    ax.tick_params(axis='x', labelsize=18)\n    ax.tick_params(axis='y', labelsize=18)\n    ax.locator_params(nbins=5)\n    ax.xaxis.set_minor_locator(AutoMinorLocator())\n    ax.yaxis.set_minor_locator(AutoMinorLocator())\n    ax.set_xlim(0.0,1.0)\n    ax.set_ylim(0.0,1.0)\n    ax.set_yticks([0.1, 0.3, 0.5, 0.7, 0.9])\n\n    #labels and true model\n  #  ax.set_title('Method: {}'.format(k))\n    ax.set_xlabel('g', fontsize=22)\n    ax.set_ylabel(r'$\\hat{w}_{k}$', fontsize=22)\n    \n    #calculate the cosine function\n    cosine = np.zeros(len(g))\n    for i in range(len(g)):\n        cosine[i] = m.switchcos(map_values, g[i])\n\n    #plot the weights\n    ax.plot(g, cosine, 'r', linewidth=3, label=r'$\\alpha(g;\\theta)$')\n    ax.plot(g, 1-cosine, 'b', linewidth=3, label=r'$1 - \\alpha(g;\\theta)$')\n    \n    #plot the training points as lines\n    ax.axvline(x=map_values[0], color='darkorange', linestyle='dashed', label=r'$\\theta_1$')\n    ax.axvline(x=map_values[2], color='darkviolet', linestyle='dashdot', label=r'$\\theta_2$')\n    ax.axvline(x=map_values[1], color='deeppink', linestyle='dashed', label=r'$\\theta_3$')\n\n    ax.legend(fontsize=14, loc='upper right', frameon=False)\n    plt.show()\n    \n    return None \n</pre> #define a weight plot function for simplicity def weight_plot(g, map_values):          #set up figure     fig = plt.figure(figsize=(8,6), dpi=600)     ax = plt.axes()     ax.tick_params(axis='x', labelsize=18)     ax.tick_params(axis='y', labelsize=18)     ax.locator_params(nbins=5)     ax.xaxis.set_minor_locator(AutoMinorLocator())     ax.yaxis.set_minor_locator(AutoMinorLocator())     ax.set_xlim(0.0,1.0)     ax.set_ylim(0.0,1.0)     ax.set_yticks([0.1, 0.3, 0.5, 0.7, 0.9])      #labels and true model   #  ax.set_title('Method: {}'.format(k))     ax.set_xlabel('g', fontsize=22)     ax.set_ylabel(r'$\\hat{w}_{k}$', fontsize=22)          #calculate the cosine function     cosine = np.zeros(len(g))     for i in range(len(g)):         cosine[i] = m.switchcos(map_values, g[i])      #plot the weights     ax.plot(g, cosine, 'r', linewidth=3, label=r'$\\alpha(g;\\theta)$')     ax.plot(g, 1-cosine, 'b', linewidth=3, label=r'$1 - \\alpha(g;\\theta)$')          #plot the training points as lines     ax.axvline(x=map_values[0], color='darkorange', linestyle='dashed', label=r'$\\theta_1$')     ax.axvline(x=map_values[2], color='darkviolet', linestyle='dashdot', label=r'$\\theta_2$')     ax.axvline(x=map_values[1], color='deeppink', linestyle='dashed', label=r'$\\theta_3$')      ax.legend(fontsize=14, loc='upper right', frameon=False)     plt.show()          return None  In\u00a0[12]: Copied! <pre>weight_plot(g, map_values)\n</pre> weight_plot(g, map_values) <p>Now we want to calculate the mixed model, which means we need to obtain the posterior predictive distribution, or PPD, as a function of points in $g$ that can be spread across the input space. For this, we make another array in $g$ and call the ppd() function, employing our thinned trace as well.</p> <p>The PPD equation that is solved in this code is given as</p> <p>$$ p(\\tilde y(g)|\\theta, \\mathbf{D}) = \\sum_{j=1}^{M} \\alpha(g; \\theta_{j}) ~\\mathcal{N}(F^{N_s}_{s}(g), \\sigma_{d_{i}}^{2} + \\sigma_{N_s}^2) + (1 - \\alpha(g; \\theta_{j}))~ \\mathcal{N}(F^{N_l}_{l}(g), \\sigma_{d_{i}}^{2} + \\sigma_{N_l}^2), $$</p> <p>much like before with the sampled posterior, except now we are predicting for a new observation $\\tilde y(g)$.</p> <p>Expected median_results, intervals for $N_{s}=N_{l}=2$, cosine mixing function:</p> <p>median_results = [2.50662827, 2.50643831, 2.50586856, 2.50491903, 2.50358972...1.57507493, 1.57205394, 1.56905142, 1.56606715, 1.56310093] intervals = [[2.50662827, 2.50662827], [2.50643831, 2.50643831], [2.50586856, 2.50586856],...[1.56905142, 1.56905142], [1.56606715, 1.56606715], [1.56310093, 1.56310093]]</p> In\u00a0[13]: Copied! <pre>#PPD linspace\ng_ppd = np.linspace(1e-6, 1.0, 200)\n\n#PPD calculation using ppd() and MAP parameter values\nmedian_results, intervals = m.ppd(thin_array, map_values, g_data, g_ppd, data, 0.68)\n</pre> #PPD linspace g_ppd = np.linspace(1e-6, 1.0, 200)  #PPD calculation using ppd() and MAP parameter values median_results, intervals = m.ppd(thin_array, map_values, g_data, g_ppd, data, 0.68) <p>And that's the mixed model result! The green curve is our PPD result, which has been generated using the median of the posterior at each point in $g$. The green band is the credibility interval at 68%, using the HPD (Highest Posterior Density) method, which is contained in the function hpd_interval() in the LMM() class, and does its work by finding the smallest region with 68% of the posterior density and plotting it.</p> <p>It is quite evident, however, that the mixed model here is not matching the true curve very well at all. In the next notebook (Bivariate_BMM) we will look at another method to mix these two series expansions together.</p> <p>Written by: Alexandra Semposki (06 June 2022)</p>"},{"location":"Tutorials/LMM/#an-introduction-to-samba-linear-mixture-model","title":"An introduction to SAMBA: Linear Mixture Model\u00b6","text":"<p>Alexandra Semposki</p> <p>Date: 06 June 2022</p>"},{"location":"Tutorials/LMM/#introduction-and-model-setup","title":"Introduction and model setup\u00b6","text":""},{"location":"Tutorials/LMM/#bmm-using-the-linear-model-mixing-method","title":"BMM using the Linear Model Mixing method\u00b6","text":""},{"location":"Tutorials/LMM/#adding-data","title":"Adding data\u00b6","text":""},{"location":"Tutorials/LMM/#choosing-a-function-for-the-weights-and-parameter-estimation-using-mcmc","title":"Choosing a function for the weights and parameter estimation using MCMC\u00b6","text":""},{"location":"Tutorials/LMM/#weights","title":"Weights\u00b6","text":""},{"location":"Tutorials/LMM/#calculating-the-posterior-predictive-distribution-ppd","title":"Calculating the posterior predictive distribution (PPD)\u00b6","text":""}]}